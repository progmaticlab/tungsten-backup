{"id":7533,"date":"2017-06-19T23:03:38","date_gmt":"2017-06-20T06:03:38","guid":{"rendered":"http:\/\/www.opencontrail.org\/?p=7533"},"modified":"2017-06-19T23:03:38","modified_gmt":"2017-06-20T06:03:38","slug":"are-service-meshes-the-next-gen-sdn","status":"publish","type":"post","link":"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/","title":{"rendered":"Are Service Meshes the Next-gen SDN?"},"content":{"rendered":"<p><img decoding=\"async\" class=\"alignnone wp-image-7534\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2017\/06\/mesh-1430107.png\" alt=\"\" width=\"100%\" \/><\/p>\n<p><a href=\"https:\/\/soundcloud.com\/james-kelly-63\/are-service-meshes-the-next-gen-sdn\" target=\"_blank\" rel=\"noopener\"><img decoding=\"async\" class=\"alignnone wp-image-7544\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2017\/06\/Screen-Shot-2017-06-19-at-11.01.15-PM.png\" alt=\"\" width=\"100%\" \/><\/a><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><em><strong>June 28, 2017 update:<\/strong><\/em> more awesome background on service meshes, proxies and Istio in particular on yet another new SE Daily <a href=\"https:\/\/softwareengineeringdaily.com\/2017\/06\/27\/istio-service-mesh-with-varun-talwar-and-louis-ryan\/\" target=\"_blank\" rel=\"noopener\">podcast<\/a> with Istio engineers from Google.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong><em>June 26, 2017 update:\u00a0<\/em><\/strong>For great background on service meshes (a relatively new concept) check out <a href=\"https:\/\/softwareengineeringdaily.com\/2017\/06\/26\/service-mesh-with-william-morgan\/\" target=\"_blank\" rel=\"noopener\">today&#8217;s podcast<\/a>\u00a0on SE Daily with the founder of Linkerd.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Whether you\u2019re adopting a containers- or functions-as-a-Service stack, or both, in the new world of micro-services architectures, one thing that has grown in importance is the network because:<\/span><\/p>\n<ol>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Micro-service application building blocks are decoupled from one another over the network. They re-integrate over the network with APIs as remote procedure calls (RPC) that have evolved from the likes of CORBA and RMI, past web services with SOAP and REST, to new methods like Apache <a href=\"https:\/\/thrift.apache.org\/\">Thrift<\/a> and the even-fresher <a href=\"http:\/\/www.grpc.io\/\">gRPC<\/a>: a new CNCF project donated by Google for secure and fast http2-based RPC. RPC has been around for a long time, but now the network is actually fast enough to handle it as a general means of communication between application components, allowing us to break down monoliths where service modules would have previously been bundled or coupled with tighter API communications based on package includes, libraries, and some of us may even remember more esoteric IPC.<\/span><\/li>\n<\/ol>\n<p>&nbsp;<\/p>\n<ol start=\"2\">\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Each micro-service building block scales out by instance <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/replicaset\/\">replication<\/a>. The front-end of each micro-service is thus a load balancer which is itself a network component, but beyond that, the services need to discover their dependent services, which is generally done with DNS and service discovery.<\/span><\/li>\n<\/ol>\n<p>&nbsp;<\/p>\n<ol start=\"3\">\n<li><span style=\"font-family: arial, helvetica, sans-serif\">To boost processing scale out and engineer better reliability, each micro-service instance is often itself decoupled from application state and its storage. The state is saved over the network as well. For example, using an API into an object store, a database, a k\/v-store, a streaming queue or a message queue. There is also good-ol\u2019 disk, but such disk and accompanying file systems too, may be virtual network-mounted volumes. The API- and RPC-accessible variants of storing state are, themselves, systems that are micro-services too, and probably the best example of using disks in fact. They would also incorporate a lot of distributed storage magic to deliver on whatever promises they make, and that magic is often performed over the network.<\/span><\/li>\n<\/ol>\n<p>&nbsp;<\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Hopefully we\u2019re all on the same page now as to why the network is important micro-services glue. If this was familiar to you, then maybe you already know about cloud-native SDN solutions and service meshes too.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">The idea and implementation of a service mesh is fairly new. The topic is also garnering a lot of attention because they handle the main networking challenges listed above (esp #1 &amp; 2), and much more in the case of new projects like the CNCF Linkerd and newly launched project Istio.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Since I\u2019ve written about SDN for container stacks before, namely OpenContrail for Kubernetes and OpenShift, I\u2019m not going to cover it super deeply. Nor am I going to cover service meshes in general detail except to make comparisons. I will also put some references below and throughout. And I\u2019ve tried to organize my blog by compartmentalizing the comparisons, so you can skip technical bits that you might not care for, and dive into the ones that matter most to you.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">So on to the fun! Let\u2019s look at some of the use cases and features of services meshes and compare them to SDN solutions, mostly OpenContrail, so we can answer the question posed in the title. Are service meshes the \u201cNext-Generation\u201d of SDN?<\/span><\/p>\n<p>&nbsp;<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"wp-image-7535 aligncenter\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2017\/06\/logos-service-mesh.png\" alt=\"\" width=\"508\" height=\"286\" \/><\/p>\n<p>&nbsp;<\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong>Automating SDN and Service Meshes<\/strong><\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">First, let\u2019s have a look at 3 general aspects of automation in various contexts where SDN and service meshes are used: 1 &#8211; programmability, 2 &#8211; configuration and 3 &#8211; installation.<\/span><\/p>\n<ol>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em>Programmability<\/em><\/span><br \/>\n<span style=\"font-family: arial, helvetica, sans-serif\"> When it comes to automating everything, programmability is a must. Good SDNs are untethered from the hardware side of networking, and many, like OpenContrail, offer a logically centralized control plane with an <a href=\"http:\/\/www.opencontrail.org\/documentation\/api\/r4.0\/\">API<\/a>. The main two service meshes introduced above do this too, and they follow an architectural pattern similar to SDNs of centralized control plane with a distributed forwarding plane agent. While Istio has a centralized control plane <a href=\"https:\/\/istio.io\/docs\/concepts\/what-is-istio\/overview.html#architecture\">API<\/a>, Linkerd is more distributed but offers an <a href=\"https:\/\/blog.buoyant.io\/2017\/05\/24\/a-service-mesh-for-kubernetes-part-x-the-service-mesh-api\/\">API<\/a> through its Namerd counterpart. Most people would probably say that the two service meshes\u2019 gRPC API is more modern and advantageous than the OpenContrail RESTful API, but then again OpenContrail\u2019s API is very well built-out and tested compared to Istio\u2019s still-primordial API functions.<\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em>Configuration<br \/>\n<\/em>A bigger difference than the API, is in how functionality can be accessed. The service meshes take in YAML to declare configuration intent that can be delivered through a CLI. I suppose most people would agree that\u2019s an advantage over SDNs that don\u2019t offer that (at least OpenContrail doesn\u2019t today). In terms of a web-based interface, the service meshes do offer those, as so many SDNs. OpenContrail\u2019s web interface is fairly sophisticated after 5 years of development, yet still modern-feeling and enterprise friendly.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> Looking toward \u201cnetwork as code\u201d trends however, CLI and YAML is codable and version controllable more easily than say OpenContrail\u2019s API calls. In an OpenStack environment OpenContrail can be configured with YAML-based <a href=\"https:\/\/docs.openstack.org\/developer\/heat\/template_guide\/hot_spec.html#hot-spec\">Heat<\/a> templates, but that\u2019s less relevant for container-based K8s and OpenShift world. In a K8s world, OpenContrail SDN configuration is annotated into K8s objects. It\u2019s intentionally simple, so it\u2019s just exposing a fraction of the OpenContrail functionality. It remains to be seen what will be done with K8s <a href=\"https:\/\/kubernetes.io\/docs\/tasks\/access-kubernetes-api\/extend-api-third-party-resource\/\">TPRs<\/a>, ConfigMaps or through some OpenContrail interpreter of its own.<\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em>Installation<\/em><\/span><br \/>\n<span style=\"font-family: arial, helvetica, sans-serif\"> When it comes to getting going with Linkerd, having a company behind it, Buoyant, means anyone can get support, but getting through day-one looks pretty straightforward on one\u2019s own anyway. Deployed with Kubernetes in the model of a <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/workloads\/controllers\/daemonset\/\">DaemonSet<\/a>, it is straightforward to use out of the box.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> Istio is brand new, but already has Helm charts to deploy it quickly with Kubernetes thanks to our friends at Deis (<a href=\"https:\/\/twitter.com\/LachlanEvenson\">@LachlanEvenson<\/a> has done some amazing demo videos already \u2013links below). Using Istio, on the other hand, means bundling its Envoy proxy into every Kubernetes pod as a <a href=\"http:\/\/blog.kubernetes.io\/2015\/06\/the-distributed-system-toolkit-patterns.html\">sidecar<\/a>. It\u2019s an extra step, but it looks fairly painless with the <a href=\"https:\/\/www.istio.io\/docs\/tasks\/integrating-services-into-istio.html\">kube-inject<\/a> Sidecar <a href=\"https:\/\/linkerd.io\/getting-started\/k8s-daemonset\/\">vs.<\/a> DaemonSet considerations aside, this bundling is doing some magic, and it\u2019s important to understand for debugging later.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> When it comes to SDNs, they\u2019re all different wrt deployments. OpenContrail is working on a Juniper-supported Helm chart for simple deployment, but in the meantime there are Ansible playbooks and other comparable configuration management solutions offered by the community.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> One thing OpenContrail has in common with the two service meshes, is that it is deployed as containers. One difference is that OpenContrail\u2019s forwarding agent on each node is both a user-space component and Kernel module (or DPDK-based or SmartNIC-based). They\u2019re containerized, but the kernel module is only there for installation purposes to bootstrap the insmod installation. You may feel ambivalent towards kernel modules\u2026 The kernel module will obviously streamline performance and integration with the networking stack, but the resources it uses are not container-based, and thus not resource restricted, so resource management is different than say a user-space sidecar process. Anyway, this is same deal as using the kube-proxy or any IP tables-based networking which OpenContrail vRouter replaces.<\/span><\/li>\n<\/ol>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong>SDN and Service Meshes: Considerations in DevOps<\/strong><\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">When reflecting on micro-services architectures, we must remember that the complexity doesn\u2019t stop there. There is also the devops apparatus to manage the application through dev, test, staging and prod, and through continuous integration, delivery, and response. Let\u2019s look at some of the considerations:<\/span><\/p>\n<ol>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em><strong>Multi-tenancy \/ multi-environment<\/strong><br \/>\n<\/em>In a shared cluster, code shouldn\u2019t focus on operational contexts like operator or application dev\/test environments. To achieve this, we need isolation mechanisms. Kubernetes namespaces and RBAC help this, but there is still more to do. I\u2019ll quickly recap my understanding of the routing in OpenContrail and service meshes to better dissect the considerations for context isolation.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> <strong><em>&lt;background&gt;<\/em><\/strong><\/span><br \/>\n<span style=\"font-family: arial, helvetica, sans-serif\"> OpenContrail for K8s recap: One common SDN approach to isolation is overlay networks. They allow us to create virtual networks that are separate from each other on the wire (different encapsulation markings) and often in the forwarding agent as well. This is indeed the case with OpenContrail, but OpenContrail also allows higher-level namespace-like wrappers called <a href=\"http:\/\/www.opencontrail.org\/opencontrail-architecture-documentation\/#section3_2\">domains\/tenants and projects<\/a>. Domains are isolated from each other, projects within domains are isolated from each other, and virtual networks within projects are isolated from each other. This hierarchy maps nicely to isolate tenants and dev\/test\/staging\/prod environments, and then we can use a virtual network to isolate every micro-service. To connect networks (optionally across domains and projects), a policy is created and applied to the networks that need connecting, and this policy can optionally specify direction, network names, ports, and service chains to insert (for example, a stateful firewall service). <\/span><span style=\"font-family: arial, helvetica, sans-serif\"> The way these domains, projects, and networks are created for Kubernetes is based on annotations. OpenContrail maps namespaces to their own OpenContrail project or their own virtual network, so optionally micro-services can all be reachable to each other on one big network (similar to the default cluster behavior). There are security concerns there, and OpenContrail can also enforce ACL rules and automate their creation as a method of isolating micro-services for security based on K8s object annotations or implementing Kubernetes <a href=\"https:\/\/kubernetes.io\/docs\/tasks\/administer-cluster\/declare-network-policy\/\">NetworkPolicy<\/a> objects as OpenContrail <a href=\"http:\/\/www.opencontrail.org\/opencontrail-architecture-documentation\/#section3_2\">security groups<\/a> and rules. Another kind of new annotations on objects like K8s deployments, jobs or services would specify the whole OpenContrail domain, project, and virtual network of choice. Personally, I think the best approach is a hierarchy designed to match devops teams and environments structure that makes use of the OpenContrail model of segmentation by domain, project and network. This is in (unfortunately) contrast to the simpler yet more frequently used global default-deny rule and ever-growing whitelist that ensues that turns your cluster into Swiss cheese. Have fun managing that :\/ <\/span><span style=\"font-family: arial, helvetica, sans-serif\"> The overlay for SDN is at layer 2, 3 and 4, meaning that when the packet is received on the node, the vRouter (in OpenContrail\u2019s case) will receive the packet destined to it and look at the inner header (the VXLAN ID or MPLS LSP number) to determine the domain\/tenant, project and network. Basically, the number identifies which routing table will be used as a lookup context for the inner destination address, and then (pending ACLs) the packet is handed off to the right container\/pod interface (per <a href=\"https:\/\/github.com\/containernetworking\/cni\">CNI<\/a> standards).<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> Service mesh background: The model of Istio\u2019s Envoy and Linkerd insofar as they are used (which can be on a per-microservice basis), is that there is a layer-7 router and proxy in front of your microservices. All traffic is intercepted at this proxy, and tunneled between nodes. Basically, it is also an overlay at a higher layer.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> The overlay at layer-7 is conceptually the same as SDN overlays except that the overlay protocol over the wire is generally HTTP or HTTP2, or TLS with one of those. In the DaemonSet deployment mode of Linkerd, there is one IP address for the host and Linkerd will proxy all traffic. It\u2019s conceptually similar to the vRouter except in reality it is just handling HTTP traffic on certain ports, not all traffic. Traffic is routed and destinations are resolved using a delegation tables (<a href=\"https:\/\/twitter.github.io\/finagle\/guide\/Names.html#interpreting-paths-with-delegation-tables\">dtabs<\/a>) format inherited from Finagle. In the <a href=\"https:\/\/linkerd.io\/in-depth\/deployment\/\">sidecar<\/a> deployment model for Linkerd or for Istio\u2019s Envoy (which is always a sidecar), the proxy is actually in the same container network context as each micro-service because it is in the same pod. There are some IP tables <a href=\"https:\/\/istio.io\/docs\/tasks\/integrating-services-into-istio.html#understanding-what-happened\">tricks<\/a> they do to sit between your application and the network. In Istio Pilot (the control plane) and Envoy (the data plane), traffic routing and destination resolution is based primarily on the Kubernetes service name.<\/span><br \/>\n<span style=\"font-family: arial, helvetica, sans-serif\"> <strong><em>&lt;\/background&gt;<\/em><\/strong><\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> With that background, here are a few implications for multi-tenancy.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> Let\u2019s observe that in the SDN setup, the tenant, environment and application (network) classification happens in the kernel vRouter. In service mesh proxies, we still need a CNI solution to get the packets into the pod in the first place. In Linkerd, we need dtab <a href=\"https:\/\/linkerd.io\/in-depth\/routing\/\">routing rules<\/a> that include tenant, environment and service. Dtabs seems to give a good way to break this down that is manageable. In the sidecar mode, more frequently used for Envoy, it\u2019s likely that the pod in which traffic ends up already has a K8s namespace associated with it, and so we would map a tenant or environment outside of the Istio <a href=\"https:\/\/istio.io\/docs\/concepts\/traffic-management\/rules-configuration.html\">rules<\/a>, and just focus on resolving the service name to a container and port when it comes to Envoy.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> It seems that OpenContrail here has a good way to match the hierarchy of separate teams, and separate out those RBAC and routing contexts. Linkerd dtabs are probably a more flexible way to create as many layers of routing interpretation as you want, but it may need a stronger RBAC to allow the splitting of dtabs among team tenants for security and coordination. Istio doesn\u2019t do much in the way of isolating tenants and environments at all. Maybe that is out of scope for it which seems reasonable since Envoy is always a sidecar container and you should have underlying multi-tenant networking anyway to get traffic into the sidecar\u2019s pod.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> One more point is that service discovery baked into the service mesh solutions, but it is still important in the SDN world, and systems that include DNS (OpenContrail does) can help manage name resolution in a multi-tenant way as well as provide IP address management (like bring your own IPs) across the environments you carve up. This is out of scope for service meshes, but with respect to multiple team and dev\/test\/staging\/prod environments, it may be desirable to have the same IP address management pools and subnets.<\/span><em style=\"font-family: arial, helvetica, sans-serif\">\u00a0<\/em><\/li>\n<\/ol>\n<ol start=\"2\">\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em><strong>Deployment and load balancing<\/strong><br \/>\n<\/em>When it comes to deployment and continuous delivery (CD), the fact that SDN is programmable helps, but service meshes have a clear advantage here because they\u2019re designed with CD in mind.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> To do <a href=\"https:\/\/martinfowler.com\/bliki\/BlueGreenDeployment.html\">blue-green<\/a> deployments with SDN, it helps to have floating IP functionality. Basically, we can cut over to green (float a virtual IP to the new version of the micro-service) and safely float it back to blue if we needed to in case of an issue. As you continuously deliver or promote staging into the non-live deployment, you can still reach it with a different floating IP address. OpenContrail handles overlapping floating IPs to let you juggle this however you want to.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> Service mesh routing rules can achieve the same thing, but based on routing switch overs at the HTTP level that point to for <a href=\"https:\/\/istio.io\/docs\/concepts\/traffic-management\/rules-configuration.html#split-traffic-between-service-versions\">example<\/a> a newer backend version. What service meshes further allow is traffic roll over like this <a href=\"https:\/\/blog.buoyant.io\/2016\/11\/04\/a-service-mesh-for-kubernetes-part-iv-continuous-deployment-via-traffic-shifting\/\">example<\/a> showing a small percentage of traffic at first and then all of it, effectively giving you a canary deployment that is traffic load-oriented as opposed to a Kubernetes rolling upgrade or the Kubernetes deployment canary <a href=\"https:\/\/kubernetes.io\/docs\/concepts\/cluster-administration\/manage-deployment\/#canary-deployments\">strategy<\/a> that gives you a canary that is instance-count based, and relies on the load balancing across instances to partition traffic.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> This brings us to load balancing. Balancing traffic between the instances of a micro-service, by default happens with the K8s kube-proxy controller by its programming of IP tables. There is a bit of a performance and scale advantage here of using OpenContrail\u2019s vRouter which uses its own ECMP load balancing and NAT instead of the kernel\u2019s IP tables.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> Service meshes also handle such load balancing. They support wider ranging features, both in terms of load balancing schemes like <a href=\"https:\/\/blog.buoyant.io\/2016\/03\/16\/beyond-round-robin-load-balancing-for-latency\/\">EWMA<\/a> and also in terms of cases to eject an instance from the load balancing pool, like if they\u2019re too slow.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> Of course service meshes do also handle load balancing for ingress HTTP frontending. Linkerd and Istio integrate with the K8s Ingress as ingress controllers. While most SDNs don\u2019t seem to offer this, OpenContrail does have a solution here that is based on haproxy, an open source TCP proxy project. One difference, is that OpenContrail does not yet support SSL\/TLS, but there are also K8s pluggable alternatives like nginx for pure software-defined load balancing.<\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em><strong>Reliability Engineering<\/strong><br \/>\n<\/em>Yes, I categorize SRE and continuous response under the DevOps umbrella. In this area, since service meshes are more application-aware, it\u2019s no surprise, they do the most further the causes of reliability.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> When it comes to reliably optimizing and engineering performance, one point here from above is that EWMA and such advanced load balancing policies will assist in avoiding or ejecting slow instances, thus improving tail latency. A Buoyant <a href=\"https:\/\/blog.buoyant.io\/2017\/01\/31\/making-things-faster-by-adding-more-steps\/\">article<\/a> about performance addresses performance in terms of latency directly. Envoy and Linkerd are after all TCP proxies, and unpacking and repacking a TCP stream is seen as notoriously slow if you\u2019re in the networking world (I can attest to this personally recalling one project I assisted with that did HTTP header injection for ad placement purposes). Anyway, processors have come far, and Envoy and Linkerd are probably some of the fastest TCP proxies you can get. That said, there are always the sensitive folks that balk at inserting such latency. I thought it was enlightening that in the test conducted in the article cited above, they\u2019ve added more latency and steps, but because they\u2019re also adding intelligence, they\u2019re netting an overall latency speed up!<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> The consensus seems to be that service meshes solve more problems than they create, such as latency. Are they right for your particular case? As somebody highly quoted once said, \u201cit depends.\u201d As is the case with DPI-based firewalls, these kind of traffic processing applications can have great latency and throughput with a given feature set or load, but wildly different performance by turning on certain features or under load. Not that it\u2019s a fair comparison, but the lightweight stateless processing that an SDN forwarding agent does is always going to be way faster than such proxies, especially when, like for OpenContrail, there are smart NIC vendors implementing the vRouter in hardware.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> Another area that needs more attention in terms of reliability is security. As soon as I think of a TCP proxy, my mind wonders about protecting against a DoS attack because so much state is created to track each session. A nice way that service meshes nicely solve this is through the use of TLS. While Linkerd can support this, Istio makes this even easier because of the Istio Auth controller for key management. This is a great step to not only securing traffic over the wire (which SDNs could do too with IPsec etc.), but also making strong identity-based AAA for each micro-service. It\u2019s worth noting that these proxies can change the wire protocol to anything they can configure, regardless of if it was initiated as such from the application. So an HTTP request could be sent as HTTP2 within TLS on the wire.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"> I\u2019ll cap off this section by mentioning circuit breaking. I don\u2019t know of any means that an SDN solution could do this very well without interpreting a lot of analytics and application tracing information and feeding that back into the API of the SDN. Even if that is possible in theory, service meshes already do this today as a built-in <a href=\"https:\/\/istio.io\/docs\/concepts\/traffic-management\/handling-failures.html\">feature<\/a> to gracefully handle failures instead of having them escalate.<\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><em><strong>Testing and debugging<\/strong><br \/>\n<\/em>This is an important topic, but there\u2019s not really an apples-to-apples comparison of features, so I\u2019ll just hit prominent points on this topic separately.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> Services meshes provide an application RPC-oriented view into the intercommunication in the mesh of micro-services. This information can be very useful for monitoring and ops visibility and during debugging by tracing the communication path across an application. Linkerd <a href=\"https:\/\/linkerd.io\/features\/distributed-tracing-and-instrumentation\/\">integrates<\/a> with Zipkin for tracing and other tools for metrics, and works for applications written in any language unlike some language-specific tracing libraries.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> Service meshes also provide per-request routing based on things like HTTP headers, which can be manipulated for testing. Additionally, Istio also provides fault <a href=\"https:\/\/istio.io\/docs\/concepts\/traffic-management\/fault-injection.html\">injection<\/a> to simulate blunders in the application.<\/span><span style=\"font-family: arial, helvetica, sans-serif\"> On the SDN side of things, solutions differ. OpenContrail is fairly mature in this space compared to the other choices one has with CNI providers. OpenContrail has the ability to run packet capture and sniffers like Wireshark on demand, and its comprehensive analytics engines and visibility tools expose flow records and other traffic stats. Aside from debugging (at a more of network level), there are interesting security applications for auditing ACL deny logs. Finally, OpenContrail can tell you the end-to-end path of your traffic if it\u2019s run atop of a physical network (not a cloud). All of this can potentially help debugging, but the kind of information is far more indirect vis-\u00e0-vis the applications, and is probably better suited for NetOps.<\/span><\/li>\n<\/ol>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong>Legacy and Other Interconnection<\/strong><\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Service meshes seem great in many ways, but one hitch to watch out for is how they can allow or block your micro-services connecting to your legacy services or any services that don\u2019t have a proxy in front of them.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">If you are storing state in S3 or making a call to a cloud service, that\u2019s an external call. If you\u2019re reaching back to a legacy application like an Oracle database, same deal. If you\u2019re calling an RPC of another micro-service that isn\u2019t on the service mesh (for example it\u2019s sitting in virtual machine instead of a container), same again. If your micro-service is supposed to deal with traffic that isn\u2019t TCP traffic, that too isn\u2019t going to be handled through your service mesh (for example, DNS is UDP traffic, ping is ICMP).<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">In the case of Istio, you can setup <a href=\"https:\/\/istio.io\/docs\/tasks\/egress.html\">egress<\/a> connectivity with a service alias, but that may require changes to the application, so a direct pass-thru is perhaps a simpler option. Also there are a lot of variants of TCP traffic that are not HTTP nor directly supported as higher-level protocols riding on HTTP. Common examples might be ssh and mail protocols.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">There is also the question of how service meshes will handle multiple IPs per pod and multiple network interfaces per pod once CNI soon allows it.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">You most certainly have some of this communication in your applications that doesn\u2019t quite fit the mesh. In these cases you not only need to plan how to allow this communication, but also how to do it securely, probably with an underlying SDN solution like OpenContrail that can span Kubernetes as well as OpenStack, VMware and metal.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong>What do you think?<\/strong><\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Going back to the original question in the title: Are Service Meshes the Next-Gen SDN?<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">On one hand: yes! because they\u2018re eating a lot of the value that some SDNs provided by enabling micro-segmentation and security for RPC between micro-services. Service meshes are able to do this with improved TLS-based security and identity assignment to each micro-service. Also service meshes are adding advanced application-aware load balancing and fault handling that is otherwise hard to achieve without application analytics and code refactoring.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">On the other hand: no! because service meshes sit atop of CNI and container connectivity. They ride on top of SDN, so they\u2019ll still need a solid foundation. Moreover, most teams will want multiple layers of security isolation when they can get micro-segmentation and multi-tenancy that comes with SDN solutions without any performance penalty. SDN solutions can also span connectivity across clusters, stacks and runtimes other than containers, and satisfy the latency obsessed.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Either way, service meshes are a new, cool and shiny networking toy. They offer a lot of value beyond the networking and security values that they subsume, and I think we\u2019ll soon see them in just about every micro-services architecture and stack.<\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong>More questions\u2026<\/strong><\/span><\/p>\n<p><span style=\"font-family: arial, helvetica, sans-serif\">Hopefully something in this never-ending blog makes you question SDN or the service meshes. Share your thoughts or questions. The anti-pattern of technology forecasting is thinking we\u2019re done, so some open questions:<\/span><\/p>\n<ol>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Should we mash-up service meshes and fit Linkerd into the Istio framework as an alternative to Envoy? If so, why?<\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Should we mash-up OpenContrail and service meshes and how?<\/span><\/li>\n<\/ol>\n<p><span style=\"font-family: arial, helvetica, sans-serif\"><strong>Resources on Learning About Service Meshes<\/strong><\/span><\/p>\n<ul>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Istio blog: <a href=\"https:\/\/istio.io\/blog\/\">https:\/\/istio.io\/blog\/<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Buoyant blog: <a href=\"https:\/\/blog.buoyant.io\/\">https:\/\/blog.buoyant.io\/<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Demo videos with Istio and Kubernetes thanks to Lachie:<\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\"><a href=\"https:\/\/www.youtube.com\/watch?v=ePwd5bK2Cuo&amp;list=PLbj_Bz58yLCw09JYfG2xbFMi5-jN89LfB\">https:\/\/www.youtube.com\/watch?v=ePwd5bK2Cuo&amp;list=PLbj_Bz58yLCw09JYfG2xbFMi5-jN89LfB<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Istio Service Mesh Podcast:\u00a0<a href=\"https:\/\/softwareengineeringdaily.com\/2017\/06\/27\/istio-service-mesh-with-varun-talwar-and-louis-ryan\/\" target=\"_blank\" rel=\"noopener\">https:\/\/softwareengineeringdaily.com\/2017\/06\/27\/istio-service-mesh-with-varun-talwar-and-louis-ryan\/<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Linkerd Service Mesh Podcast: <a href=\"https:\/\/softwareengineeringdaily.com\/2017\/06\/26\/service-mesh-with-william-morgan\/\" target=\"_blank\" rel=\"noopener\">https:\/\/softwareengineeringdaily.com\/2017\/06\/26\/service-mesh-with-william-morgan\/<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Scaling Twitter Podcast about Linkerd: <a href=\"https:\/\/softwareengineeringdaily.com\/2016\/06\/22\/scaling-twitter-buoyant-ios-william-morgan\/\">https:\/\/softwareengineeringdaily.com\/2016\/06\/22\/scaling-twitter-buoyant-ios-william-morgan\/<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Service Proxying Podcast about Envoy: <a href=\"https:\/\/softwareengineeringdaily.com\/2017\/02\/14\/service-proxying-with-matt-klein\/\">https:\/\/softwareengineeringdaily.com\/2017\/02\/14\/service-proxying-with-matt-klein\/<\/a><\/span><\/li>\n<li><span style=\"font-family: arial, helvetica, sans-serif\">Comparing Envoy and Linkerd: <a href=\"https:\/\/lyft.github.io\/envoy\/docs\/intro\/comparison.html#id7\">https:\/\/lyft.github.io\/envoy\/docs\/intro\/comparison.html#id7<\/a><\/span><\/li>\n<\/ul>\n<p>&nbsp;<\/p>\n<p><em><span style=\"font-family: arial, helvetica, sans-serif\">This blog was originally posted at\u00a0<a href=\"http:\/\/jameskelly.net\/blog\/2017\/6\/19\/are-service-meshes-the-next-gen-sdn\" target=\"_blank\" rel=\"noopener\">http:\/\/jameskelly.net\/blog\/2017\/6\/19\/are-service-meshes-the-next-gen-sdn<\/a>\u00a0<\/span><\/em><\/p>\n","protected":false},"excerpt":{"rendered":"<p>June 28, 2017 update: more awesome background on service meshes, proxies and Istio in particular on yet another new SE Daily podcast with Istio engineers from Google. June 26, 2017&#8230;<\/p>\n","protected":false},"author":484,"featured_media":0,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[30,16,40,17,41],"tags":[],"acf":[],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v21.6 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Are Service Meshes the Next-gen SDN? - Tungsten Fabric<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Are Service Meshes the Next-gen SDN? - Tungsten Fabric\" \/>\n<meta property=\"og:description\" content=\"June 28, 2017 update: more awesome background on service meshes, proxies and Istio in particular on yet another new SE Daily podcast with Istio engineers from Google. June 26, 2017...\" \/>\n<meta property=\"og:url\" content=\"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/\" \/>\n<meta property=\"og:site_name\" content=\"Tungsten Fabric\" \/>\n<meta property=\"article:published_time\" content=\"2017-06-20T06:03:38+00:00\" \/>\n<meta property=\"og:image\" content=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2017\/06\/mesh-1430107.png\" \/>\n<meta name=\"author\" content=\"James Kelly\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"WebPage\",\"@id\":\"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/\",\"url\":\"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/\",\"name\":\"Are Service Meshes the Next-gen SDN? - Tungsten Fabric\",\"isPartOf\":{\"@id\":\"https:\/\/tungsten.io\/#website\"},\"datePublished\":\"2017-06-20T06:03:38+00:00\",\"dateModified\":\"2017-06-20T06:03:38+00:00\",\"author\":{\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/8c323ac79e67fd8b0b6c77dd82df0030\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/\"]}]},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/tungsten.io\/#website\",\"url\":\"https:\/\/tungsten.io\/\",\"name\":\"Tungsten Fabric\",\"description\":\"multicloud multistack SDN\",\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/tungsten.io\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"Person\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/8c323ac79e67fd8b0b6c77dd82df0030\",\"name\":\"James Kelly\",\"image\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-US\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/image\/\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/69dcddda0d46307fc09706c1811c5c63?s=96&d=mm&r=pg\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/69dcddda0d46307fc09706c1811c5c63?s=96&d=mm&r=pg\",\"caption\":\"James Kelly\"},\"url\":\"https:\/\/tungsten.io\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"Are Service Meshes the Next-gen SDN? - Tungsten Fabric","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/","og_locale":"en_US","og_type":"article","og_title":"Are Service Meshes the Next-gen SDN? - Tungsten Fabric","og_description":"June 28, 2017 update: more awesome background on service meshes, proxies and Istio in particular on yet another new SE Daily podcast with Istio engineers from Google. June 26, 2017...","og_url":"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/","og_site_name":"Tungsten Fabric","article_published_time":"2017-06-20T06:03:38+00:00","og_image":[{"url":"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2017\/06\/mesh-1430107.png"}],"author":"James Kelly","twitter_card":"summary_large_image","schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"WebPage","@id":"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/","url":"https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/","name":"Are Service Meshes the Next-gen SDN? - Tungsten Fabric","isPartOf":{"@id":"https:\/\/tungsten.io\/#website"},"datePublished":"2017-06-20T06:03:38+00:00","dateModified":"2017-06-20T06:03:38+00:00","author":{"@id":"https:\/\/tungsten.io\/#\/schema\/person\/8c323ac79e67fd8b0b6c77dd82df0030"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/tungsten.io\/are-service-meshes-the-next-gen-sdn\/"]}]},{"@type":"WebSite","@id":"https:\/\/tungsten.io\/#website","url":"https:\/\/tungsten.io\/","name":"Tungsten Fabric","description":"multicloud multistack SDN","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/tungsten.io\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"Person","@id":"https:\/\/tungsten.io\/#\/schema\/person\/8c323ac79e67fd8b0b6c77dd82df0030","name":"James Kelly","image":{"@type":"ImageObject","inLanguage":"en-US","@id":"https:\/\/tungsten.io\/#\/schema\/person\/image\/","url":"https:\/\/secure.gravatar.com\/avatar\/69dcddda0d46307fc09706c1811c5c63?s=96&d=mm&r=pg","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/69dcddda0d46307fc09706c1811c5c63?s=96&d=mm&r=pg","caption":"James Kelly"},"url":"https:\/\/tungsten.io"}]}},"_links":{"self":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/7533"}],"collection":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/users\/484"}],"replies":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/comments?post=7533"}],"version-history":[{"count":0,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/7533\/revisions"}],"wp:attachment":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/media?parent=7533"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/categories?post=7533"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/tags?post=7533"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}