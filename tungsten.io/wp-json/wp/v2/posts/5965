{"id":5965,"date":"2015-02-12T16:05:54","date_gmt":"2015-02-13T00:05:54","guid":{"rendered":"http:\/\/www.opencontrail.org\/?p=5965"},"modified":"2015-02-12T16:05:54","modified_gmt":"2015-02-13T00:05:54","slug":"integrating-vmware-esxi-with-openstack-opencontrail","status":"publish","type":"post","link":"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/","title":{"rendered":"Integrating VMware ESXi with OpenStack &#038; OpenContrail"},"content":{"rendered":"<p><strong><em>This is \u00a0a guest blog by Jakub Pavlik, tcpCloud. To see\u00a0the original post, <a href=\"http:\/\/tcpcloud.eu\/en\/blog\/2015\/02\/08\/intergrating-vmware-esxi-openstack-opencontrail\/\">click here<\/a>.<\/em><\/strong><\/p>\n<p>As OpenStack Integrators, we often have discussions with customers regarding interoperability of VMware and KVM virtualization on daily basis. We usually answer questions like:<\/p>\n<ul class=\"simple\">\n<li>Can we use KVM for production? Is it not just for experimental purposes?<\/li>\n<li>How to integrate current VMware vSphere environment?<\/li>\n<li>Is it possible to use hybrid environment KVM together with VMware?<\/li>\n<li>What limitations and obstacles we get with hybrid environment?<\/li>\n<\/ul>\n<p>Today I would like to try answers regarding networking on hybrid hypervisor environment. Vanilla Neutron implementation using openVSwitch cannot be used with VMware infrastructure. Only Nova vCenter driver (ESXi driver deprecated in Icehouse release) for compute exists, Cinder VMDK driver for volumes. For the networking virtualization only Nova flat networks can be used. Of course, there is an effort to provide at least Neutron VLAN driver, but the development is done outside the OpenStack community. The reason is that VMware forces customer to buy its SDN solution VMware NSX and use Neutron NSX plugin. This is not open source solution and it is very hard to get it for testing. Therefore the only way how to get the SDN features and stay in open source world (with the possibility of commercial support) is OpenContrail, which introduces the VMware ESXi support in new release 2.0.<\/p>\n<p>This first implementation has limited capability of Contrail compute node functionality at hypervisors running the VMware ESXi virtualization platform. Next releases will provide vCenter driver with Live Migration, DRS, and other VMware functions. Currently Contrail supports only the standard VMware vSwitches and port groups in ESXi. Distributed vSwitch functionality will be supported by the vCenter plugin too. Today the most important thing is to verify functions and measure real performance.<\/p>\n<p>To run OpenContrail on ESXi 5.5, virtual machine is spawned on a physical ESXi server and compute node is configured on that virtual machine. For OpenContrail on ESXi, only compute node functionality is provided.<\/p>\n<p>The Contrail compute virtual machine can be downloaded as VMDK from Juniper website. Contrail ESXi VM for Ubuntu 12.04.3 LTS is a standard ubuntu image with few modifications:<\/p>\n<ul class=\"simple\">\n<li>Python bit string library<\/li>\n<li>Script to set hostname from DHCP (dhclient-exit-hooks)<\/li>\n<li>Disk size is set 60GB that can hold Contrail packages repository<\/li>\n<\/ul>\n<div id=\"architectural-scenario\" class=\"section\">\n<h4>ARCHITECTURAL SCENARIO<\/h4>\n<p>We show how to deploy hybrid KVM&amp;VMware OpenStack cloud through Contrail fabric deploy scripts. We dedicated two physical IBM HS22V blade servers with two 10Gbps NICs for this purpose.<\/p>\n<\/div>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter wp-image-5966\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/esxi-kvm-network.png\" alt=\"esxi-kvm-network\" width=\"647\" height=\"300\" data-id=\"5966\" \/><\/p>\n<div id=\"architectural-scenario\" class=\"section\">\n<ul class=\"simple\">\n<li>OpenStack and Contrail Controller &#8211; 10.0.102.211<\/li>\n<li>KVM Compute &#8211; 10.0.102.215<\/li>\n<li>Contrail Compute VM &#8211; 10.0.102.214<\/li>\n<li>ESXi Management IP &#8211; 10.0.103.234<\/li>\n<\/ul>\n<p>We use the <cite>MPLSoverGRE<\/cite> for overlay encapsulation.<\/p>\n<p>Our performance testings scenario covers:<\/p>\n<ol class=\"arabic simple\">\n<li>Deployment and installation<\/li>\n<li>Functional testing<\/li>\n<li>Performance testing<\/li>\n<\/ol>\n<\/div>\n<div id=\"testing-scenario-deployment\" class=\"section\">\n<h4>TESTING SCENARIO DEPLOYMENT<\/h4>\n<p>Following list covers the deployment process:<\/p>\n<ul class=\"arabic simple\" style=\"list-style-type: circle;\">\n<li>Download <cite>contrail-install-packages_2.0-22~icehouse_all.deb<\/cite> from <a class=\"reference external\" href=\"http:\/\/www.juniper.net\/support\/downloads\/?p=contrail#sw\">http:\/\/www.juniper.net\/support\/downloads\/?p=contrail#sw<\/a> and copy it to <cite>\/tmp\/<\/cite> on the first server for further system installation.<\/li>\n<li>Install package through<\/li>\n<\/ul>\n<pre><span style=\"font-family: 'courier new', courier;\">dpkg -i \/tmp\/contrail-install-packages_2.0-22~icehouse_all.deb<\/span><\/pre>\n<p>Run the setup.sh script. This step will create the Contrail packages repository as well as the Fabric utilities needed for provisioning:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">cd \/opt\/contrail\/contrail_packages .\/setup.sh\n<\/span><\/pre>\n<p>Fill in the testbed definitions file.<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">from fabric.api import env\n\n#Management ip addresses of hosts in the cluster\n#Controller\nhost1 = 'root@10.0.102.211'\n#Contrail-compute-vm\nhost2 = 'root@10.0.102.214'\n#KVM compute node\nhost3 = \u2018root@10.0.102.215\u2019\n\n#External routers if any\next_routers = []\n\n#Autonomous system number\nrouter_asn = 64512\n\n#Host from which the fab commands are triggered to install and provision\nhost_build = 'root@10.0.102.211'\n\n#Role definition of the hosts.\nenv.roledefs = {\n'all': [host1,host2,host3],\n'cfgm': [host1],\n'openstack': [host1],\n'control': [host1],\n'compute': [host1,host2,host3],\n'collector': [host1],\n'webui': [host1],\n'database': [host1],\n'build': [host_build],\n'storage-master': [host1],\n'storage-compute': [host1],\n# 'vgw': [host1], # Optional, Only to enable VGW. Only compute can support vgw\n# 'backup':[backup_node], # only if the backup_node is defined\n}\n\n#Openstack admin password\nenv.openstack_admin_password = 'password'\n#Hostnames\nenv.hostnames = {\n'all': ['a0s1','contrail-vm-compute2']\n}\n\nenv.password = 'password'\n#Passwords of each host\nenv.passwords = {\nhost1: 'password',\nhost2: 'c0ntrail123',\nhost3: \u2018password\u2019,\n# backup_node: 'secret',\nhost_build: 'password',\n}\n\n#For reimage purpose\nenv.ostypes = {\nhost1:'ubuntu',\nhost3:\u2019ubuntu\u2019,\n}\n\n#Following are ESXi Hypervisor details.\nesxi_hosts = {\n\n#Ip address of Hypervisor\n'esxi_host1' : {'ip': '10.0.103.234',\n\n#Username and password of ESXi Hypervisor\n'username': 'root',\n'password':\n\n#Uplink port of Hypervisor through which it is connected to external world\n'uplink_nic': 'vmnic2',\n\n#Vswitch on which above uplinc exists\n'fabric_vswitch' : 'vSwitch0',\n\n#Port group on 'fabric_vswitch' through which ContrailVM connects to external world\n'fabric_port_group' : 'VLAN1502',\n\n#Vswitch name to which all openstack virtual machine's are hooked to 'vm_vswitch': 'vSwitch1',\n\n#Port group on 'vm_vswitch', which is a member of all vlans, to which ContrailVM is connected to all openstack VM's\n'vm_port_group' : 'VMvlanCloud',\n\n#links 'host2' ContrailVM to esxi_host1 hypervisor\n'contrail_vm' : {\n'name' : 'contrail-vm-compute2', # Name for the contrail-compute-vm,\n'mac' : '00:50:56:8c:c6:85', # VM's eth0 mac address, same should be configured on DHCP server\n'host' : host2, # host string for VM, as specified in the env.rolesdef['compute']\n'vmdk' : '\/opt\/ESXi-v5.5-Contrail-host-Ubuntu-precise-12.04.3-LTS.vmdk', # local path for the VMDK file\n\n},\n\n# Another ESXi hypervisor follows\n},\n}<\/span><\/pre>\n<p>The fab prov_esxi task configures the ESXi hypervisor (ssh root must be permitted) and launches the Contrail compute VM. This task creates or verify two vSwitches, port groups and deploys VM with two NICs connected to these two vSwitches. Installation can be also verified in vSphere client, where you should see two vSwitches.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\" wp-image-5967 alignnone\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/contrail-compute-vm.png\" alt=\"contrail-compute-vm\" width=\"650\" height=\"330\" data-id=\"5967\" \/><\/p>\n<p>Following screens show newly created vSwitches and Port Groups. We use ESXi for other purposes as well, so there are more Port Groups.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter size-full wp-image-5968\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/vSwitch0.png\" alt=\"vSwitch0\" width=\"434\" height=\"267\" data-id=\"5968\" \/><\/p>\n<p>After installation vSwitch1 contains only one contrail-compute-vm. This screen was captured after instances were provisioned.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"aligncenter size-full wp-image-5969\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/vSwitch1.png\" alt=\"vSwitch1\" width=\"493\" height=\"359\" data-id=\"5969\" \/><\/p>\n<p>&nbsp;<\/p>\n<p>If you do not have DHCP server in your network, you need to set static IP address inside of contrail-compute-vm. Default password for root user is <cite>c0ntrail123<\/cite>.<\/p>\n<p>When the contrail-compute-vm is ready, you can continue with standard Contrail installation commands:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">fab install_pkg_all:\/tmp\/&lt;pkg&gt;\nfab install_contrail\nfab setup_all<\/span><\/pre>\n<p>You can verify the VMware configuration at contrail-compute-vm. Look at Nova service configuration at \/etc\/nova\/nova.conf.<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">[DEFAULT]\n\u2026\ncompute_driver = vmwareapi.ContrailESXDriver\n\u2026\n\n[vmware]\nhost_ip = 10.0.103.234\nhost_username = root\nhost_password = cloudlab\nvmpg_vswitch = vSwitch1\n\u2026\n\n\/etc\/contrail\/contrail-vrouter-agent.conf\n\u2026\n\n[HYPERVISOR]\n# Everything in this section is optional\n\n# Hypervisor type. Possible values are kvm, xen and vmware type=vmware\nmode=\n\u2026\n\n# Physical interface name when hypervisor type is vmware vmware_physical_interface=eth1\n\u2026<\/span><\/pre>\n<p>Now we can start with actual performance testing the newly deployed hybrid infrastructure.<\/p>\n<h4>FUNCTIONAL TESTING<\/h4>\n<p>At this point, we have created one virtual network <cite>net1<\/cite> with address range 192.168.6.0\/24 in OpenStack. Before the actual provisioning, we have created two availability zones based on hypervisor type. We have provisioned several VMs to different availability zones\/hypervisors.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5970\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/hostagregates.png\" alt=\"hostagregates\" width=\"650\" height=\"179\" data-id=\"5970\" \/><\/p>\n<p>Nova service status can be verified through the command nova-manage service list.<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">root@contrail:~# nova-manage service list \nBinary   \t   Host \t        Zone       Status   State \t\tUpdated_At \nnova-scheduler \t   contrail   \t        internal   enabled  :-) \t\t2015-02-08 21:47:55 \nnova-console   \t   contrail             internal   enabled  :-) \t\t2015-02-08 21:47:55 \nnova-consoleauth   contrail             internal   enabled  :-) \t\t2015-02-08 21:47:55 \nnova-conductor \t   contrail             internal   enabled  :-) \t\t2015-02-08 21:47:55 \nnova-compute \t   contrail-kvm         kvm \t   enabled  :-) \t\t2015-02-08 21:47:55 \nnova-compute \t   contrail-vm-compute2 vmware     enabled  :-) \t\t2015-02-08 21:47:52 <\/span><\/pre>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5971\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/instances.png\" alt=\"instances\" width=\"650\" height=\"233\" data-id=\"5971\" \/><\/p>\n<p>VMware ESXi compute driver creates instances with its UUID as name.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\" size-full wp-image-5972 alignnone\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/ESXiVMs.png\" alt=\"ESXiVMs\" width=\"211\" height=\"282\" data-id=\"5972\" \/><\/p>\n<p>Following screen shows detail of Ubuntu server instance at ESXi hypervisor.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5973\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/ESXIinstance.png\" alt=\"ESXIinstance\" width=\"650\" height=\"294\" data-id=\"5973\" \/><\/p>\n<p>OpenContrail Dashboard shows contrail-compute-vm as a standard vRouter node.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5974\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/opencontraildashboard.png\" alt=\"opencontraildashboard\" width=\"650\" height=\"270\" data-id=\"5974\" \/><\/p>\n<p>This screen shows that standard OpenStack user cannot see any difference between instances running ESXi or KVM virtualization. We are able to ping virtual machines running on different hypervisors, that are connected to the same overlay network.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5975\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/PingFromVMware.png\" alt=\"PingFromVMware\" width=\"650\" height=\"276\" data-id=\"5975\" \/><\/p>\n<p>Administrator user can also use vSphere console to see the very same virtual machine console output.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5976\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/vmwareconsole.png\" alt=\"vmwareconsole\" width=\"650\" height=\"232\" data-id=\"5976\" \/><\/p>\n<p>Following images show the routes at both vRouters in OpenContrail dashboard.<\/p>\n<p>Routes at the ESXi hypervisor:<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5977\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/routesESXI.png\" alt=\"routesESXI\" width=\"650\" height=\"342\" data-id=\"5977\" \/><\/p>\n<p>Routes at the KVM hypervisor:<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5978\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/routesKVM.png\" alt=\"routesKVM\" width=\"650\" height=\"348\" data-id=\"5978\" \/><\/p>\n<h4>PERFORMANCE TESTING<a class=\"headerlink\" title=\"Permalink to this headline\" href=\"http:\/\/tcpcloud.eu\/en\/blog\/2015\/02\/08\/intergrating-vmware-esxi-openstack-opencontrail\/#performance-testing\">\u00b6<\/a><\/h4>\n<p>The most important and interesting part is the performance throughput measurements between VMware ESXi and KVM hypervisors. The amazing near the line bandwidth (9.6Gbit at 10Gbps links) of OpenContrail is well known, but how fast is the VMware contrail-compute-vm? We decided to use <cite>iperf<\/cite> for the network testing. <cite>Iperf<\/cite> is a tool to measure maximal TCP bandwidth, allowing the tuning of various TCP\/UDP parameters and characteristics. <cite>Iperf<\/cite> reports bandwidth, delay jitter, datagram loss and other useful information.<\/p>\n<ul class=\"arabic simple\" style=\"list-style-type: circle;\">\n<li>UbuntuESXi instance is running at ESXi hypervisor with IP address 192.168.6.5. We start the <cite>iperf<\/cite> server against which we will conduct the tests.<\/li>\n<\/ul>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone wp-image-5979\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/iperf01.png\" alt=\"iperf01\" width=\"650\" height=\"276\" data-id=\"5979\" \/><\/p>\n<ul class=\"arabic simple\" style=\"list-style-type: circle;\">\n<li>CentosKVM is the second instance and running at KVM hypervisor with IP address 192.168.6.6.<\/li>\n<\/ul>\n<p>The first measurement is one session from centoskvm to ubuntuesxi.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-5980\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/iperf06.png\" alt=\"iperf06\" width=\"722\" height=\"132\" data-id=\"5980\" \/><\/p>\n<p>As you can see form screenshot, the network throughput was <strong>3.03Gbits\/sec<\/strong> without any special performance tuning.<\/p>\n<p>3.The next test launches 5 parallel sessions in dual testing mode for 30 seconds.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-5981\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/iperf04.png\" alt=\"iperf04\" width=\"719\" height=\"304\" data-id=\"5981\" \/><\/p>\n<p>The final test launches 5 parallel sessions in dual testing mode for 30 seconds.<\/p>\n<p><img loading=\"lazy\" decoding=\"async\" class=\"alignnone size-full wp-image-5982\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/iperf.png\" alt=\"iperf\" width=\"625\" height=\"386\" data-id=\"5982\" \/><\/p>\n<div id=\"performance-testing\" class=\"section\">\n<p>You can see that the maximum bandwidth reached is about <strong>3Gbits\/sec<\/strong>.<\/p>\n<\/div>\n<div id=\"conclusion\" class=\"section\">\n<h4>CONCLUSION<\/h4>\n<p>We proved that hybrid hypervisor environment is possible to build and it enables quite good performance out of box. The bandwidth of 3Gbits\/sec is similar to what other SDN solutions like VMware NSX or PlumGrid can achieve. These solutions also use virtual machines as gateways (south-nord traffic) for OpenStack Neutron.<\/p>\n<p>Future release will support vCenter with advanced VMware features and possibility to integrate it with ToR OVSDB switches through VXLAN encapsulation. At that case there will be no need to have dedicated service virtual machine at each ESXi.<\/p>\n<p><strong>Jakub Pavlik<\/strong><br \/>\n<strong>tcp cloud platform engineer<\/strong><\/p>\n<\/div>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n<p>&nbsp;<\/p>\n<\/div>\n","protected":false},"excerpt":{"rendered":"<p>This is \u00a0a guest blog by Jakub Pavlik, tcpCloud. To see\u00a0the original post, click here. As OpenStack Integrators, we often have discussions with customers regarding interoperability of VMware and KVM&#8230;<\/p>\n","protected":false},"author":479,"featured_media":0,"comment_status":"open","ping_status":"open","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[11,1],"tags":[],"acf":[],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v21.6 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Integrating VMware ESXi with OpenStack &amp; OpenContrail - Tungsten Fabric<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Integrating VMware ESXi with OpenStack &amp; OpenContrail - Tungsten Fabric\" \/>\n<meta property=\"og:description\" content=\"This is \u00a0a guest blog by Jakub Pavlik, tcpCloud. To see\u00a0the original post, click here. As OpenStack Integrators, we often have discussions with customers regarding interoperability of VMware and KVM...\" \/>\n<meta property=\"og:url\" content=\"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/\" \/>\n<meta property=\"og:site_name\" content=\"Tungsten Fabric\" \/>\n<meta property=\"article:published_time\" content=\"2015-02-13T00:05:54+00:00\" \/>\n<meta property=\"og:image\" content=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/esxi-kvm-network.png\" \/>\n<meta name=\"author\" content=\"Jakub Pavlik\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"WebPage\",\"@id\":\"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/\",\"url\":\"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/\",\"name\":\"Integrating VMware ESXi with OpenStack & OpenContrail - Tungsten Fabric\",\"isPartOf\":{\"@id\":\"https:\/\/tungsten.io\/#website\"},\"datePublished\":\"2015-02-13T00:05:54+00:00\",\"dateModified\":\"2015-02-13T00:05:54+00:00\",\"author\":{\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/\"]}]},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/tungsten.io\/#website\",\"url\":\"https:\/\/tungsten.io\/\",\"name\":\"Tungsten Fabric\",\"description\":\"multicloud multistack SDN\",\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/tungsten.io\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"Person\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75\",\"name\":\"Jakub Pavlik\",\"image\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-US\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/image\/\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg\",\"caption\":\"Jakub Pavlik\"},\"url\":\"https:\/\/tungsten.io\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"Integrating VMware ESXi with OpenStack & OpenContrail - Tungsten Fabric","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/","og_locale":"en_US","og_type":"article","og_title":"Integrating VMware ESXi with OpenStack & OpenContrail - Tungsten Fabric","og_description":"This is \u00a0a guest blog by Jakub Pavlik, tcpCloud. To see\u00a0the original post, click here. As OpenStack Integrators, we often have discussions with customers regarding interoperability of VMware and KVM...","og_url":"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/","og_site_name":"Tungsten Fabric","article_published_time":"2015-02-13T00:05:54+00:00","og_image":[{"url":"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2015\/02\/esxi-kvm-network.png"}],"author":"Jakub Pavlik","twitter_card":"summary_large_image","schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"WebPage","@id":"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/","url":"https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/","name":"Integrating VMware ESXi with OpenStack & OpenContrail - Tungsten Fabric","isPartOf":{"@id":"https:\/\/tungsten.io\/#website"},"datePublished":"2015-02-13T00:05:54+00:00","dateModified":"2015-02-13T00:05:54+00:00","author":{"@id":"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/tungsten.io\/integrating-vmware-esxi-with-openstack-opencontrail\/"]}]},{"@type":"WebSite","@id":"https:\/\/tungsten.io\/#website","url":"https:\/\/tungsten.io\/","name":"Tungsten Fabric","description":"multicloud multistack SDN","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/tungsten.io\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"Person","@id":"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75","name":"Jakub Pavlik","image":{"@type":"ImageObject","inLanguage":"en-US","@id":"https:\/\/tungsten.io\/#\/schema\/person\/image\/","url":"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg","caption":"Jakub Pavlik"},"url":"https:\/\/tungsten.io"}]}},"_links":{"self":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/5965"}],"collection":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/users\/479"}],"replies":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/comments?post=5965"}],"version-history":[{"count":0,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/5965\/revisions"}],"wp:attachment":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/media?parent=5965"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/categories?post=5965"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/tags?post=5965"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}