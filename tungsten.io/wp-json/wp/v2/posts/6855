{"id":6855,"date":"2015-11-11T23:06:25","date_gmt":"2015-11-12T07:06:25","guid":{"rendered":"http:\/\/www.opencontrail.org\/?p=6855"},"modified":"2015-11-11T23:06:25","modified_gmt":"2015-11-12T07:06:25","slug":"installing-kubernetes-opencontrail","status":"publish","type":"post","link":"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/","title":{"rendered":"Installing Kubernetes &#038; Opencontrail"},"content":{"rendered":"<p>In this post we walk through the steps required to install a 2 node cluster running Kubernetes that uses OpenContrail as the network provider. In addition to the 2 compute nodes, we use a master and a gateway node. The master runs both the kubernetes api server and scheduler as well as the opencontrail configuration management and control plane.<\/p>\n<p>OpenContrail implements an overlay network using standards based network protocols:<\/p>\n<ul>\n<li><a href=\"https:\/\/tools.ietf.org\/html\/rfc4364\">BGP\/MPLS L3VPN<\/a> is used as the control-plane;<\/li>\n<li><a href=\"https:\/\/tools.ietf.org\/html\/rfc7510\">MPLS over UDP<\/a> or <a href=\"https:\/\/tools.ietf.org\/html\/rfc4023\">MPLS over GRE<\/a> are used as encapsulations;<\/li>\n<\/ul>\n<p>This means that, in production environments, it is possible to use existing network appliances from multiple vendors that can serve as the gateway between the un-encapsulated network (a.k.a. underlay) and the network overlay. However for the purposes of a test cluster we will use an extra node (the gateway) whose job is to provide access between the underlay and overlay networks.<\/p>\n<p>For this exercise, I decided to use my MacBookPro which has 16G of RAM. However all the tools used are supported on Linux also; it should be relativly simple to reproduce the same steps on a Linux machine or on a cloud such as AWS or GCE.<\/p>\n<p>The first step in the process is to obtain binaries for kubernetes <a href=\"https:\/\/github.com\/kubernetes\/kubernetes\/releases\/download\/v1.1.1\/kubernetes.tar.gz\">release-1.1.1<\/a>. I then unpacked the tar file into the <code>~\/tmp<\/code> and then extracted the linux binaries required to run the cluster using the command:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">cd ~\/tmp;tar zxvf kubernetes\/server\/kubernetes-server-linux-amd64.tar.gz<\/span><\/pre>\n<p>In order to create the 4 virtual-machines required for this scenario I used virtual-box and vagrant. Both are trivial to install on OSX.<\/p>\n<p>In order to provision the virtual-machines we use ansible. Ansible can be installed via \u201cpip install ansible\u201d. I then created a default ansible.cfg that enables the pipelining option and disables ssh connection sharing. The later was required to work around failures on tasks that use \u201cdelegate_to\u201d and run concurrently (i.e. run_once is false). From a cursory internet search, it appears that the openssh server that ships with ubuntu 14.04 has a concurrency issue when handling multi-session.<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">\n~\/.ansible.cfg\n[default]\npipelining=True\n \n[ssh_connection]\nssh_args = -o ControlMaster=no -o ControlPersist=60s<\/span><\/pre>\n<p>With ansible and vagrant installed, we can proceed to create the VMs used by this testbed. The vagrant configuration for this example is available in <a href=\"https:\/\/github.com\/pedro-r-marques\/examples\/tree\/master\/vagrant\">github<\/a>. The <code>servers.yaml<\/code> file lists the names and resource requirements for the 4 VMs. Please note that if you are adjusting this example to run in a different vagrant provider the Vagrantfile needs to be edited to specify the resource requirements for that provider.<br \/>\nAfter checking out this directory (or copying over the files) the VMs can be created by executing the command:<\/p>\n<pre><code>vagrant up<\/code><\/pre>\n<p>Vagrant will automatically execute<\/p>\n<pre><code>config.yaml<\/code><\/pre>\n<p>which will configure the hostname on the VMs.<\/p>\n<p>The Vagranfile used int this example will cause vagrant to create VMs\u00a0with 2 interfaces: a NAT interface (eth0) used for by the ssh\u00a0management sessions and external access and a private network<br \/>\ninterface (eth1) providing a private network between the host and the\u00a0VMs. OpenContrail will use the private network interface; the\u00a0management interface is optional and may not exist in other<br \/>\nconfigurations (e.g. AWS, GCE).<\/p>\n<p>After <code>vagrant up<\/code> completes, it is useful to add entries to \/etc\/hosts on all the VMs so that names can be resolved. For this purpose i used another ansible script invoked as:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">ansible-playbook -u vagrant -i .vagrant\/provisioners\/ansible\/inventory\/vagrant_ansible_inventory resolution.yaml<\/span><\/pre>\n<p>This step must be executed independently of the ansible configuration performed by vagrant since vagrant invokes ansible for each VM at a time, while this playbook expects to be invoked for all hosts.<\/p>\n<p>The command above dependens on the inventory file that vagrant creates automatically when configuring the VMs. We will use the contents of this inventory file in order to provision kubernetes and OpenContrail also.<\/p>\n<p>With the VMs running, we need to checkout the ansible playbooks that configure kubernetes + opencontrail. While an earlier version of the playbook is available upstream in the kubernetes <a href=\"https:\/\/github.com\/kubernetes\/contrib\">contrib<\/a> repository, the most recent version of the playbook is in a development <a href=\"https:\/\/github.com\/pedro-r-marques\/contrib\/tree\/opencontrail\">branch<\/a> on a fork of that repository. Checkout the repository via:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">git clone https:\/\/github.com\/pedro-r-marques\/contrib\/tree\/opencontrail<\/span><\/pre>\n<p>The branch HEAD commit id, at the time of this post, is\u00a0<a class=\"commit-tease-sha\" href=\"https:\/\/github.com\/pedro-r-marques\/contrib\/commit\/15ddfd531d568ae3eedb026834f78ff1f081851a\">15ddfd5<\/a>.<\/p>\n<p>I will work to upstream the updated opencontrail playbook to both the kubernetes and openshift provisioning repositories as soon as possible.<\/p>\n<p>With the ansible playbook available on the contrib\/ansible directory it is necessary to edit the file ansible\/group_vars\/all.yml replace the network provider:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\"># Network implementation (flannel|opencontrail)\nnetworking: opencontrail<\/span><\/pre>\n<p>We then need to create an inventory file:<\/p>\n<pre><span style=\"font-family: 'courier new', courier;\">[opencontrail:children]\nmasters\nnodes\ngateways\n \n[opencontrail:vars]\nlocalBuildOutput=\/Users\/roque\/src\/golang\/src\/k8s.io\/kubernetes\/_output\/dockerized\nopencontrail_public_subnet=100.64.0.0\/16\nopencontrail_interface=eth1\n \n[masters]\nk8s-master ansible_ssh_user=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_private_key_file=\/Users\/roque\/k8s-provision\/.vagrant\/machines\/k8s-master\/virtualbox\/private_key\n \n[etcd]\nk8s-master ansible_ssh_user=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=2222 ansible_ssh_private_key_file=\/Users\/roque\/k8s-provision\/.vagrant\/machines\/k8s-master\/virtualbox\/private_key\n \n[gateways]\nk8s-gateway ansible_ssh_user=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=2200 ansible_ssh_private_key_file=\/Users\/roque\/k8s-provision\/.vagrant\/machines\/k8s-gateway\/virtualbox\/private_key\n \n[nodes]\nk8s-node-01 ansible_ssh_user=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=2201 ansible_ssh_private_key_file=\/Users\/roque\/k8s-provision\/.vagrant\/machines\/k8s-node-01\/virtualbox\/private_key\nk8s-node-02 ansible_ssh_user=vagrant ansible_ssh_host=127.0.0.1 ansible_ssh_port=<\/span><\/pre>\n<p>This inventory file does the following:<\/p>\n<ul>\n<li>Declares that hosts for the roles: masters, gateways, etcd, nodes;The ssh information is derived from the inventory created by vagrant.<\/li>\n<li>Declares the location of the kubernetes binaries downloaded from the github release;<\/li>\n<li>Defines the IP address prefix used for \u2018External IPs\u2019 by kubernetes services that require external access;<\/li>\n<li>Instructs opencontrail to use the private network interface (eth1); without this setting the opencontrail playbook defaults to eth0.<\/li>\n<\/ul>\n<p>Once this file is created, we can execute the ansible playbook by running the script<code>\"setup.sh\"<\/code> in the contrib\/ansible directory.<\/p>\n<p>This script will run through all the steps required to provision kubernetes and opencontrail; it is not unusual for the script to fail to perform some of network based operations (downloading the repository keys for docker for instance or downloading a file from github); the ansible playbook is ment to be declarative (i.e. define the end state of the system) and it is supposed to be re-run if a network based failure is encountered.<\/p>\n<p>At the end of the script we should be able to login to the master via the command \u201cvagrant ssh k8s-master\u201d and observe the following:<\/p>\n<ul>\n<li><code>kubectl get nodes<\/code><br \/>\nThis should show two nodes: k8s-node-01 and k8s-node-02.<\/li>\n<li><code>kubectl --namespace=kube-system get pods<\/code>This command should show that the kube-dns pod is running; if this pod is in a restart loop that usually means that the kube2sky container is not able to reach the kube-apiserver.<\/li>\n<li><code>curl <a href=\"http:\/\/localhost:8082\/virtual-networks\" rel=\"nofollow\">http:\/\/localhost:8082\/virtual-networks<\/a> | python -m json.tool<\/code>This should display a list of virtual-networks created in the opencontrail api<\/li>\n<li><code>netstat -nt | grep 5269<\/code><br \/>\nWe expect 3 established TCP sessions for the control channel (xmpp) between the master and the nodes\/gateway.<\/li>\n<\/ul>\n<p>On the host (OSX) one should be able to access the diagnostic web interface of the vrouter agent running on the compute nodes:<\/p>\n<ul>\n<li><a href=\"http:\/\/192.168.1.3:8085\/Snh_ItfReq\" rel=\"nofollow\">http:\/\/192.168.1.3:8085\/Snh_ItfReq<\/a><\/li>\n<li><a href=\"http:\/\/192.168.1.4:8085\/Snh_ItfReq\" rel=\"nofollow\">http:\/\/192.168.1.4:8085\/Snh_ItfReq<\/a><\/li>\n<\/ul>\n<p>These commands show display the information regarding the interfaces attached to each pod.<\/p>\n<p>Once the cluster is operational, one can start an example application such as \u201cguestbook-go\u201d. This example can be found in the kubernetes examples directory. In order for it to run successfully the following modifications are necessary:<\/p>\n<ul>\n<ul>\n<li>Edit guestbook-controller.json, in order to add the labels \u201cname\u201d and \u201cuses\u201d as in:<\/li>\n<\/ul>\n<\/ul>\n<pre><span style=\"font-family: 'courier new', courier;\">\n\"spec\":{\n  [...]\n  \"template\":{\n    \"metadata\":{\n      \"labels\":{\n        \"app\":\"guestbook\",\n        \"name\":\"guestbook\",\n        \"uses\":\"redis\"\n      }\n    },\n  [...]\n}<\/span><\/pre>\n<ul>\n<ul>\n<li>Edit redis-master-service.json and redis-slave-service.json in order to add a service name. The following is the configuration for the master:<\/li>\n<\/ul>\n<\/ul>\n<pre><span style=\"font-family: 'courier new', courier;\">\"metadata\": {\n  [...]\n  \"labels\" {\n         \"app\":\"redis\",\n         \"role\": \"master\",\n         \"name\":\"redis\"\n  }\n}<\/span><\/pre>\n<ul>\n<li>Edit redis-master-controller.json and redis-slave-controller.json in order to add the \u201cname\u201d label to the pods. As in:\n<pre><span style=\"font-family: 'courier new', courier;\">\"spec\":{\n   [...]\n   \"template\":{\n      \"metadata\":{\n         \"labels\":{\n            \"app\":\"redis\",\n            \"role\":\"master\",\n            \"name\":\"redis\"\n         }\n      },\n   [...]\n }<\/span><\/pre>\n<\/li>\n<\/ul>\n<p>After the example is started the guestbook service will be allocated an ExternalIP on the external subnet (e.g. 100.64.255.252).<\/p>\n<p>In order to access the external IP network from the host one needs to add a route to 192.168.1.254 (the gateway address). Once that is done you should be able to access the application via a web browser via <a href=\"http:\/\/100.64.255.252:3000\/\" rel=\"nofollow\">http:\/\/100.64.255.252:3000<\/a>.<\/p>\n","protected":false},"excerpt":{"rendered":"<p>In this post we walk through the steps required to install a 2 node cluster running Kubernetes that uses OpenContrail as the network provider. In addition to the 2 compute&#8230;<\/p>\n","protected":false},"author":458,"featured_media":0,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[16,17,11],"tags":[],"acf":[],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v21.6 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Installing Kubernetes &amp; Opencontrail - Tungsten Fabric<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Installing Kubernetes &amp; Opencontrail - Tungsten Fabric\" \/>\n<meta property=\"og:description\" content=\"In this post we walk through the steps required to install a 2 node cluster running Kubernetes that uses OpenContrail as the network provider. In addition to the 2 compute...\" \/>\n<meta property=\"og:url\" content=\"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/\" \/>\n<meta property=\"og:site_name\" content=\"Tungsten Fabric\" \/>\n<meta property=\"article:published_time\" content=\"2015-11-12T07:06:25+00:00\" \/>\n<meta name=\"author\" content=\"Pedro Marques\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"WebPage\",\"@id\":\"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/\",\"url\":\"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/\",\"name\":\"Installing Kubernetes & Opencontrail - Tungsten Fabric\",\"isPartOf\":{\"@id\":\"https:\/\/tungsten.io\/#website\"},\"datePublished\":\"2015-11-12T07:06:25+00:00\",\"dateModified\":\"2015-11-12T07:06:25+00:00\",\"author\":{\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/4482c87f5bb06e7236ba1ff004df8def\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/\"]}]},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/tungsten.io\/#website\",\"url\":\"https:\/\/tungsten.io\/\",\"name\":\"Tungsten Fabric\",\"description\":\"multicloud multistack SDN\",\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/tungsten.io\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"Person\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/4482c87f5bb06e7236ba1ff004df8def\",\"name\":\"Pedro Marques\",\"image\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-US\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/image\/\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/6e92702fd500d2259b81e6285c050e68?s=96&d=mm&r=pg\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/6e92702fd500d2259b81e6285c050e68?s=96&d=mm&r=pg\",\"caption\":\"Pedro Marques\"},\"url\":\"https:\/\/tungsten.io\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"Installing Kubernetes & Opencontrail - Tungsten Fabric","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/","og_locale":"en_US","og_type":"article","og_title":"Installing Kubernetes & Opencontrail - Tungsten Fabric","og_description":"In this post we walk through the steps required to install a 2 node cluster running Kubernetes that uses OpenContrail as the network provider. In addition to the 2 compute...","og_url":"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/","og_site_name":"Tungsten Fabric","article_published_time":"2015-11-12T07:06:25+00:00","author":"Pedro Marques","twitter_card":"summary_large_image","schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"WebPage","@id":"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/","url":"https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/","name":"Installing Kubernetes & Opencontrail - Tungsten Fabric","isPartOf":{"@id":"https:\/\/tungsten.io\/#website"},"datePublished":"2015-11-12T07:06:25+00:00","dateModified":"2015-11-12T07:06:25+00:00","author":{"@id":"https:\/\/tungsten.io\/#\/schema\/person\/4482c87f5bb06e7236ba1ff004df8def"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/tungsten.io\/installing-kubernetes-opencontrail\/"]}]},{"@type":"WebSite","@id":"https:\/\/tungsten.io\/#website","url":"https:\/\/tungsten.io\/","name":"Tungsten Fabric","description":"multicloud multistack SDN","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/tungsten.io\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"Person","@id":"https:\/\/tungsten.io\/#\/schema\/person\/4482c87f5bb06e7236ba1ff004df8def","name":"Pedro Marques","image":{"@type":"ImageObject","inLanguage":"en-US","@id":"https:\/\/tungsten.io\/#\/schema\/person\/image\/","url":"https:\/\/secure.gravatar.com\/avatar\/6e92702fd500d2259b81e6285c050e68?s=96&d=mm&r=pg","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/6e92702fd500d2259b81e6285c050e68?s=96&d=mm&r=pg","caption":"Pedro Marques"},"url":"https:\/\/tungsten.io"}]}},"_links":{"self":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/6855"}],"collection":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/users\/458"}],"replies":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/comments?post=6855"}],"version-history":[{"count":0,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/6855\/revisions"}],"wp:attachment":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/media?parent=6855"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/categories?post=6855"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/tags?post=6855"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}