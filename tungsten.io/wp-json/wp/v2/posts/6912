{"id":6912,"date":"2016-02-16T12:04:27","date_gmt":"2016-02-16T20:04:27","guid":{"rendered":"http:\/\/www.opencontrail.org\/?p=6912"},"modified":"2016-02-16T12:04:27","modified_gmt":"2016-02-16T20:04:27","slug":"kubernetes-and-openstack-multi-cloud-networking","status":"publish","type":"post","link":"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/","title":{"rendered":"Kubernetes and OpenStack multi-cloud networking"},"content":{"rendered":"<p><em>This is a guest blog from tcpCloud, authored by Marek Celoud &amp; Jakub Pavlik (tcp cloud engineers). To see the original post,<a href=\"http:\/\/www.tcpcloud.eu\/en\/blog\/2016\/02\/12\/kubernetes-and-openstack-multi-cloud-networking\/\" target=\"_blank\">click here<\/a>.<\/em><\/p>\n<p>This blog brings first insight into usage of real bare metal Kubernetes clusters for application workloads from networking point of view. A special thanks goes to Lachlan Evenson and his colleagues from Lithium for collaboration on this post and providing real use cases.<\/p>\n<p>Since the last OpenStack Summit in Tokyo last November we realized the magnitude of impact the containers have on a global community. Everyone has been speaking about using containers and Kubernetes instead of standard virtual machines. There are couple of reasons for that, especially because it is lightweight nature, easy and fast deploys, and developers love this. They can easily develop, maintain, scale and roll-update their applications. We at tcp cloud focus on building private cloud solutions based on open source technologies wanted to get Kubernetes and see if it can really be used in production setup along or within the OpenStack powered virtualization.<\/p>\n<p>Kubernetes brings a new way to manage container-based workloads and enables similar features like OpenStack for VMs for start. If you start using Kubernetes you will soon realize that you can deploy easily it in AWS, GCE or Vagrant, but what about your on-premise bare-metal deployment? How to integrate it into your current OpenStack or virtualized infrastructure? All blog posts and manuals document small clusters running in virtual machines with sample web applications, but none of them show real scenario for bare-metal or enterprise performance workload with integration in current network design. To properly design networking is the most difficult part of architectural design, just like with OpenStack. Therefore we have defined following networking requirements:<\/p>\n<ul class=\"simple\">\n<li><strong>Multi tenancy<\/strong> &#8211; separation of containers workload is basic requirement for every security policy standard. e.g. default Flannel networking only provides flat network architecture.<\/li>\n<li><strong>Multi-cloud support<\/strong> &#8211; not every workload is suiteble for containers and you still need to put heavy loads like databases in VMs or even on bare metals. For this reason single control plane for the SDN is the best option.<\/li>\n<li><strong>Overlay<\/strong> &#8211; is related to multi-tenancy. Almost every OpenStack Neutron deployment uses some kind of overlays (VXLAN, GRE, MPLSoverGRE, MPLSoverUDP) and we have to be able inter-connect them.<\/li>\n<li><strong>Distributed routing engine<\/strong> &#8211; East-West and North-South traffic cannot go through one central software service. Network traffic has to go directly between OpenStack compute nodes and Kubernetes nodes. Optimal is to provide routing on routers instead of proprietary gateway appliances.<\/li>\n<\/ul>\n<p>Based on these requirements we have decided to start using OpenContrail SDN first and our mission was to integrate OpenStack workload with Kubernetes, then find a suitable application stack for the actual load testing.<\/p>\n<div id=\"opencontrail-overview\" class=\"section\">\n<h2>OpenContrail overview<\/h2>\n<p><a class=\"reference external\" href=\"http:\/\/opencontrail.org\">OpenContrail<\/a> is open source SDN &amp; NFV solution, with tight ties to OpenStack since Havana. It was one of the first production ready Neutron plugins along with Nicira (now VMware NSX-VH) and last summit\u2019s <a class=\"reference external\" href=\"https:\/\/www.openstack.org\/assets\/survey\/Public-User-Survey-Report.pdf\">survey<\/a> showed it is the second most deployed solution after OpenVwitch and first of the Vendor based solutions. OpenContrail has integrations to OpenStack, VMware, Docker and Kubernetes.<\/p>\n<p>Kubernetes network plugin <a class=\"reference external\" href=\"https:\/\/github.com\/Juniper\/contrail-kubernetes\">kube-network-manager<\/a> was under development since OpenStack summit at Vancouver last year and first announcement was released in end of year.<\/p>\n<p>The kube-network-manager process uses the kubernetes controller framework to listen to changes in objects that are defined in the API and add annotations to some of these objects. Then it creates network solution for the application using the OpenContrail API that define objects such as virtual-networks, network interfaces and access control policies. More information is available at this <a class=\"reference external\" href=\"https:\/\/pedrormarques.wordpress.com\/2015\/07\/14\/kubernetes-networking-with-opencontrail\/\">blog<\/a><\/p>\n<\/div>\n<div id=\"architecture\" class=\"section\">\n<h2>Architecture<\/h2>\n<p>We started testing with two independent Contrail deployments and then set up BGP federation. The reason for federation is keystone authentication of kube-network-manager. When contrail-neutron-plugin is enabled, contrail API uses keystone authentication and this feature is not yet implemented at kubernetes plugin. The Contrail federation is described in more later in this post.<\/p>\n<p>The following schema shows high level architecture, where on left side is OpenStack cluster and Kubernetes cluster is on the right side. OpenStack and OpenContrail are deployed in fully High Available best practice design, which can be scaled up to hundreds of compute nodes.<\/p>\n<p><a href=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes.png\" rel=\"attachment wp-att-6915\"><img loading=\"lazy\" decoding=\"async\" class=\"alignleft wp-image-6915\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes.png\" alt=\"opencontrail-kubernetes\" width=\"800\" height=\"542\" data-id=\"6915\" \/><\/a><\/p>\n<\/div>\n<p>The following figure shows federation of two Contrail clusters. In general, this feature enables Contrail controllers connection between different sites of a Multi-site DC without the need of a physical gateway. The control nodes at each site are peered with other sites using BGP. It is possible to stretch both L2 and L3 networks across multiple DCs this way.<\/p>\n<p>This design is usually used for two independent OpenStack cloud or two OpenStack Region. All components of Contrail including vRouter are exactly the same. Kube-network-manager and neutron-contrail-plugin just translate API requests for different platforms. The core functionality of the networking solution remains unchanged. This brings not only robust networking engine, but analytics too.<\/p>\n<p><a href=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes-bgp.png\" rel=\"attachment wp-att-6916\"><img loading=\"lazy\" decoding=\"async\" class=\"alignleft wp-image-6916\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes-bgp.png\" alt=\"opencontrail-kubernetes-bgp\" width=\"800\" height=\"586\" data-id=\"6916\" \/><\/a><\/p>\n<h2>Application Stack<\/h2>\n<div id=\"overview\" class=\"section\">\n<h3>Overview<\/h3>\n<p>Lets have a look at typical scenario. Our developers gave us docker <a class=\"reference external\" href=\"https:\/\/github.com\/django-leonardo\/django-leonardo\/blob\/master\/contrib\/haproxy\/docker-compose.yml\">compose.yml<\/a> , which is use for development and local tests on their laptop. This situation is easier, because our developers already know docker and application workload is docker-ready. This application stack contains following components:<\/p>\n<ul class=\"simple\">\n<li><strong>Database<\/strong> &#8211; PostgreSQL or MySQL database cluster.<\/li>\n<li><strong>Memcached<\/strong> &#8211; it is for content caching.<\/li>\n<li><strong>Django app Leonardo<\/strong> &#8211; Django CMS <a class=\"reference external\" href=\"https:\/\/www.leonardo-cms.org\">Leonardo<\/a> was used for application stack testing.<\/li>\n<li><strong>Nginx<\/strong> &#8211; web proxy.<\/li>\n<li><strong>Load balancer<\/strong> &#8211; HAProxy load balancer for containers scaling.<\/li>\n<\/ul>\n<p>When we want to get it into production, we can transform everything into kubernetes replication controllers with services, but as we mentioned at beginning not everything is suitable for containers. Therefore we separate database cluster to OpenStack VMs and rewrite rest into kubernetes manifests.<\/p>\n<\/div>\n<div id=\"application-deployment\" class=\"section\">\n<h3>Application deployment<\/h3>\n<p>This section describes workflow for application provisioning on OpenStack and Kubernetes.<\/p>\n<div id=\"openstack-side\" class=\"section\">\n<h4><\/h4>\n<div id=\"openstack-side\" class=\"section\">\n<h4>OpenStack side<\/h4>\n<p>At the first step, we have launched Heat database stack on OpenStack. This created 3 VMs with PostgreSQL and database network. Database network is private tenant isolated network.<\/p>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre><span class=\"c\"># nova list<\/span>\n+--------------------------------------+--------------+--------+------------+-------------+-----------------------+\n<span class=\"p\">|<\/span> ID                                   <span class=\"p\">|<\/span> Name         <span class=\"p\">|<\/span> Status <span class=\"p\">|<\/span> Task State <span class=\"p\">|<\/span> Power State <span class=\"p\">|<\/span> Networks              <span class=\"p\">|<\/span>\n+--------------------------------------+--------------+--------+------------+-------------+-----------------------+\n<span class=\"p\">|<\/span> d02632b7-9ee8-486f-8222-b0cc1229143c <span class=\"p\">|<\/span> PostgreSQL-1 <span class=\"p\">|<\/span> ACTIVE <span class=\"p\">|<\/span> -          <span class=\"p\">|<\/span> Running     <span class=\"p\">|<\/span> <span class=\"nv\">leonardodb<\/span><span class=\"o\">=<\/span>10.0.100.3 <span class=\"p\">|<\/span>\n<span class=\"p\">|<\/span> b5ff88f8-0b81-4427-a796-31f3577333b5 <span class=\"p\">|<\/span> PostgreSQL-2 <span class=\"p\">|<\/span> ACTIVE <span class=\"p\">|<\/span> -          <span class=\"p\">|<\/span> Running     <span class=\"p\">|<\/span> <span class=\"nv\">leonardodb<\/span><span class=\"o\">=<\/span>10.0.100.4 <span class=\"p\">|<\/span>\n<span class=\"p\">|<\/span> 7681678e-6e75-49f7-a874-2b1bb0a120bd <span class=\"p\">|<\/span> PostgreSQL-3 <span class=\"p\">|<\/span> ACTIVE <span class=\"p\">|<\/span> -          <span class=\"p\">|<\/span> Running     <span class=\"p\">|<\/span> <span class=\"nv\">leonardodb<\/span><span class=\"o\">=<\/span>10.0.100.5 <span class=\"p\">|<\/span>\n+--------------------------------------+--------------+--------+------------+-------------+-----------------------+\n<\/pre>\n<\/div>\n<\/div>\n<\/div>\n<div id=\"kubernetes-side\" class=\"section\">\n<h4>Kubernetes side<\/h4>\n<p>At kubernetes side we have to launch manifests with Leonardo and Nginx services. All of them can be displayed <a class=\"reference external\" href=\"https:\/\/github.com\/pupapaik\/scripts\/tree\/master\/kubernetes\/leonardo\">there<\/a>.<\/p>\n<p>In order for it to run successfully with networking isolation, look at the following sections.<\/p>\n<ul class=\"simple\">\n<li><strong>leonardo-rc.yaml<\/strong> &#8211; Replication Controller for Leonardo app with replicas 3 and virtual network leonardo<\/li>\n<\/ul>\n<div class=\"highlight-yaml\">\n<div class=\"highlight\">\n<pre><span class=\"l-Scalar-Plain\">apiVersion<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">v1<\/span>\n<span class=\"l-Scalar-Plain\">kind<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">ReplicationController<\/span>\n<span class=\"nn\">...<\/span>\n  <span class=\"l-Scalar-Plain\">template<\/span><span class=\"p-Indicator\">:<\/span>\n<span class=\"l-Scalar-Plain\">metadata<\/span><span class=\"p-Indicator\">:<\/span>\n  <span class=\"l-Scalar-Plain\">labels<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"l-Scalar-Plain\">app<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">leonardo<\/span>\n    <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">leonardo<\/span> <span class=\"c1\"># label name defines and creates new virtual network in contrail<\/span>\n<span class=\"nn\">...<\/span><\/pre>\n<\/div>\n<\/div>\n<ul class=\"simple\">\n<li><strong>leonardo-svc.yaml<\/strong> &#8211; leonardo service expose application pods with virtual IP from cluster network on port 8000.<\/li>\n<\/ul>\n<div class=\"highlight-yaml\">\n<div class=\"highlight\">\n<pre><span class=\"l-Scalar-Plain\">apiVersion<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">v1<\/span>\n<span class=\"l-Scalar-Plain\">kind<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">Service<\/span>\n<span class=\"l-Scalar-Plain\">metadata<\/span><span class=\"p-Indicator\">:<\/span>\n  <span class=\"l-Scalar-Plain\">labels<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">ftleonardo<\/span>\n  <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">ftleonardo<\/span>\n<span class=\"l-Scalar-Plain\">spec<\/span><span class=\"p-Indicator\">:<\/span>\n  <span class=\"l-Scalar-Plain\">ports<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"p-Indicator\">-<\/span> <span class=\"l-Scalar-Plain\">port<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">8000<\/span>\n  <span class=\"l-Scalar-Plain\">selector<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">leonardo<\/span> <span class=\"c1\"># selector\/name matches label\/name in replication controller to receive traffic for this service<\/span>\n<span class=\"nn\">...<\/span><\/pre>\n<\/div>\n<\/div>\n<ul class=\"simple\">\n<li><strong>nginx-rc.yaml<\/strong> &#8211; NGINX replication controller with 3 replicas and virtual network nginx and policy allowing traffic to leonardo-svc network. This sample does not use SSL.<\/li>\n<\/ul>\n<div class=\"highlight-yaml\">\n<div class=\"highlight\">\n<pre><span class=\"l-Scalar-Plain\">apiVersion<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">v1<\/span>\n<span class=\"l-Scalar-Plain\">kind<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">ReplicationController<\/span>\n<span class=\"nn\">...<\/span>\n  <span class=\"l-Scalar-Plain\">template<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"l-Scalar-Plain\">metadata<\/span><span class=\"p-Indicator\">:<\/span>\n      <span class=\"l-Scalar-Plain\">labels<\/span><span class=\"p-Indicator\">:<\/span>\n        <span class=\"l-Scalar-Plain\">app<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">nginx<\/span>\n        <span class=\"l-Scalar-Plain\">uses<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">ftleonardo<\/span> <span class=\"c1\"># uses creates policy to allow traffic between leonardo service and nginx pods.<\/span>\n        <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">nginx<\/span> <span class=\"c1\"># creates virtual network nginx with policy ftleonardo<\/span>\n<span class=\"nn\">...<\/span><\/pre>\n<\/div>\n<\/div>\n<ul class=\"simple\">\n<li><strong>nginx-svc.yaml<\/strong> &#8211; creates service with cluster vip IP and public virtual IP to access application from Internet.<\/li>\n<\/ul>\n<div class=\"highlight-yaml\">\n<div class=\"highlight\">\n<pre><span class=\"l-Scalar-Plain\">apiVersion<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">v1<\/span>\n<span class=\"l-Scalar-Plain\">kind<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">Service<\/span>\n<span class=\"l-Scalar-Plain\">metadata<\/span><span class=\"p-Indicator\">:<\/span>\n  <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">nginx<\/span>\n  <span class=\"l-Scalar-Plain\">labels<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"l-Scalar-Plain\">app<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">nginx<\/span>\n    <span class=\"l-Scalar-Plain\">name<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">nginx<\/span>\n<span class=\"nn\">...<\/span>\n  <span class=\"l-Scalar-Plain\">selector<\/span><span class=\"p-Indicator\">:<\/span>\n    <span class=\"l-Scalar-Plain\">app<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">nginx<\/span> <span class=\"c1\"># selector\/name matches label\/name in RC to receive traffic for the svc<\/span>\n  <span class=\"l-Scalar-Plain\">type<\/span><span class=\"p-Indicator\">:<\/span> <span class=\"l-Scalar-Plain\">LoadBalancer<\/span> <span class=\"c1\"># this creates new floating IPs from external virtual network and associate with VIP IP of the service.<\/span>\n<span class=\"nn\">...<\/span><\/pre>\n<\/div>\n<\/div>\n<p>Lets run all manifests by calling kubeclt<\/p>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre>kubectl create -f \/directory_with_manifests\/\n<\/pre>\n<\/div>\n<\/div>\n<p>This creates following pods and services in Kubernetes.<\/p>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre><span class=\"c\"># kubectl get pods<\/span>\nNAME             READY     STATUS    RESTARTS   AGE\nleonardo-369ob   1\/1       Running   <span class=\"m\">0<\/span>          35m\nleonardo-3xmdt   1\/1       Running   <span class=\"m\">0<\/span>          35m\nleonardo-q9kt3   1\/1       Running   <span class=\"m\">0<\/span>          35m\nnginx-jaimw      1\/1       Running   <span class=\"m\">0<\/span>          35m\nnginx-ocnx2      1\/1       Running   <span class=\"m\">0<\/span>          35m\nnginx-ykje9      1\/1       Running   <span class=\"m\">0<\/span>          35m\n<\/pre>\n<\/div>\n<\/div>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre><span class=\"c\"># kubectl get service<\/span>\nNAME         CLUSTER_IP      EXTERNAL_IP     PORT<span class=\"o\">(<\/span>S<span class=\"o\">)<\/span>    SELECTOR        AGE\nftleonardo   10.254.98.15    &lt;none&gt;          8000\/TCP   <span class=\"nv\">name<\/span><span class=\"o\">=<\/span>leonardo   35m\nkubernetes   10.254.0.1      &lt;none&gt;          443\/TCP    &lt;none&gt;          35m\nnginx        10.254.225.19   185.22.97.188   80\/TCP     <span class=\"nv\">app<\/span><span class=\"o\">=<\/span>nginx       35m\n<\/pre>\n<\/div>\n<\/div>\n<p>Only Nginx service has public ip 185.22.97.188, which is floating ip configured as LoadBalancer. All traffic is now balanced by ECMP on Juniper MX.<\/p>\n<p>To get cluster fully working, there must set routing between leonardo virtual network in Kubernetes Contrail and database virtual network in OpenStack Contrail. Go into both Contrail UI and set same Route Target for both networks. This can be automated too through contrail heat resources.<\/p>\n<\/div>\n<p><a href=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/route-target.png\" rel=\"attachment wp-att-6918\"><img loading=\"lazy\" decoding=\"async\" class=\"alignleft size-full wp-image-6918\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/route-target.png\" alt=\"route-target\" width=\"968\" height=\"513\" data-id=\"6918\" \/><\/a><\/p>\n<\/div>\n<\/div>\n<p>The following figure shows how should look final production application stack. At top there are 2 Juniper MXs with Public VRF, where are floating IPs propagated. The traffic is ballanced through ECMP to MPLSoverGRE tunnel to 3 nginx pods. Nginx proxies request to Leonardo application server, which stores sessions and content into PostgreSQL database cluster running at OpenStack VMs. Connection between PODs and VMs is direct without any routed central point. Juniper MXs are used only for outgoing connection to Internet. Thanks to storing application session into database (normally is memcached or redis), we do not need specific L7 load balancer and ECMP works without any problem.<\/p>\n<p><a href=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes-scenario.png\" rel=\"attachment wp-att-6917\"><img loading=\"lazy\" decoding=\"async\" class=\"alignleft wp-image-6917\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes-scenario.png\" alt=\"opencontrail-kubernetes-scenario\" width=\"800\" height=\"645\" data-id=\"6917\" \/><\/a><\/p>\n<h3>Other Outputs<\/h3>\n<p>This section shows other interesting outputs from application stack. Nginx service description with LoadBalancer shows floating IP and private cluster IP. Then 3 IP addresses of nginx pods. Traffic is distributed through vrouter ecmp.<\/p>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre><span class=\"c\"># kubectl describe svc\/nginx<\/span>\nName:                   nginx\nNamespace:              default\nLabels:                 <span class=\"nv\">app<\/span><span class=\"o\">=<\/span>nginx,name<span class=\"o\">=<\/span>nginx\nSelector:               <span class=\"nv\">app<\/span><span class=\"o\">=<\/span>nginx\nType:                   LoadBalancer\nIP:                     10.254.225.19\nLoadBalancer Ingress:   185.22.97.188\nPort:                   http    80\/TCP\nNodePort:               http    30024\/TCP\nEndpoints:              10.150.255.243:80,10.150.255.248:80,10.150.255.250:80\nSession Affinity:       None\n<\/pre>\n<\/div>\n<\/div>\n<p>Nginx routing table shows internal routes between pods and route 10.254.98.15\/32, which points to leonardo service.<\/p>\n<p><a href=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/nginxRT.png\" rel=\"attachment wp-att-6914\"><img loading=\"lazy\" decoding=\"async\" class=\"alignleft wp-image-6914\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/nginxRT.png\" alt=\"nginxRT\" width=\"800\" height=\"312\" data-id=\"6914\" \/><\/a><\/p>\n<p>The previous route 10.254.98.15\/32 is inside of description for leonardo service.<\/p>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre><span class=\"c\"># kubectl describe svc\/ftleonardo<\/span>\nName:                   ftleonardo\nNamespace:              default\nLabels:                 <span class=\"nv\">name<\/span><span class=\"o\">=<\/span>ftleonardo\nSelector:               <span class=\"nv\">name<\/span><span class=\"o\">=<\/span>leonardo\nType:                   ClusterIP\nIP:                     10.254.98.15\nPort:                   &lt;unnamed&gt;       8000\/TCP\nEndpoints:              10.150.255.245:8000,10.150.255.247:8000,10.150.255.252:8000\n<\/pre>\n<\/div>\n<\/div>\n<p>The routing table for leonardo looks similar like nginx except routes 10.0.100.X\/32, whose points to OpenStack VMs in different Contrail.<\/p>\n<p><a href=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/leonardoRT.png\" rel=\"attachment wp-att-6913\"><img loading=\"lazy\" decoding=\"async\" class=\"alignleft wp-image-6913\" src=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/leonardoRT.png\" alt=\"leonardoRT\" width=\"800\" height=\"475\" data-id=\"6913\" \/><\/a><\/p>\n<p>The last output is from Juniper MXs VRF showing multiple routes to nginx pods.<\/p>\n<div class=\"highlight-bash\">\n<div class=\"highlight\">\n<pre><span class=\"c\">185.22.97.188\/32   @[BGP\/170] 00:53:48, localpref 200, from 10.0.170.71\n                      AS path: ?, validation-state: unverified\n                    > via gr-0\/0\/0.32782, Push 20\n                    [BGP\/170] 00:53:31, localpref 200, from 10.0.170.71\n                      AS path: ?, validation-state: unverified\n                    > via gr-0\/0\/0.32778, Push 36\n                    [BGP\/170] 00:53:48, localpref 200, from 10.0.170.72\n                      AS path: ?, validation-state: unverified\n                    > via gr-0\/0\/0.32782, Push 20\n                    [BGP\/170] 00:53:31, localpref 200, from 10.0.170.72\n                      AS path: ?, validation-state: unverified\n                    > via gr-0\/0\/0.32778, Push 36\n                   #[Multipath\/255] 00:53:48, metric2 0\n                    > via gr-0\/0\/0.32782, Push 20\n                      via gr-0\/0\/0.32778, Push 36\n<\/pre>\n<\/div>\n<\/div>\n<h2>Conclusion<\/h2>\n<p>We have proved that you can use single SDN solution for OpenStack, Kubernetes, Bare metal and VMware vCenter. The more important thing is that this use case can be actually used for production environments.<\/p>\n<p>If you are more interested in this topic, you can vote for our session <a class=\"reference external\" href=\"https:\/\/www.openstack.org\/summit\/austin-2016\/vote-for-speakers\/Presentation\/8688\">Multi-cloud Networking for OpenStack Summit at Austin<\/a>.<\/p>\n<p>Currently we are working on requirements for Kubernetes networking stacks and then provide detailed comparison between different Kubernetes network plugins like Weave, Calico, OpenVSwitch, Flannel and Contrail at scale of 250 bare metal servers.<\/p>\n<p>We are also working on OpenStack Magnum with Kubernetes backend to bring developers self-service portal for simple testing and development. Then they will be able to prepare application manifests inside of OpenStack VMs, a then push changes of final production definitions into git and at the end use them at production.<\/p>\n<p>Special thanks go to Pedro Marques from Juniper for his support and contribution during testing.<\/p>\n<p>Jakub Pavlik &amp; tcp cloud team<\/p>\n","protected":false},"excerpt":{"rendered":"<p>This is a guest blog from tcpCloud, authored by Marek Celoud &amp; Jakub Pavlik (tcp cloud engineers). To see the original post,click here. This blog brings first insight into usage&#8230;<\/p>\n","protected":false},"author":479,"featured_media":0,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"footnotes":""},"categories":[16,17,11],"tags":[],"acf":[],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v21.6 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Kubernetes and OpenStack multi-cloud networking - Tungsten Fabric<\/title>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Kubernetes and OpenStack multi-cloud networking - Tungsten Fabric\" \/>\n<meta property=\"og:description\" content=\"This is a guest blog from tcpCloud, authored by Marek Celoud &amp; Jakub Pavlik (tcp cloud engineers). To see the original post,click here. This blog brings first insight into usage...\" \/>\n<meta property=\"og:url\" content=\"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/\" \/>\n<meta property=\"og:site_name\" content=\"Tungsten Fabric\" \/>\n<meta property=\"article:published_time\" content=\"2016-02-16T20:04:27+00:00\" \/>\n<meta property=\"og:image\" content=\"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes.png\" \/>\n<meta name=\"author\" content=\"Jakub Pavlik\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"WebPage\",\"@id\":\"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/\",\"url\":\"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/\",\"name\":\"Kubernetes and OpenStack multi-cloud networking - Tungsten Fabric\",\"isPartOf\":{\"@id\":\"https:\/\/tungsten.io\/#website\"},\"datePublished\":\"2016-02-16T20:04:27+00:00\",\"dateModified\":\"2016-02-16T20:04:27+00:00\",\"author\":{\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75\"},\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/\"]}]},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/tungsten.io\/#website\",\"url\":\"https:\/\/tungsten.io\/\",\"name\":\"Tungsten Fabric\",\"description\":\"multicloud multistack SDN\",\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https:\/\/tungsten.io\/?s={search_term_string}\"},\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"Person\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75\",\"name\":\"Jakub Pavlik\",\"image\":{\"@type\":\"ImageObject\",\"inLanguage\":\"en-US\",\"@id\":\"https:\/\/tungsten.io\/#\/schema\/person\/image\/\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg\",\"contentUrl\":\"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg\",\"caption\":\"Jakub Pavlik\"},\"url\":\"https:\/\/tungsten.io\"}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","yoast_head_json":{"title":"Kubernetes and OpenStack multi-cloud networking - Tungsten Fabric","robots":{"index":"index","follow":"follow","max-snippet":"max-snippet:-1","max-image-preview":"max-image-preview:large","max-video-preview":"max-video-preview:-1"},"canonical":"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/","og_locale":"en_US","og_type":"article","og_title":"Kubernetes and OpenStack multi-cloud networking - Tungsten Fabric","og_description":"This is a guest blog from tcpCloud, authored by Marek Celoud &amp; Jakub Pavlik (tcp cloud engineers). To see the original post,click here. This blog brings first insight into usage...","og_url":"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/","og_site_name":"Tungsten Fabric","article_published_time":"2016-02-16T20:04:27+00:00","og_image":[{"url":"http:\/\/www.opencontrail.org\/wp-content\/uploads\/2016\/02\/opencontrail-kubernetes.png"}],"author":"Jakub Pavlik","twitter_card":"summary_large_image","schema":{"@context":"https:\/\/schema.org","@graph":[{"@type":"WebPage","@id":"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/","url":"https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/","name":"Kubernetes and OpenStack multi-cloud networking - Tungsten Fabric","isPartOf":{"@id":"https:\/\/tungsten.io\/#website"},"datePublished":"2016-02-16T20:04:27+00:00","dateModified":"2016-02-16T20:04:27+00:00","author":{"@id":"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https:\/\/tungsten.io\/kubernetes-and-openstack-multi-cloud-networking\/"]}]},{"@type":"WebSite","@id":"https:\/\/tungsten.io\/#website","url":"https:\/\/tungsten.io\/","name":"Tungsten Fabric","description":"multicloud multistack SDN","potentialAction":[{"@type":"SearchAction","target":{"@type":"EntryPoint","urlTemplate":"https:\/\/tungsten.io\/?s={search_term_string}"},"query-input":"required name=search_term_string"}],"inLanguage":"en-US"},{"@type":"Person","@id":"https:\/\/tungsten.io\/#\/schema\/person\/fa242938c01e9144363e911cf07ecd75","name":"Jakub Pavlik","image":{"@type":"ImageObject","inLanguage":"en-US","@id":"https:\/\/tungsten.io\/#\/schema\/person\/image\/","url":"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg","contentUrl":"https:\/\/secure.gravatar.com\/avatar\/0fe1e918e30d022ef4c9895cc59c9d7f?s=96&d=mm&r=pg","caption":"Jakub Pavlik"},"url":"https:\/\/tungsten.io"}]}},"_links":{"self":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/6912"}],"collection":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/users\/479"}],"replies":[{"embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/comments?post=6912"}],"version-history":[{"count":0,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/posts\/6912\/revisions"}],"wp:attachment":[{"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/media?parent=6912"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/categories?post=6912"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/tungsten.io\/wp-json\/wp\/v2\/tags?post=6912"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}