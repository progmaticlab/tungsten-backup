<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Routing/Switching Archives - Tungsten Fabric</title>
	<atom:link href="https://tungsten.io/category/routingswitching/feed/" rel="self" type="application/rss+xml" />
	<link>https://tungsten.io/category/routingswitching/</link>
	<description>multicloud multistack SDN</description>
	<lastBuildDate>Mon, 11 Jul 2016 22:17:57 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.1</generator>

<image>
	<url>https://tungsten.io/wp-content/uploads/sites/73/2018/03/cropped-TungstenFabric_Stacked_Gradient_3000px-150x150.png</url>
	<title>Routing/Switching Archives - Tungsten Fabric</title>
	<link>https://tungsten.io/category/routingswitching/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Migrating a legacy cloud infrastructure to Contrail SDN based infrastructure</title>
		<link>https://tungsten.io/migrating-a-legacy-cloud-infrastructure-to-contrail-sdn-based-infrastructure/</link>
		
		<dc:creator><![CDATA[Vivekananda Shenoy]]></dc:creator>
		<pubDate>Mon, 11 Jul 2016 22:17:57 +0000</pubDate>
				<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Legacy Migration]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[SDN]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7114</guid>

					<description><![CDATA[This app-note describes a mechanism to migrate a legacy cloud-based infrastructure which could use a L2 based network segmentation using technologies such as Linux Bridge or OVS to a Contrail...]]></description>
										<content:encoded><![CDATA[<p>This app-note describes a mechanism to migrate a legacy cloud-based infrastructure which could use a L2 based network segmentation using technologies such as Linux Bridge or OVS to a Contrail SDN based cloud infrastructure. Both Legacy and Contrail clusters can co-exist side by side such that a single L3 subnet can span across both legacy and contrail clusters. This facilitates the cloud administrators to perform a phase wise migration of virtual workloads from legacy to contrail cluster. In addition to this both Legacy and Contrail workloads also have a public internet access.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image1.png"><img fetchpriority="high" decoding="async" class="wp-image-7115 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image1.png" alt="migration to EVPN_blogpost_image1" width="650" height="349" data-id="7115" /></a></p>
<h3>Proposed solutions:</h3>
<p>Idea is to use separate logical L2 &amp; L3 SDN gateways to interconnect the Legacy cloud and the Contrail cloud and provide public internet access from both the clouds. L2 gateway PE will be peered to Contrail Controller for EVPN address family and will hence exchange the MAC address routes of the VMs. Also the the PE-CE link of the EVPN routing-instance is connected to the legacy cloud L2 fabric. This will make the Contrail Virtual Network to be stretched to the legacy cloud. 2 EVPN PE’s are installed for redundancy and the PE Ethernet interface on both the PE’s are configured as single homed Ethernet segment (ESI 0).  Spanning tree will be used to break the Layer 2 loop formed by the L2 Network and the redundant EVPN PE’s. In this case, only one L3-SDN GW is shown. Adding the second L3-GW for the redundancy doesn’t add much complexity and is just a matter of configuring BGP between Contrail and the second L3-GW.</p>
<p><strong>Note: The L2 &amp; L3 gateway functionalities can be easily collapsed 2 physical MX routers. This can be done either by using 2 separate routing instances for EVPN and L3VPN and separate PE-CE link towards Legacy cloud (EVPN L2 ifl) and the public network (L3VPN L3 ifl). In this example, separate routers are used for ease of illustration.</strong></p>
<p>The public access from the legacy cloud VM is through the L3-GW configured on the IRB (VLAN) interface aggregation switch (192.168.1.250 in this example) whereas for the VMs running on the Contrail cloud the traffic from the VM towards the public internet is through the default route originated from MX L3GW VRF. This default route actually will redirect the traffic to inet.0 on the MX L3-GW for internet access.</p>
<p>&nbsp;</p>
<p>Return traffic from the internet always will be routed to the legacy and the contrail cloud via the legacy L3 GW configured on the aggregation switch. The interface between qfx5100-sw1 and mx-l3gw is configured as a L3 interface and provides the return path from the public network to the legacy cloud and the contrail cloud.</p>
<h3><strong>Example setup: Verification:</strong></h3>
<p><strong>Contrail Virtual Network configuration and vrouter routing table entries</strong></p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image2.png"><img decoding="async" class="aligncenter wp-image-7116" src="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image2.png" alt="migration to EVPN_blogpost_image2" width="800" height="437" data-id="7116" /></a></p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image3.png"><img decoding="async" class="aligncenter wp-image-7117" src="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image3.png" alt="migration to EVPN_blogpost_image3" width="800" height="358" data-id="7117" /></a></p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image4.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7118" src="http://www.opencontrail.org/wp-content/uploads/2016/07/migration-to-EVPN_blogpost_image4.png" alt="migration to EVPN_blogpost_image4" width="801" height="309" data-id="7118" /></a></p>
<p>&nbsp;</p>
<p><strong>Verify traffic flow from Legacy VM to Contrail VM and Internet:</strong></p>
<pre><span style="font-family: 'courier new', courier;">
root@legacy-vm:~# ip route
<mark>default via 192.168.1.250 dev p514p1 </mark>
10.84.0.0/16 via 10.87.65.126 dev p514p2
10.87.0.0/16 via 10.87.65.126 dev p514p2
10.87.65.0/25 dev p514p2  proto kernel  scope link  src 10.87.65.2
<mark>192.168.1.0/24 dev p514p1  proto kernel  scope link  src 192.168.1.111</mark>
root@legacy-vm:~#
root@legacy-vm:~# ping 8.8.8.8 -c 2
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=63 time=0.398 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=63 time=0.412 ms
 

--- 8.8.8.8 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 999ms
rtt min/avg/max/mdev = 0.398/0.405/0.412/0.007 ms
root@legacy-vm:~# ping 192.168.1.4 -c 2
PING 192.168.1.4 (192.168.1.4) 56(84) bytes of data.
64 bytes from 192.168.1.4: icmp_seq=1 ttl=62 time=1.34 ms
64 bytes from 192.168.1.4: icmp_seq=2 ttl=62 time=0.384 ms
 

--- 192.168.1.4 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.384/0.863/1.342/0.479 ms
root@legacy-vm:~# arp
Address                  HWtype  HWaddress           Flags Mask            Iface
<mark>192.168.1.4              ether   02:9a:66:37:06:5c   C                     p514p1</mark>
<mark>192.168.1.250            ether   54:4b:8c:a8:8c:00   C                     p514p1</mark>
10.87.65.126             ether   30:7c:5e:0f:8f:c0   C                     p514p2
root@legacy-vm:~#</span></pre>
<p><strong>On the MX L2 &amp; L3 gateways:</strong></p>
<p><strong>mx-l2-gw1 (connected to qfx5100-sw1 whose interface is in spanning blocked state)</strong></p>
<pre><span style="font-family: 'courier new', courier;">
root@mx-l2-gw1# run show bridge mac-table

 

MAC flags       (S -static MAC, D -dynamic MAC, L -locally learned, C -Control MAC
O -OVSDB MAC, SE -Statistics enabled, NM -Non configured MAC, R -Remote PE MAC)

 

Routing instance : contrail_l2_4_VN-1
Bridging domain : bd-4, VLAN : none
MAC                 MAC      Logical                Active
address             flags    interface              source
02:9a:66:37:06:5c   D        vtep.32769             10.87.65.1
54:4b:8c:a0:cc:82   D        vtep.32770             172.16.101.3
90:e2:ba:aa:81:20   D        vtep.32770             172.16.101.3
 

[edit]

root@mx-l2-gw1#</span></pre>
<p><strong>mx-l2-gw1 (connected to qfx5100-sw2 whose interface is in spanning forwarding state)</strong></p>
<pre><span style="font-family: 'courier new', courier;">
root@mx-l2-gw2# run show bridge mac-table

 

MAC flags       (S -static MAC, D -dynamic MAC, L -locally learned, C -Control MAC
O -OVSDB MAC, SE -Statistics enabled, NM -Non configured MAC, R -Remote PE MAC)

 

Routing instance : contrail_l2_4_VN-1
Bridging domain : bd-4, VLAN : none
MAC                 MAC      Logical                Active
address             flags    interface              source
02:9a:66:37:06:5c   D        vtep.32769             10.87.65.1
54:4b:8c:a0:cc:82   D        xe-0/0/1.0
90:e2:ba:aa:81:20   D        xe-0/0/1.0

[edit]

root@mx-l2-gw2#</span></pre>
<p><strong>mx-l3-gw</strong></p>
<pre><span style="font-family: 'courier new', courier;">
root@mx-l3-gw# run show route table contrail-l3_4_VN-1.inet.0

 

contrail-l3_4_VN-1.inet.0: 3 destinations, 3 routes (3 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

 
0.0.0.0/0          *[Static/5] 02:48:06
to table inet.0
192.168.1.0/24     *[Static/5] 1d 08:30:30
Discard
192.168.1.4/32     *[BGP/170] 03:03:33, MED 100, localpref 200, from 10.87.65.1
AS path: ?, validation-state: unverified
&gt; via gr-0/0/0.32769, Push 16

[edit]

root@mx-l3-gw# run show route table inet.0 192.168.1.0/24
inet.0: 10 destinations, 11 routes (10 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

 
192.168.1.0/24     *[Static/5] 07:59:03
&gt; to 192.168.50.1 via xe-0/0/1.0
 

[edit]

root@mx-l3-gw#</span></pre>
<p><strong>qfx5100-sw1</strong></p>
<pre><span style="font-family: 'courier new', courier;">
root@qfx5100-sw1# run show spanning-tree interface

 

Spanning tree interface parameters for instance 0
 
Interface                  Port ID    Designated         Designated         Port    State  Role
port ID           bridge ID          Cost
xe-0/0/42                 128:1095     128:1097  16384.544b8ca0cc82         2000    BLK    ALT
xe-0/0/45                 128:1101     128:1101   8192.544b8ca88c02         2000    FWD    ROOT
 

{master:0}[edit]

root@qfx5100-sw1#</span></pre>
<p><strong>qfx5100-sw2</strong></p>
<pre><span style="font-family: 'courier new', courier;">
root@qfx5100-sw2# run show spanning-tree interface

 
Spanning tree interface parameters for instance 0
 
Interface                  Port ID    Designated         Designated         Port    State  Role
port ID           bridge ID          Cost
xe-0/0/43                 128:1097     128:1097  16384.544b8ca0cc82         2000    FWD    DESG
xe-0/0/44                 128:1099     128:1099   8192.544b8ca88c02         2000    FWD    ROOT
 

{master:0}[edit]

root@qfx5100-sw2#</span></pre>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Openstack Bare Metal server integration using Contrail SDN in a multi vendor DC fabric environment</title>
		<link>https://tungsten.io/contrail-integration-with-arista-tors-with-cloud-vision/</link>
		
		<dc:creator><![CDATA[Vivekananda Shenoy]]></dc:creator>
		<pubDate>Mon, 20 Jun 2016 19:48:08 +0000</pubDate>
				<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Integration]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[SDN]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7087</guid>

					<description><![CDATA[Introduction: In this blog we are going to show a hands on lab demonstration of how Juniper Contrail SDN controller allows the cloud administrator to seamlessly integrate Bare-metal servers into a...]]></description>
										<content:encoded><![CDATA[<h4>Introduction:</h4>
<p>In this blog we are going to show a hands on lab demonstration of how Juniper Contrail SDN controller allows the cloud administrator to seamlessly integrate Bare-metal servers into a virtualized network environment that may consist of existing virtual machine workloads. The accompanying video shows the steps required to configure various features of this lab.</p>
<p><iframe loading="lazy" src="https://www.youtube.com/embed/PXHZAP1rXM0" width="650" height="370" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>The solution uses standards based OVSDB protocol to program the Ethernet Ports, VLANS and the MAC table entries on the switches and hence any switch that supports OVSDB protocol can be used as the top of the rack switch to implement Contrail BMS solution and we have already demonstrated one such solution with Cumulus Linux Switches in this <a href="https://www.youtube.com/watch?v=PxG9Dfa1QkE">video.</a></p>
<p>Specifically, in this blog we are going to use QFX5100 and ARISTA as the two top of rack switches.</p>
<h4>Setup:</h4>
<p>The figure below shows the physical topology which consists of an all in one Contrail controller and compute node (IP=10.84.30.34) where we are going to spin the VMs. The TSN node has a IP address of 10.84.30.33. The second server (10.84.30.33) is going to act as the TSN node which is going to run the TOR agent (OVSDB clients) for the two TOR switches and also going to provide the network services such as ARP/DHCP/DSN etc. to the bare-metal services through a service known as TOR services node. TSN node vRouter forwarder translates the OVSDB message exchanges done with the TOR switches to XMPP messages which is advertised to the Control node. This is done by vRouter forwarder agent running on the TSN node.</p>
<p>The TWO switches QFX5100 (IP=10.84.30.44) &amp; Arista 7050 (IP=10.84.30.7.38) have one bare-metal servers connected to them on the 1G interfaces as shown in the figure. In the case of Arista, the OVSDB session is established not with the TOR but with the cloudvision VM. Cloudvision in turn is going to push the OVSDB state into the TOR switch using Arista&#8217;s cloudvision protocol. 10.84.0.0/16 is DC fabric underlay subnet. 10.84.63.144 is the loopback IP address of QFX5100 and for Arista switch it is 10.84.63.189.</p>
<p>Finally, the MX SDN gateway (10.84.63.133) is also connected to the DC fabric. The MX gateway provides two main functionalities. 1. Public access to VMs and bare-metal servers and 2. required to provide inter-VN connectivity between the bare-metal servers.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image1.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7088" src="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image1.png" alt="Contrail integration with Arista TORs_blog_image1" width="800" height="546" data-id="7088" /></a></p>
<p>&nbsp;</p>
<p>The MX gateway provides two main functionalities:1. Public access to VMs and bare-metal servers and 2. required to provide inter-VN connectivity between the bare-metal servers.</p>
<p>1. Public access to VMs and bare-metal servers and 2. required to provide inter-VN connectivity between the bare-metal servers.</p>
<p>2. required to provide inter-VN connectivity between the bare-metal servers.</p>
<h4>Provisioning &amp; control-plane:</h4>
<p>The TOR switches can be added to the cluster during provisioning of the cluster itself or can be added to an existing cluster using fab tasks. The TSN and the TOR agent roles are configured under &#8216;tsn&#8217; and &#8216;toragent&#8217; roles in the env.roledefs section of testbed.py. All the TOR switch related parameters such as the TOR switch loopback address (VXLAN VTEP source), OVSDB transport type (PSSL vs TCP), OVSDB TCP port etc. is defined under the env.tor_agent section of testbed.py. Please refer the github link provided at in the references section for more details.</p>
<p>Once the cluster is provisioned along with the TOR switches the only other thing that is required is to configure the TOR switches with the required configurations for OVSDB, VXLAN etc. Once all these are configured properly this should result in the control plane protocols being established between the various nodes, TOR switches and the MX gateway. The configurations for the TOR switches and the MX gateway is provided below.</p>
<p>TSN has OVSDB sessions to QFX5100 &amp; the Arista cloudvision VM. TSN also has a XMPP session with the control node. Control and MX gateway exchange BGP routes over the IBGP session.</p>
<h4>Data Plane:</h4>
<p>As one can see in the video we are going to create two virtual networks RED (IP Subnet 172.16.1.0/24) and BLUE (IP Subnet 172.16.2.0/24). QFX5100 ge-0/0/16 logical interface is added to the RED VN and the Arista switch Et3 interface is added to the BLUE VN. In addition to this on the all in one contrail node we are going to create one VM each for the RED and the BLUE VNs.</p>
<p>The resulting data-plane interactions are as shown in fig 3. The RED and BLUE VTEPs are created on the QFX5100 and Arista 7050 switches by OVSDB intra-VN communication between the bare-metal server and the virtual machine in the RED and BLUE VNs are facilitated by the VXLAN tunnels that are setup between the TOR switches and the compute node (all in one node in this case 10.84.30.34). Broadcast Ethernet frames for protocols such as ARP, DHCP etc., are directed towards the TSN node using VXLAN tunnels established between the TOR switches and the TSN nodes. All the inter-VN traffic originated by the bare-metal server which is destined to another bare-metal server or a VM in a different VN (RED to BLUE or vice versa) is sent to the EVPN instances for the respective VNs on the MX SDN gateway wherein routing takes place between the RED and BLUE VRFs. The VRF and the EVPN (virtual-switch routing-instance) configuration on the MX gateway is automated using the Contrail Device Manager feature.</p>
<p>In addition to the all in one compute node has VXLAN and MPLSoGRE tunnels to the MX gateway for L2 and L3 stretch of the virtual network.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image3.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7090" src="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image3.png" alt="Contrail integration with Arista TORs_blog_image3" width="800" height="547" data-id="7090" /></a></p>
<p>&nbsp;</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image4.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7091" src="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image4.png" alt="Contrail integration with Arista TORs_blog_image4" width="800" height="556" data-id="7091" /></a></p>
<p>Resulting logical tenant virtual network logical connectivity is as shown below.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image5.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7092" src="http://www.opencontrail.org/wp-content/uploads/2016/06/Contrail-integration-with-Arista-TORs_blog_image5.png" alt="Contrail integration with Arista TORs_blog_image5" width="800" height="676" data-id="7092" /></a></p>
<h4>References:</h4>
<p>Sample testbed.py file &amp; the MX GW, QFX5100 , Arista Switch, Cloudvision configurations<br />
<strong><a href="https://github.com/vshenoy83/Contrail-BMS-Configs.git">https://github.com/vshenoy83/Contrail-BMS-Configs.git</a></strong></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>OpenStack Neutron IPv6 support in OpenContrail SDN</title>
		<link>https://tungsten.io/openstack-neutron-ipv6-support-in-opencontrail-sdn/</link>
		
		<dc:creator><![CDATA[Jakub Pavlik]]></dc:creator>
		<pubDate>Fri, 25 Mar 2016 05:20:06 +0000</pubDate>
				<category><![CDATA[DataCenter]]></category>
		<category><![CDATA[ipv6]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=6950</guid>

					<description><![CDATA[This is a guest blog from tcpCloud, authored by Marek Celoud &#38; Jakub Pavlik (tcp cloud engineers). To see the original post,click here. As private cloud (primary based on OpenStack)...]]></description>
										<content:encoded><![CDATA[<p><em>This is a guest blog from tcpCloud, authored by Marek Celoud &amp; Jakub Pavlik (tcp cloud engineers). To see the original post,<a href="http://www.tcpcloud.eu/en/blog/2016/03/23/openstack-neutron-ipv6-support-opencontrail-sdn/" target="_blank">click here</a>.</em></p>
<p>As private cloud (primary based on OpenStack) deployers and integrators lots of customer ask as about support of IPv6. Most of our deployments run on OpenContrail SDN&amp;NFV. Reasons are described in our previous blogs (<a class="reference external" href="http://www.tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/">http://www.tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/</a>) . OpenContrail SDN supports IPv6 for quite long, but there is not so many real tests. Therefore we decided to share procedure how we configured and used IPv6 in OpenStack.</p>
<p>This short blog desribes support of IPv6 in OpenStack using Neutron plugin for SDN/NFV &#8211; OpenContrail.</p>
<p>With cloud deployments there is significant growth of need for public IP addresses. These deployments are facing problems due to lack of IPv4 addresses. One of the solutions is to migrate to public IPv6.</p>
<p>We start with capability of IPv6 for internal communication between virtual machines within same virtal network and across different virtual network. Then we show how to expand IPv6 public addresses to external world. In our case we use Juniper MX routers as cloud gateway.</p>
<h2>Creating IPv6 network</h2>
<p>We need to consider few things when creating IPv6 virtual network. First one is adding also IPv4 subnet, because without IPv4 address instance can not connect to nova metadata api. Cloud images are built to use cloud-init to connect to API on 169.254.169.254:80 address. So if you create network without IPv4 subnet, you will not receive metadata to your instance. Second consideration is whether to you want to go to internet with your IPv6 capable instances. There is currently problem with IPv6 floating IP pool, so if you want to expand to external world, you need to boot to network with associated route target.</p>
<p>We first create private IPv6 network for demonstation.</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-6951 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv61.png" alt="ipv61" width="967" height="514" data-id="6951" /></p>
<p>When the network is created we can boot instances. We will boot 2 of them to demonstrate functional communication. You will probably need to modify network interface configuration, because there is not enabled dhcp for IPv6. For nonpreemptive recieve you can use:</p>
<pre><span style="font-family: 'courier new', courier;">#dhclient -6</span></pre>
<p><img loading="lazy" decoding="async" class="size-full wp-image-6953 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv63.png" alt="ipv63" width="559" height="176" data-id="6953" /></p>
<p>As you can see, you have both IPv4 and IPv6 address associated with interface of instance.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6954" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv64.png" alt="ipv64" width="815" height="241" data-id="6954" /></p>
<p>Before testing communication, we need to modify security groups to enable traffic. For testing purposes we will enable everything.</p>
<p><img loading="lazy" decoding="async" class="alignleft size-full wp-image-6952" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv62.png" alt="ipv62" width="1855" height="92" data-id="6952" /></p>
<p>We choose ubuntu-ipv6-1 from instance list and try to ping instance ubuntu-ipv6-2 with fd00::3 IPv6 address.</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-6955 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv65.png" alt="ipv65" width="691" height="149" data-id="6955" /></p>
<p>As you can see, we are now able to ping other instance.</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-6957 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv66.png" alt="ipv66" width="514" height="203" data-id="6957" /></p>
<p>This capability is nice, but not very useful without connecting to external world. We will create route with associated route target to expand routes to Juniper MX routers via BGP. In the picture below is sample architecture. There is one VRF CLOUD-INET created on each of MX routers. The route target associated with this VRF matches route target added to virtual network in Contrail. In the picture is demonstrated both IPv4 and IPv6 addresses propagated to same VRF. There is also INET virtual-router, that is connected to VRF via lt tunnel interfaces running ospf and ospf3. From this virtual-router is aggregated default route ::/0 from all internet routes from upstream EBGP.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6956" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv66-expanded.png" alt="ipv66 expanded" width="617" height="883" data-id="6956" /></p>
<p>There are few things to configure on MX routers to enable IPv6 traffic from cloud. First is enabling ipv6 tunneling through mpls tunnels.</p>
<pre><span style="font-family: 'courier new', courier;">protocols {
    mpls {
        ipv6-tunneling;
        interface all;
        }</span></pre>
<p>It is also good practice to filter what routes you export and import to and from cloud. We only need default route present in cloud. And we also want to filter only IPv6 addresses to be imported from Contrail, because of IPv4 pool created with IPv6 virtual network.</p>
<pre><span style="font-family: 'courier new', courier;">
policy-statement CLOUD-INET-EXPORT {
    term FROM-MX-IPV6 {
        from {
            protocol ospf3;
            route-filter ::/0 exact;
        }
        then {
            community add CLOUD-INET-EXPORT-COMMUNITY;
            accept;
        }
    }
    term LAST {
        then reject;
    }
}
policy-statement CLOUD-INET-IMPORT {
    term FROM-CONTRAIL-IPV6 {
        from {
            family inet6;
            community CLOUD-INET-IMPORT-COMMUNITY;
            route-filter 2a06:f6c0::/64 orlonger;
        }
        then accept;
    }
    term LAST {
        then reject;
    }
}
community CLOUD-INET-EXPORT-COMMUNITY members target:64513:10;
community CLOUD-INET-IMPORT-COMMUNITY members target:64513:10;
</span></pre>
<p>So now we create network 2a06:f6c0::/64 and we associate route target 64513:10 to this network. We can also make it shared so all tenants can boot in this network. Once we create instance to this network, there is already routing information in MX routing table.</p>
<pre><span style="font-family: 'courier new', courier;">
# run show route table CLOUD-INET.inet6.0

CLOUD-INET.inet6.0: 8 destinations, 9 routes (8 active, 0 holddown, 0 hidden)
+ = Active Route, - = Last Active, * = Both

::/0               *[OSPF3/150] 20:37:13, metric 0, tag 0
                    &gt; to fe80::6687:8800:0:2f7 via lt-0/0/0.3
2a06:f6c0::3/128   *[BGP/170] 00:00:15, localpref 100, from 10.0.106.84
                      AS path: ?, validation-state: unverified
                    &gt; via gr-0/0/0.32789, Push 1046
                    [BGP/170] 00:00:15, localpref 100, from 10.0.106.85
                      AS path: ?, validation-state: unverified
                    &gt; via gr-0/0/0.32789, Push 1046</span></pre>
<p>We can also verify that default route is propagated by ispecting routing tables in Contrail.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6959" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv610.png" alt="ipv610" width="1728" height="696" data-id="6959" /></p>
<p>When we verify that instance have public IPv6 address, we can try to access internet.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6958" src="http://www.opencontrail.org/wp-content/uploads/2016/03/ipv67.png" alt="ipv67" width="810" height="240" data-id="6958" /></p>
<div id="creating-ipv6-network" class="section">
<p>ping google</p>
</div>
<div id="conclusion" class="section">
<h2>Conclusion</h2>
<p>We proved that OpenContrail SDN solution is fully IPv6 capable with cloud platform OpenStack for private and public communication and communicate directly with edge routers as Juniper MX, Cisco ASR, etc.</p>
</div>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Multi DataCenter Interconnect using OpenContrail</title>
		<link>https://tungsten.io/multi-datacenter-interconnect-using-opencontrail/</link>
		
		<dc:creator><![CDATA[Ranjini Rajendran]]></dc:creator>
		<pubDate>Fri, 31 Jul 2015 02:14:04 +0000</pubDate>
				<category><![CDATA[Automation]]></category>
		<category><![CDATA[BGPaaS]]></category>
		<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=6454</guid>

					<description><![CDATA[In typical Enterprise Data Center deployments, the data center would span across multiple sites and there would be a need to have workloads across these sites. This would boil down...]]></description>
										<content:encoded><![CDATA[<p>In typical Enterprise Data Center deployments, the data center would span across multiple sites and there would be a need to have workloads across these sites. This would boil down to extending virtual networks to these multiple sites and ability to launch workloads on any site and be able to communicate between these workloads seamlessly as if they are in the same cluster.</p>
<p>In OpenContrail, this is made possible by federating the controllers in the different sites of a Multi-site DC without the need of a physical gateway. The control nodes in each site are peered with other sites using BGP. With this it is possible to stretch both L2 and L3 networks across multiple DCs.</p>
<p>The physical topology in this case is as shown below:</p>
<p><img loading="lazy" decoding="async" class=" wp-image-6456 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2015/07/physical_topology_opencontrail_controller_federation_blogpost.png" alt="physical_topology_opencontrail_controller_federation_blogpost" width="700" height="166" data-id="6456" /></p>
<p>The two DCs in different locations are having two different AS numbers and their control nodes are federated using BGP. The virtual networks can span across these two DCs. Also the network policies and security groups can also work seamlessly across these two DCs.</p>
<p>The logical view of the system is shown below:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/logical_view_opencontrail_controller_federation_blogpost.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6455" src="http://www.opencontrail.org/wp-content/uploads/2015/07/logical_view_opencontrail_controller_federation_blogpost.png" alt="logical_view_opencontrail_controller_federation_blogpost" width="700" height="247" data-id="6455" /></a></p>
<p>Logically, the virtual machines spawned in another DC in the same VN can talk to each other like VMs in the same DC. They don’t see any difference.</p>
<p>A demo video on how controller can be federated in OpenContrail is available here:</p>
[video_lightbox_youtube video_id=&#8221;HIslWml97Ps&#8221; width=&#8221;720&#8243; height=&#8221;540&#8243; auto_thumb=&#8221;1&#8243;]
<p>With this, we have shown that using controller federation in OpenContrail; we can seamlessly stretch virtual networks ( both Layer 3 and Layer 2), network policies and security groups across multiple remote data center locations.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>A journey of a packet within OpenContrail</title>
		<link>https://tungsten.io/a-journey-of-a-packet-within-opencontrail/</link>
		
		<dc:creator><![CDATA[Sylvain Afchain]]></dc:creator>
		<pubDate>Wed, 29 Jul 2015 20:40:58 +0000</pubDate>
				<category><![CDATA[Automation]]></category>
		<category><![CDATA[BGPaaS]]></category>
		<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=6435</guid>

					<description><![CDATA[This is a guest blog by Sylvain Afchain from RedHat. Click here for the original post. In this post we will see how a packet generated by a VM is...]]></description>
										<content:encoded><![CDATA[<p><em>This is a guest blog by <span class="author vcard"><a class="url fn n" title="Sylvain Afchain" href="https://twitter.com/s_afchain" target="_blank" rel="author">Sylvain Afchain </a></span> from RedHat. <a href="http://techs.enovance.com/7640/a-journey-of-a-packet-within-opencontrail" target="_blank">Click here</a> for the original post.</em></p>
<p>In this post we will see how a packet generated by a VM is able to reach another VM or an external resource, what are the key concepts/components in the context of Neutron using the OpenContrail plugin. We will focus on OpenContrail, how it implements the overlay and the tools that it provides to check/troubleshoot how the packet are forwarded. Before getting started, I’ll give a little overview of the key concepts of OpenContrail.</p>
<h4>Virtual networks, Overlay with OpenContrail</h4>
<p>For the overlay, OpenContrail uses MPLS L3VPNs and MPLS EVPNs in order to address both l3 overlay and l2 overlay. There are a lot of components within OpenContrail, however we will focus on two key components – controller and the vRouter.</p>
<p>For the control plane each controller acts as a BGP Route Reflector using the BGP and the XMPP protocols. BGP is used between the controllers and the physical routers. XMPP is used between the controllers and the vRouters. The XMPP protocol transports BGP route announcements but also some other informations for non routing needs.</p>
<p>For the data plane, OpenContrail supports GRE/VXLAN/UDP for the tunneling. OpenContrail requires the following features to be supported by the gateway router :</p>
<ul>
<li>L3VPN
<ul>
<li><a href="http://tools.ietf.org/html/rfc4364">http://tools.ietf.org/html/rfc4364</a></li>
</ul>
</li>
<li>MP-BGP
<ul>
<li><a href="http://tools.ietf.org/html/rfc4760">http://tools.ietf.org/html/rfc4760</a></li>
</ul>
</li>
<li>Dynamic Tunneling</li>
</ul>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image1.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6436" src="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image1.png" alt="safchain_blogpost_728_image1" width="533" height="300" data-id="6436" /></a></p>
<p>In this post we will focus on the data plane area.</p>
<h2 id="3">The packet’s journey</h2>
<p>In order to show what is the journey of a packet, let’s play with the following topology, where we have two VMs on two different networks connected thanks to a router.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6437" src="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image2.png" alt="safchain_blogpost_728_image2" width="564" height="552" data-id="6437" /></p>
<p>Assuming we have allowed the ICMP packets by setting the security groups accordingly we can start a ping from <i>vm1</i> toward <i>vm2</i>.</p>
<p>There are a lot of introspection tools within OpenContrail which can be used to get a clear status on how the packets are forwarded.</p>
<p>Initiating a ping between <i>vm1</i> and <i>vm2</i>, we can check step by step where the packets go.</p>
<p>Since the VMs are not on the same network, they will both use their default gateway. The local vRouter answers to the ARP request of the default gateway IP with its own MAC.</p>
<pre><span style="font-family: 'courier new', courier;">
vm1$ ip route
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0  src 10.0.0.3
 
$ cat /proc/net/arp
IP address       HW type     Flags       HW address            Mask     Device
10.0.0.1         0x1         0x2         00:00:5e:00:01:00     *        eth0</span></pre>
<p>Now that we have seen that the packets will be forwarded to the local vRouter, we are going to check how the vRouter will forward them.</p>
<p>So let’s start by checking at the data plane layer by browsing the vRouter agent introspect Web interface running on the compute nodes hosting our VMs at <i>http://&lt;vrouter agent ip&gt;:8085/agent.xml</i></p>
<p>There is a plenty of sub-interfaces, but we will only use three of them:</p>
<ul>
<li>VrfListReq, http://&lt;vrouter agent ip&gt;:8085/Snh_VrfListReqWhich gives you the networks and the VRFs related. For a given VRF – let’s say the Unicast VRF (ucindex) – we can see all the routes.</li>
</ul>
<ul>
<li>ItfReq, http://&lt;vrouter agent ip&gt;:8085/Snh_ItfReqWhich gives you all the interfaces handled by the vRouter.</li>
<li>MplsReq, http://&lt;vrouter agent ip&gt;:8085/Snh_MplsReqWhich gives all the association MPLS Label/NextHop for the given vRouter</li>
</ul>
<p>These interfaces are just XML document rendered thanks to a XSL stylesheet, so can be easily processed by some monitoring scripts for example.</p>
<p>We can start by the interfaces (ItfReq) introspect page to find the TAP interface corresponding to VM1. The name of the TAP contains a part of the neutron port ID.</p>
<p><img loading="lazy" decoding="async" class="aligncenter wp-image-6445" src="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image6.png" alt="safchain_blogpost_728_image6" width="739" height="300" data-id="6445" /></p>
<p>Beside the interface we see the VRF name associated to the network that the interface belong to. On the same line we have some others informations, security group, floating-ips, VM id, etc.</p>
<p>Clicking on the VRF link brings us to the index page of this VRF. We see that we have links to VRFs according to their type: Unicast, Multicast, Layer 2. By default, OpenContrail doesn’t handle the Layer 2. As said before most of the Layer 2 traffic from the virtual machines are trapped by the local vRouter which acts as an ARP responder. But some specific packets like broadcasts still need to be handled, that’s why there is a specific Layer 2 VRF.</p>
<p><img loading="lazy" decoding="async" class="aligncenter wp-image-6442 size-full" src="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image3.png" alt="safchain_blogpost_728_image3" width="688" height="241" data-id="6442" /></p>
<p>Clicking on the link in the <i>ucindex</i> (Unicast) column, we can see all the unicast L3 routes of our virtual network handled by this vRouter. Since <i>vm1</i> should be able to reach vm2, we should see a route with the IP of <i>vm2</i>.</p>
<p><img loading="lazy" decoding="async" class="aligncenter wp-image-6443" src="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image4.png" alt="safchain_blogpost_728_image4" width="842" height="400" data-id="6443" /></p>
<p>Thanks to this interface we see that in order to reach the IP 192.168.0.3 which is the IP of our <i>vm2</i>, the packet is going to be forwarded through a GRE tunnel whose endpoint is the IP of the compute node hosting <i>vm2</i>. That’s what we see in the “<i>dip</i>” (Destination IP) field. We see that the packet will be encapsulated in a MPLS packet. The MPLS label will be 16, as shown in the label column.</p>
<p>Ok, so we saw at the agent level how the packet is going to be forwarded, but we may want to check on the datapath side. OpenContrail provides command line tools for that purpose.</p>
<p>In the case of the agent for instance, we can see the interfaces handled by the vRouter kernel module and the associated VRF.</p>
<pre><span style="font-family: 'courier new', courier;">$ vif --list
Vrouter Interface Table
 
Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
      Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
      D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
      Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, 
      Mon=Interface is Monitored, Uuf=Unknown Unicast Flood
 
vif0/0      OS: eth0
           Type:Physical HWaddr:fa:16:3e:68:f9:e8 IPaddr:0
           Vrf:0 Flags:TcL3L2Vp MTU:1514 Ref:5
           RX packets:1598309  bytes:315532297 errors:0
           TX packets:1407307  bytes:383580260 errors:0
 
vif0/1      OS: vhost0
           Type:Host HWaddr:fa:16:3e:68:f9:e8 IPaddr:a2b5b0a
           Vrf:0 Flags:L3L2 MTU:1514 Ref:3
           RX packets:1403461  bytes:383378275 errors:0
           TX packets:1595855  bytes:315456061 errors:0
 
vif0/2      OS: pkt0
           Type:Agent HWaddr:00:00:5e:00:01:00 IPaddr:0
           Vrf:65535 Flags:L3 MTU:1514 Ref:2
           RX packets:4389  bytes:400688 errors:0
           TX packets:6931  bytes:548756 errors:0
 
vif0/3      OS: tapa87ad91e-28
           Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:0
           Vrf:1 Flags:PL3L2 MTU:9160 Ref:6
           RX packets:565  bytes:105481 errors:0
           TX packets:587  bytes:80083 errors:0
 
vif0/4350   OS: pkt3
           Type:Stats HWaddr:00:00:00:00:00:00 IPaddr:0
           Vrf:65535 Flags:L3L2 MTU:9136 Ref:1
           RX packets:3  bytes:294 errors:0
           TX packets:3  bytes:252 errors:0
 
vif0/4351   OS: pkt1
           Type:Stats HWaddr:00:00:00:00:00:00 IPaddr:0
           Vrf:65535 Flags:L3L2 MTU:9136 Ref:1
           RX packets:10  bytes:840 errors:0
           TX packets:10  bytes:840 errors:0</span></pre>
<p>We have our TAP interface at this index 3 and the VRF associated which is the number 1.</p>
<p>Let’s now check the routes for this VRF. For that purpose we use the rt command line.</p>
<pre><span style="font-family: 'courier new', courier;">$ rt --dump 1
Vrouter inet4 routing table 0/1/unicast
Flags: L=Label Valid, P=Proxy ARP, T=Trap ARP, F=Flood ARP
 
Destination          PPL        Flags        Label         Nexthop    Stitched MAC(Index)
 
...
192.168.0.3/32         32           LP         16             19        -
...</span></pre>
<p>We see that the MPLS label used is 16. In order to know how the packet will be forwarded we have to check the NextHop used for this route.</p>
<pre><span style="font-family: 'courier new', courier;">$ nh --get 19
Id:19         Type:Tunnel    Fmly: AF_INET  Flags:Valid, MPLSoGRE,   Rid:0  Ref_cnt:2 Vrf:0
             Oif:0 Len:14 Flags Valid, MPLSoGRE,  Data:fa 16 3e 4b f6 05 fa 16 3e 68 f9 e8 08 00
             Vrf:0  Sip:10.43.91.10  Dip:10.43.91.12</span></pre>
<p>We have almost the same informations that the agent gave us. Here in the Oif field, we have the interface where the packet will be sent to the other compute node. Thanks to the vif command line we can get the details about this interface.</p>
<pre><span style="font-family: 'courier new', courier;">$ vif --get 0
Vrouter Interface Table
 
Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
      Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
      D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
      Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
      Uuf=Unknown Unicast Flood
 
vif0/0      OS: eth0
           Type:Physical HWaddr:fa:16:3e:68:f9:e8 IPaddr:0
           Vrf:0 Flags:TcL3L2Vp MTU:1514 Ref:5
           RX packets:1602164  bytes:316196179 errors:0
           TX packets:1410642  bytes:384855228 errors:0</span></pre>
<p>As the packet will go through the eth0 interface, a tcpdump should confirm what we described above.</p>
<pre><span style="font-family: 'courier new', courier;">$ sudo tcpdump -n -i eth0 dst 10.43.91.12
12:13:16.908957 IP 10.43.91.10 &gt; 10.43.91.12: GREv0, 
length 92: MPLS (label 16, exp 0, [S], ttl 63) 
IP 10.0.0.3 &gt; 192.168.0.3: ICMP echo request, id 5889, seq 43, length 64</span></pre>
<p>As the tunnel endpoint shows, the packet will be directly forwarded to the compute node that is hosting the destination VM, not using a third party routing device.</p>
<p>On the other side, the vRouter on the second compute node will receive the encapsulated packet. According to the MPLS Label, it does a lookup on a MPLS Label/NextHop as we can see on its introspect.</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6444" src="http://www.opencontrail.org/wp-content/uploads/2015/07/safchain_blogpost_728_image5.png" alt="safchain_blogpost_728_image5" width="678" height="538" data-id="6444" /></p>
<p>As we can see here the NextHop field for the Label 16 is the TAP interface of our second VM. On the datapath side we can check the same informations. Checking the MPLS Label/NextHop table :</p>
<pre><span style="font-family: 'courier new', courier;">$ mpls --get 16
MPLS Input Label Map
 
  Label    NextHop
-------------------
     16        14</span></pre>
<p>..and finally the NextHop and the interface with the following commands :</p>
<pre><span style="font-family: 'courier new', courier;">$ nh --get 14
Id:14         Type:Encap     Fmly: AF_INET  Flags:Valid, Policy,   Rid:0  Ref_cnt:4 Vrf:1
             EncapFmly:0806 Oif:3 Len:14 Data:02 8a 39 ff 98 d3 00 00 5e 00 01 00 08 00

$ vif --get 3
Vrouter Interface Table
 
Flags: P=Policy, X=Cross Connect, S=Service Chain, Mr=Receive Mirror
      Mt=Transmit Mirror, Tc=Transmit Checksum Offload, L3=Layer 3, L2=Layer 2
      D=DHCP, Vp=Vhost Physical, Pr=Promiscuous, Vnt=Native Vlan Tagged
      Mnp=No MAC Proxy, Dpdk=DPDK PMD Interface, Rfl=Receive Filtering Offload, Mon=Interface is Monitored
      Uuf=Unknown Unicast Flood
 
vif0/3      OS: tap8a39ff98-d3
           Type:Virtual HWaddr:00:00:5e:00:01:00 IPaddr:0
           Vrf:1 Flags:PL3L2 MTU:9160 Ref:6
           RX packets:2957  bytes:293636 errors:0
           TX packets:3085  bytes:297115 errors:0</span></pre>
<p>This post was just an overview on how the packets are forwarded from one node to another and what are the interfaces/tools that you can use for troubleshooting purpose. One of the interesting thing with OpenContrail is that almost all the components have their own introspect interface helping you a lot during troubleshooting sessions. As we saw, the routing is fully distributed in OpenContrail, each vRouter handles a part of the routing using well known routing protocols like BGP/MPLS which proved their ability to scale.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>OpenContrail SDN Lab testing 1 &#8211; ToR Switches with OVSDB</title>
		<link>https://tungsten.io/opencontrail-sdn-lab-testing-1-tor-switches-with-ovsdb/</link>
		
		<dc:creator><![CDATA[Jakub Pavlik]]></dc:creator>
		<pubDate>Tue, 14 Jul 2015 00:03:02 +0000</pubDate>
				<category><![CDATA[Automation]]></category>
		<category><![CDATA[BGPaaS]]></category>
		<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=6368</guid>

					<description><![CDATA[This is a guest blog from tcpCloud authored by Marek Celoud &#38; Jakub Pavlik (tcp cloud engineers) along with Rostislav Safar (Arrow ECS network engineer). To see the original post,...]]></description>
										<content:encoded><![CDATA[<p>This is a guest blog from tcpCloud authored by Marek Celoud &amp; Jakub Pavlik (tcp cloud engineers) along with Rostislav Safar (Arrow ECS network engineer). To see the original post, <a href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/">click here</a>.</p>
<p>Nobody doubts that OpenStack is the best open source project for private and public clouds today. OpenStack has begun to be perceived as a standard platform not only for laboratory or development environments, but has became suitable for large enterprises and service providers as well. This OpenStack revolution brought along a new topic called SDN. Software Defined Networking (SDN) can be seen as kind of buzzword and lot of our customers thought that they did not have a need for SDN, because their environment is not large enough, too dynamic, etc. But SDN is not just about scaling, it gives you opportunity to use NFV (Network Function Virtualization) like LbaaS, FWaaS, VPNaaS. Finally the most important reason is that Neutron supports vendor driven SDN solutions (Neutron vendor plugin) and it is possible to use this vendor SDN solution together with OpenStack in real enterprise production. The reason to use vendor plugin is that upstream Neutron solution with OpenVSwitch (DVR or L3 agent) does not provide native High Availability, scalability, performance and service provider features (L3VPN, EVPN) required by the large enterprises.</p>
<p>Our team in tcp cloud has been solving very advanced network questions and problems for different service providers for past 6 months. Based on that we had to choose SDN solution, which would let us to satisfy customer requirements and provide robust and stable cloud solution. We compared several SDN solutions from different vendors and chose OpenContrail, simply because “it works” and it is not only slideware or any other from of secret marketing product, which can be installed only by vendor behind the closed door.</p>
<p>Therefore tcp cloud together with Arrow ECS decided to create LAB environment, where we can proof and verify all marketing messages in real deployment to show customer that given solution exists, works and we know how to implement it without hidden issues.</p>
<p>This blog is first one from series of articles about OpenContrail SDN Lab testing, where we would like to cover topics like:</p>
<ul class="simple">
<li>L3VPN termination at cloud environment</li>
<li>VxLAN to EVPN Stitching for L2 Extension</li>
<li>VxLAN Routing and SDN</li>
<li>OVSDB Provides Control for VxLAN</li>
<li>Translating between SDN Types</li>
<li>MPLSoverGRE or VxLAN encapsulation</li>
<li>Kubernetes integration (container virtualization)</li>
</ul>
<div id="tor-integration-overview" class="section">
<h2>TOR INTEGRATION OVERVIEW</h2>
<p>SDN brings idea that everything can be virtualized, however there are still technological or legal limitation, which block possibility to integrate them into overlay like:</p>
<ul class="simple">
<li><strong>Legacy hardware infrastructure</strong> &#8211; PowerVM, HP Itanium, OEM appliances</li>
<li><strong>Licenses</strong> &#8211; some software cannot be operated on virtual hardware</li>
<li><strong>Physical network appliances</strong> &#8211; firewalls, load balancers, etc</li>
<li><strong>Database clusters</strong> &#8211; Oracle SuperCluster, etc</li>
</ul>
<p>Therefore there must be way how to connect the underlay world with overlay. OpenContrail provides 3 ways to connect overlay with underlay:</p>
<ul class="simple">
<li><strong>Link Local Services</strong> &#8211; it might be required for a virtual machine to access specific services running on the fabric infrastructure. For example, a VM requiring access to the backup service running in the fabric. Such access can be provided by configuring the required service as a link local service.</li>
<li><strong>Router L3/L2 gateway (VRF, EVI)</strong> &#8211; standard cloud gateway used for external routing networks. Standard use-case is OpenStack floating IPs. This will be discuss in next blog article.</li>
<li><strong>ToR switch</strong> &#8211; top-of-rack switch provides L2 connection for baremetal server or any other L2 service.</li>
</ul>
<p>This blog focuses at ToR switch integration and should answer following questions:</p>
<ul class="simple">
<li><strong>Baremetal server into overlay VN</strong></li>
<li><strong>VxLAN with OVSDB terminated at ToR switch</strong></li>
<li><strong>Multi vendor support for ToR switches with OVSDB</strong> &#8211; OpenContrail lets to use any Switch vendor with standard OVSDB protocol.</li>
<li><strong>Redundantly connected Bare Metal Servers</strong> &#8211; Physical port in virtual network is amazing, but how to solve HA for this server?</li>
<li><strong>High Availability for ToR configuration</strong> &#8211; Functional test is not equal to production setup.</li>
</ul>
<p>The beginning covers OpenContrail’s support for ToR switches with OVSDB is explained at official Juniper documentation. The next section introduces the Lab infrastructure and architecture with server role description. At the end Contrail deployment is briefly described. The last two sections cover implementation of ToR Agent with Juniper QFX and OpenVSwitch.</p>
</div>
<div id="opencontrail-support-for-tor-switch-and-ovsdb" class="section">
<h2>OPENCONTRAIL SUPPORT FOR TOR SWITCH AND OVSDB</h2>
<p>This overview is taken from <a id="id1" class="reference internal" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#contrailtor">[ContrailToR]</a>. Since 2.1 release, Contrail supports extending a cluster to include bare metal servers or other virtual instances connected to a top-of-rack (ToR) switch that supports the Open vSwitch Database Management (OVSDB) Protocol. The bare metal servers and other virtual instances can belong to any of the virtual networks configured in the Contrail overlay, facilitating communication with the virtual instances running in the OpenStack cluster. Contrail policy configuration is used to control behaviour of this communication.</p>
<p>OVSDB protocol is used to configure the TOR switch and to import dynamically-learned addresses. VXLAN encapsulation is used in the data plane for communication with the TOR switch.</p>
<div id="tor-services-node-tsn" class="section">
<h3>TOR Services Node (TSN)</h3>
<p>The TSN acts as the multicast controller for the TOR switches. The TSN also provides DHCP and DNS services to the bare metal servers or virtual instances running behind TOR ports.</p>
<p>The TSN receives all the broadcast packets from the TOR, and replicates them to the required compute nodes in the cluster and to other EVPN nodes. Broadcast packets from the virtual machines in the cluster are sent directly from the respective compute nodes to the TOR switch.</p>
<p>The TSN can also act as the DHCP server for the bare metal servers or virtual instances, leasing IP addresses to them, along with other DHCP options configured in the system. The TSN also provides a DNS service for the bare metal servers.</p>
<p>Multiple TSN nodes can be configured in the system based on the scaling needs of the cluster.</p>
</div>
<div id="contrail-tor-agent" class="section">
<h3>Contrail TOR Agent</h3>
<p>A TOR agent provisioned in the Contrail cluster acts as the OVSDB client for the TOR switch, and all of the OVSDB interactions with the TOR are performed by using the TOR agent. The TOR agent programs the different OVSDB tables onto the TOR switch and receives the local unicast table entries from the TOR switch.</p>
<p>There is more information about <a id="id2" class="reference internal" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#contrailtor">[ContrailToR]</a>.</p>
</div>
</div>
<div id="actual-lab-environment" class="section">
<h2>ACTUAL LAB ENVIRONMENT</h2>
<p>Arrow LAB infrastructure consists of several Juniper boxes. The following figure captures the testing rack.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/rack_oc_image1.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6369" src="http://www.opencontrail.org/wp-content/uploads/2015/07/rack_oc_image1.png" alt="rack_oc_image1" width="573" height="400" data-id="6369" /></a></p>
<p>The following diagram shows high level network design of the lab environment.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/high-level-design.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6370" src="http://www.opencontrail.org/wp-content/uploads/2015/07/high-level-design.png" alt="high-level-design" width="548" height="300" data-id="6370" /></a></p>
<p>The following diagram shows logical architecture for TOR testing. As already mentioned we use two Juniper MX5 routers and QFX5100 switches.</p>
<ul class="simple">
<li><strong>CTPx</strong> &#8211; Using 4 physical servers as compute nodes. Each compute node is KVM hypervisor with Contrail vRouter.</li>
<li><strong>BMS01</strong> &#8211; Represents physical server with one 10Gbps port connected to QFX.</li>
<li><strong>TNS01</strong> &#8211; Physical server (can be virtual) for TOR Services Node with 2 ToR agents (QFX and OpenVSwitch).</li>
<li><strong>CTL</strong> &#8211; OpenStack and OpenContrail standalone controller. It contains all OpenStack APIs, database, message queue and OpenContrail control, config and analytics roles.</li>
<li><strong>OVS</strong> &#8211; Physical server with OpenVSwitch installed that is used as ToR switch. Details are described in section with openvswitch integration.</li>
<li><strong>BMS2</strong> &#8211; Physical server connected to OVS node.</li>
</ul>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/arrowlab.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6371" src="http://www.opencontrail.org/wp-content/uploads/2015/07/arrowlab.png" alt="arrowlab" width="466" height="400" data-id="6371" /></a></p>
</div>
<div id="actual-lab-environment" class="section">
<p>The next section describes installation Contrail with TNS (ToR agent).</p>
</div>
<div id="contrail-installation" class="section">
<h2>CONTRAIL INSTALLATION</h2>
<p>The lab testing was commited on Contrail 2.1 with OpenStack IceHouse. The reason for choosing version 2.1 over 2.2 is that official Contrail 2.2 release has been available since last week. Therefore ToR in high availability setup will be discusses in next blog post, because of significant performance and availabitlity improvements in release 2.2.</p>
<p>The installation guide is available at official Juniper <a id="id3" class="reference internal" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#site">[site]</a>. The following output shows our testbed.py file, where hosts are:</p>
<ul class="simple">
<li><strong>host1</strong> &#8211; OpenStack and OpenContrail standalone controller. It contains all OpenStack APIs, database, message queue and OpenContrail control, config and analytics role.</li>
<li><strong>host2 &#8211; 5</strong> &#8211; Compute nodes</li>
<li><strong>host6</strong> &#8211; TNS node with ToR agents. We installed first ToR agent by Fabric provisioning and second ToR manually in OpenVSwitch section.</li>
</ul>
<p>&nbsp;</p>
<pre><span style="font-family: 'courier new', courier;">from fabric.api import env 
#Management ip addresses of hosts in the clusterhost1='ubuntu@10.10.90.129' host2='ubuntu@10.100.10.2' host3='ubuntu@10.100.10.3' host4='ubuntu@10.100.10.4' host5='ubuntu@10.100.10.5' host6='ubuntu@10.100.10.6'ext_routers=[]router_asn= 65412  
host_build='ubuntu@10.10.90.129'
env.roledefs ={'all': [host1, host2, host3,host6],
     'cfgm': [host1], 
     'openstack': [host1], 
     'control': [host1], 
     'compute': [host2,host3,host4,host5,host6], 
     'collector': [host1], 
     'webui': [host1], 
     'database': [host1], 
     'build': [host_build], 
     'storage-master': [host1], 
     'storage-compute': [host2, host3,host4,host5], 
     'tsn': [host6], # Optional, Only to enable TSN. Only compute can support TSN'toragent': [host6], #, Optional, Only to enable Tor Agent. Only compute can support Tor Agent} 

env.openstack_admin_password ='arrowlab' 
env.hostnames ={'all': ['ctl01', 'cpt01', 'cpt02', 'cpt03', 'cpt04','tns01']} 

env.passwords ={ 
      host1: 'ubuntu', 
      host2: 'ubuntu', 
      host3: 'ubuntu', 
      host4: 'ubuntu', 
      host5: 'ubuntu', 
      host6: 'ubuntu', 
      host_build: 'ubuntu', 
} 

env.ostypes ={ 
      host1:'ubuntu', 
      host2:'ubuntu', 
      host3:'ubuntu', 
      host4:'ubuntu', 
      host5:'ubuntu', 
      host6:'ubuntu', 
} 

env.tor_agent ={ 
host6: [{'tor_ip':'10.100.10.1', # IP address of the TOR'tor_id':'1', # Numeric value to uniquely identify TOR 'tor_type':'ovs''tor_ovs_port':'9999', # the TCP port to connect on the TOR'tor_ovs_protocol':'tcp', # always tcp, for now'tor_tsn_ip':'10.100.10.6', # IP address of the TSN for this TOR'tor_tsn_name':'tns01', # Name of the TSN node'tor_name':'qfx5100', # Name of the TOR switch'tor_tunnel_ip':'10.10.80.6', # IP address of Data tunnel endpoint'tor_vendor_name':'QFX5100', # Vendor name for TOR switch'tor_http_server_port':'8085', # HTTP port for TOR Introspect}]}

</span></pre>
<h2>CONTRAIL BAREMETAL TOR IMPLEMENTATION WITH JUNIPER QFX5100</h2>
<p>This section shows how to setup Juniper QFX as ToR switch with baremetal server connection to the virtual network.</p>
<pre><span style="font-family: 'courier new', courier;"><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/torQFX1.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6374" src="http://www.opencontrail.org/wp-content/uploads/2015/07/torQFX1.png" alt="torQFX1" width="471" height="350" data-id="6374" /></a></span></pre>
<p class="section">Scenario for testing:</p>
<p class="section">1. Create virtual network <em>vxlannet</em> 10.0.10.0/24 and set VxLAN encapsulation through VNI 10<br />
2. Boot two instances VM1 (10.0.10.4) and VM2 (10.0.10.3) at two compute nodes into created virtual network <em>vxlannet3. </em><br />
3. Configure QFX for managing by OVSDB.<br />
4. Configure QFX port xe-0/0/40.1000 through Contrail as L2 10.0.10.100 to network <em>vxlannet</em>.<br />
5. Verify connectivity and configuration.</p>
<p>Step 1. and 2. is not covered in this blog post.</p>
<div id="qfx5100-configuration" class="section">
<h3>QFX5100 Configuration</h3>
<p>OVSDB software package must be installed in order to enable following configuration in QFX side. We run Junos version 14.1X53-D15.2 with JUNOS SDN Software Suite 14.1X53-D15.2.</p>
<p>The following output shows commands for configuration QFX switch to enable managing interface xe-0/0/40 through ovsdb. This configuration parameters have to meet values from <em>testbed.py</em>.</p>
<pre><span style="font-family: 'courier new', courier;">set interfaces lo0 unit 0 family inet address 10.10.80.6/32 
set switch-options ovsdb-managed 
set switch-options vtep-source-interface lo0.0 
set protocols ovsdb passive-connection protocol tcp port 6632 
set protocols ovsdb interfaces xe-0/0/40 

</span></pre>
<h3>Contrail Configuration</h3>
<p>Configuration for ToR agent was already defined in testbed.py, therefore the rest can be done in Contrail WebUI.</p>
<p>On Contrail side we had to add physical device QFX. After that we added physical and logical port for bare metal server.</p>
<p>In Contrail WebUI go to <em>Configure &gt; Physical Devices &gt; Physical Routers</em> and create new entry for the TOR switch, providing the TOR’s IP address and VTEP address. The router name should match the hostname of the TOR. Also configure the TSN and TOR agent addresses for the TOR.</p>
<pre><span style="font-family: 'courier new', courier;"><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/edit-physical-qfx.png"><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6375" src="http://www.opencontrail.org/wp-content/uploads/2015/07/edit-physical-qfx.png" alt="edit-physical-qfx" width="698" height="516" data-id="6375" /></a>
</span></pre>
<p>Go to <em>Configure &gt; Physical Devices &gt; Interfaces</em> and add physical and logical interface to be configured on the TOR. The name of the logical interface must match the name on the TOR (xe-0/0/40 and xe-0/0/40.1000). Also enter other logical interface configurations, such as VLAN ID, MAC address, and IP address of the bare metal server and the virtual network to which it belongs.</p>
<p>We made several tests and this configuration shows connection of bare metal server with VLAN tagged with ID 1000.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/logical-port-qfx.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6376" src="http://www.opencontrail.org/wp-content/uploads/2015/07/logical-port-qfx.png" alt="logical-port-qfx" width="980" height="300" data-id="6376" /></a></p>
<p>The following output shows configuration changes done by Contrail on interfaces and VLAN section.</p>
<pre><span style="font-family: 'courier new', courier;">softtronik@QFX5100_VC# show interfaces xe-0/0/40 
flexible-vlan-tagging; 
encapsulation extended-vlan-bridge; 
unit 1000 { 
           vlan-id 1000; 
} 

softtronik@QFX5100_VC# show vlans 
Contrail-c68a622b-9248-4535-bf04-4859012d7a2a { 
           interface xe-0/0/40.1000; 
           vxlan { 
                  vni 10; 
           }} </span></pre>
<p>To list interfaces managed via ovsdb, use <em>show ovsdb interface</em> command.</p>
<pre><span style="font-family: 'courier new', courier;">softtronik@QFX5100_VC&gt; show ovsdb interface 
Interface           VLAN ID           Bridge-domain 
xe-0/0/40           1000              Contrail-c68a622b-9248-4535-bf04-4859012d7a2a</span></pre>
<p>List all learned MAC addresses and connection with particular VTEP with <em>show ovsdb mac</em>. Grep only remote addresses by adding keyword remote at the end of the command.</p>
<pre><span style="font-family: 'courier new', courier;">softtronik@QFX5100_VC&gt; show ovsdb mac 
Logical Switch Name: Contrail-c68a622b-9248-4535-bf04-4859012d7a2a 
Mac                                    IP                                 Encapsulation                                          Vtep 
Address                                Address                                                                                   Address 
ff:ff:ff:ff:ff:ff                      0.0.0.0                            Vxlan over Ipv4                                        10.10.80.6 
10:0e:7e:bf:9e:ec                      0.0.0.0                            Vxlan over Ipv4                                        10.10.80.6 
02:30:84:c3:d1:13                      0.0.0.0                            Vxlan over Ipv4                                        10.100.10.2 
02:e1:bb:af:65:11                      0.0.0.0                            Vxlan over Ipv4                                        10.100.10.4 
02:fc:94:91:42:f2                      0.0.0.0                            Vxlan over Ipv4                                        10.100.10.5 
1a:7f:6d:fb:0e:3d                      0.0.0.0                            Vxlan over Ipv4                                        10.100.10.7 
40:a6:77:9a:b3:38                      0.0.0.0                            Vxlan over Ipv4                                        10.10.80.4 
ff:ff:ff:ff:ff:ff                      0.0.0.0                            Vxlan over Ipv4                                        10.100.10.6 

softtronik@QFX5100_VC&gt; show ovsdb virtual-tunnel-end-point 

Encapsulation                   Ip Address                               Num of MAC's 
VXLAN over IPv4                 10.10.80.4                               1
VXLAN over IPv4                 10.10.80.6                               2 
VXLAN over IPv4                 10.100.10.2                              1 
VXLAN over IPv4                 10.100.10.4                              1 
VXLAN over IPv4                 10.100.10.5                              1 
VXLAN over IPv4                 10.100.10.6                              1 
VXLAN over IPv4                 10.100.10.7                              1</span></pre>
<p>With this command we can see all Vtep addresses present in out network.</p>
<pre><span style="font-family: 'courier new', courier;">softtronik@QFX5100_VC&gt; show vlans 
Routing instance        VLAN name                                               Tag              Interfaces 
default-switch          Contrail-c68a622b-9248-4535-bf04-4859012d7a2a           NA               vtep.32769* 
                                                                                                 vtep.32770* 
                                                                                                 vtep.32771* 
                                                                                                 vtep.32772* 
                                                                                                 vtep.32773* 
                                                                                                 vtep.32774* 
                                                                                                 xe-0/0/40.1000*</span></pre>
<p>Vtep interfaces can be also listed with <em>show interfaces terse vtep</em> command.</p>
<pre><span style="font-family: 'courier new', courier;">softtronik@QFX5100_VC&gt; show interfaces terse vtep 
Interface                Admin            Link            Proto            Local           Remote 
vtep                     up               up 
vtep.32768               up               up 
vtep.32769               up               up              eth-switch   
vtep.32770               up               up              eth-switch 
vtep.32771               up               up              eth-switch 
vtep.32772               up               up              eth-switch 
vtep.32773               up               up              eth-switch 
vtep.32774               up               up              eth-switch</span></pre>
<p>To see detailed information, use the previous command with particular interface.</p>
<pre><span style="font-family: 'courier new', courier;">softtronik@QFX5100_VC&gt; show interfaces vtep.32769 
     Logical interface vtep.32769 (Index 576)(SNMP ifIndex 544) 
           Flags: Up SNMP-Traps Encapsulation: ENET2 
           VXLAN Endpoint Type: Remote, VXLAN Endpoint Address: 10.100.10.2, L2 Routing Instance: default-switch, L3 Routing Instance: default 
           Input packets : 0 
           Output packets: 8 
           Protocol eth-switch, MTU: 1600 
              Flags: Trunk-Mode

</span></pre>
<div class="section">
<div id="redundant-connection-of-bare-metal-servers" class="section">
<h3>Redundant Connection of Bare Metal Servers</h3>
<p>We wanted to use MC-LAG (QFX in virtual chassis) to enable run LACP on both ports, but MC-LAG is not currently supported with VxLAN (but it’s on the roadmap). Therefore only viable option is to connect both port into same Virtual Network (VNI) and configure active-passive bonding on the bare metal server.</p>
</div>
</div>
<h2>CONTRAIL BAREMETAL TOR IMPLEMENTATION WITH OPENVSWITCH VTEP</h2>
<p>We tested and verified Juniper QFX5100 works well as TOR switch in the previous section, which was no surprise to us because they are from the same vendor. At this part we want to show that Contrail in not a vendor locked-in solution by using standard network protocols. There are several switch vendors (Cumulus, Arista) who support OVSDB capability in their boxes. We have decided to proof this openness on OpenVSwitch.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/OVS.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6377" src="http://www.opencontrail.org/wp-content/uploads/2015/07/OVS.png" alt="OVS" width="528" height="450" data-id="6377" /></a></p>
<p>We deployed another two physical server OVS, BSM02 and ToR agent TNS1-02. OVS server with openvswitch represents same role as Juniper QFX. We tested 3 use cases:</p>
<ul class="simple">
<li>simulate netns <em>ns1</em> namespaces as BMS endpoint</li>
<li>install KVM on OVS and launch VM5 as BMS endpoint</li>
<li>use physical NIC eth3 and connect BMS02 physical bare metal server</li>
</ul>
<p class="section">Scenario for testing:</p>
<p>1. Create TOR agent &#8211; deploy TOR agent for managing OVS<br />
2. Setup OVS &#8211; install openvswitch-vtep, configure physical switch and connect namespace.<br />
3. Connect KVM VM to cloud &#8211; install kvm, launch VM5 with OVS interface and connect through a new logical port.<br />
4. Connect BMS to cloud &#8211; add OVS physical interface eth2 and verify connectivity from BMS02.</p>
<h3>Create TOR agent</h3>
<p class="section">In previous chapter we used tns1-01 TOR agent for QFX5100. If we want to manage another switch via OVSDB, next TOR agent service has to be started. It can be done on the same TNS node. Provisioning can be done through Fabric, but we show how to do that manually. Start witch copying config file of tns1-01 agent.</p>
<pre><span style="font-family: 'courier new', courier;">root@tns01:~# cp /etc/contrail/contrail-tor-agent-1.conf /etc/contrail/contrail-tor-agent-2.conf</span></pre>
<p>Then we had to change some values in this copied file.</p>
<pre><span style="font-family: 'courier new', courier;">[DEFAULT]agent_name=tns01-2 
log_file=/var/log/contrail/contrail-tor-agent-2.log 
http_server_port=8086 
[TOR]tor_ip=10.100.10.7</span></pre>
<p>We have copy of supervisor file to start new service also.</p>
<pre><span style="font-family: 'courier new', courier;">root@tns01:~# cp /etc/contrail/supervisord_vrouter_files/contrail-tor-agent-1.ini /etc/contrail/supervisord_vrouter_files/contrail-tor-agent-2.ini</span></pre>
<p>And change these configuration values.</p>
<pre><span style="font-family: 'courier new', courier;">command=/usr/bin/contrail-tor-agent --config_file /etc/contrail/contrail-tor-agent-2.conf 
stdout_logfile=/var/log/contrail/contrail-tor-agent-2-stdout.log</span></pre>
<p>Now restart supervisor to see changes.</p>
<pre><span style="font-family: 'courier new', courier;">root@tns01:~# service supervisor-vrouter restart</span></pre>
<p>And verify changes:</p>
<pre><span style="font-family: 'courier new', courier;">root@tns01:~# contrail-status 
== Contrail vRouter== 
supervisor-vrouter:            active 
contrail-tor-agent-1           active 
contrail-tor-agent-2           active 
contrail-vrouter-agent         active 
contrail-vrouter-nodemgr       active 

</span></pre>
<h3>Setup OVS</h3>
<p class="section">We need to install at least openvswitch-2.3.1, because it has ovs-vtep with VTEP simulator <a id="id4" class="reference internal" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#vtep">[vtep]</a>. However Ubuntu 14.04.2 contains 2.0.2. Therefore you have to build your own packages or use source tarball. We found packages at PPA <a class="reference external" href="https://launchpad.net/~vshn/+archive/ubuntu/openvswitch">https://launchpad.net/~vshn/+archive/ubuntu/openvswitch</a>.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# cat /etc/apt/sources.list.d/ovs.list 
deb http://ppa.launchpad.net/vshn/openvswitch/ubuntu trusty main 
deb-src http://ppa.launchpad.net/vshn/openvswitch/ubuntu trusty main 

root@ovs:~# apt-get install openvswitch-vtep</span></pre>
<p>We have to delete default existing database created during installation.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#rm /etc/openvswitch/*.db</span></pre>
<p>And create two new databases: ovs.db and vtep.db</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#ovsdb-tool create /etc/openvswitch/ovs.db /usr/share/openvswitch/vswitch.ovsschema ; ovsdb-tool create /etc/openvswitch/vtep.db /usr/share/openvswitch/vtep.ovsschema</span></pre>
<p>Restart services and make sure, that the ptcp port number matches the port number in contrail-tor-agent-2.conf on TNS node.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#service openvswitch-switch stop 
root@ovs:~#ovsdb-server --pidfile --detach --log-file --remote punix:/var/run/openvswitch/db.sock --remote=db:hardware_vtep,Global,managers --remote ptcp:6632 /etc/openvswitch/ovs.db /etc/openvswitch/vtep.db root@ovs:~#ovs-vswitchd --log-file --detach --pidfile unix:/var/run/openvswitch/db.sock</span></pre>
<p>Verify creation of databases with:</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#ovsdb-client list-dbs unix:/var/run/openvswitch/db.sock 
Open_vSwitch 
hardware_vtep</span></pre>
<p>First we need to test our installation with connecting namespace to virtual network. Start with creating bridge.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#ovs-vsctl add-br TOR1 
root@ovs:~#vtep-ctl add-ps TOR1</span></pre>
<p>Setup VTEP of bridge. IP addresses are underlay addresses of our node with OVS.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#vtep-ctl set Physical_Switch TOR1 tunnel_ips=10.100.10.7 
root@ovs:~#vtep-ctl set Physical_Switch TOR1 management_ips=10.100.10.7 
root@ovs:~#python /usr/share/openvswitch/scripts/ovs-vtep --log-file=/var/log/openvswitch/ovs-vtep.log --pidfile=/var/run/openvswitch/ovs-vtep.pid --detach TOR1</span></pre>
<p>Now create namespace and link its interface with OVS interface.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#ip netns add ns1 
root@ovs:~#ip link add nstap1 type veth peer name tortap1 
root@ovs:~#ovs-vsctl add-port TOR1 tortap1 
root@ovs:~#ip link set nstap1 netns ns1 
root@ovs:~#ip netns exec ns1 ip link set dev nstap1 up 
root@ovs:~#ip link set dev tortap1 up</span></pre>
<p>And configure namespace to be able to communicate with world.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#ip netns
root@ovs:~#ip netns exec ns1 ip a a 127.0.0.1/8 dev lo
root@ovs:~#ip netns exec ns1 ip a
root@ovs:~#ip netns exec ns1 ip a a 10.0.10.120/24 dev nstap1
root@ovs:~#ip netns exec ns1 ping 10.0.10.120
root@ovs:~#ip netns exec ns1 ip link set up dev lo
root@ovs:~#ip netns exec ns1 ping 10.0.10.120</span></pre>
<p>You can verify previous steps with looking into database.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# vtep-ctl list Physical_Switch
_uuid               : f00f2242-409e-43fc-8d4f-32e2225937d8
description         : "OVS VTEP Emulator"
management_ips      : ["10.100.10.7"]
name                : "TOR1"
ports               : [82afe753-25f8-4127-839b-2c5c8f7948b2]
switch_fault_status : []
tunnel_ips          : ["10.100.10.7"]
tunnels             : []</span></pre>
<p>Now we have to add TOR1 as a new physical device in Contrail managed by TOR agent tns01-2.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6380" src="http://www.opencontrail.org/wp-content/uploads/2015/07/edit-physical-router.png" alt="edit-physical-router" width="697" height="514" data-id="6380" /></p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6381" src="http://www.opencontrail.org/wp-content/uploads/2015/07/physical-routers.png" alt="physical-routers" width="806" height="187" data-id="6381" /></p>
<p>Then create physical port tortap1 with logical tortap1.0 interface, which goes to our ns1 namespace.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/interfaces.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6382" src="http://www.opencontrail.org/wp-content/uploads/2015/07/interfaces.png" alt="interfaces" width="972" height="300" data-id="6382" /></a></p>
<p>&nbsp;</p>
<p>We can verify Contrail configuration by following output.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# vtep-ctl list-ls
Contrail-c68a622b-9248-4535-bf04-4859012d7a2a</span></pre>
<p>Check namespace IP addresses.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ip netns exec ns1 ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
7: nstap1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 1a:7f:6d:fb:0e:3d brd ff:ff:ff:ff:ff:ff
    inet 10.0.10.120/24 scope global nstap1
       valid_lft forever preferred_lft forever
    inet6 fe80::187f:6dff:fefb:e3d/64 scope link
       valid_lft forever preferred_lft forever</span></pre>
<p>Try to ping VM4 from ns1.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ip netns exec ns1 ping 10.0.10.3
PING 10.0.10.3 (10.0.10.3) 56(84) bytes of data.
64 bytes from 10.0.10.3: icmp_seq=1 ttl=64 time=1.25 ms
64 bytes from 10.0.10.3: icmp_seq=2 ttl=64 time=0.311 ms
64 bytes from 10.0.10.3: icmp_seq=3 ttl=64 time=0.307 ms
64 bytes from 10.0.10.3: icmp_seq=4 ttl=64 time=0.270 ms
^C
--- 10.0.10.3 ping statistics ---
4 packets transmitted, 4 received, 0% packet loss, time 2999ms
rtt min/avg/max/mdev = 0.270/0.536/1.256/0.416 ms</span></pre>
<p>Following output shows all interface on physical server OVS.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:95:60:e8 brd ff:ff:ff:ff:ff:ff
    inet 10.10.70.135/24 brd 10.10.70.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::250:56ff:fe95:60e8/64 scope link
       valid_lft forever preferred_lft forever
3: eth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:95:6b:14 brd ff:ff:ff:ff:ff:ff
    inet 10.100.10.7/24 brd 10.100.10.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::250:56ff:fe95:6b14/64 scope link
       valid_lft forever preferred_lft forever
4: ovs-system: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether 4a:4f:14:53:c6:df brd ff:ff:ff:ff:ff:ff
5: TOR1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether 2a:11:8c:74:61:46 brd ff:ff:ff:ff:ff:ff
6: tortap1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master ovs-system state UP group default qlen 1000
    link/ether 16:62:97:5a:62:e8 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::1462:97ff:fe5a:62e8/64 scope link
       valid_lft forever preferred_lft forever
8: vtep_ls1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default
    link/ether 62:2b:86:c3:c2:4b brd ff:ff:ff:ff:ff:ff </span></pre>
<p>Following output shows openvswitch configuration. Patch ports 0000-tortap1-p and 0000-tortap1-lwere created by Contrail.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ovs-vsctl show
93d90385-f9c6-4cfc-b67d-4f64eec15479
    Bridge "TOR1"
        Port "TOR1"
            Interface "TOR1"type: internal
        Port "tortap1"
            Interface "tortap1"
        Port "0000-tortap1-p"
            Interface "0000-tortap1-p"type: patch
                options: {peer="0000-tortap1-l"}
    Bridge "vtep_ls1"
        Port "vx4"
            Interface "vx4"type: vxlan
                options: {key="10", remote_ip="10.100.10.2"}
        Port "vx2"
            Interface "vx2"type: vxlan
                options: {key="10", remote_ip="10.100.10.5"}
        Port "vx5"
            Interface "vx5"type: vxlan
                options: {key="10", remote_ip="10.100.10.6"}
        Port "vx3"
            Interface "vx3"type: vxlan
                options: {key="10", remote_ip="10.100.10.4"}
        Port "vx9"
            Interface "vx9"type: vxlan
                options: {key="10", remote_ip="10.10.80.6"}
        Port "vtep_ls1"
            Interface "vtep_ls1"type: internal
        Port "0000-tortap1-l"
            Interface "0000-tortap1-l"type: patch
                options: {peer="0000-tortap1-p"}</span></pre>
<h4>Connect KVM VM to the Cloud</h4>
<p>Now we want to try to boot VM5 on OVS server and connect it into our virtual network as a bare metal server. At first we need to install qemu and boot a virtual machine.</p>
<pre><span style="font-family: 'courier new', courier;">sudo apt-get install qemu-system-x86 ubuntu-vm-builder uml-utilities
sudo ubuntu-vm-builder kvm precise</span></pre>
<p>Once that is done, create a VM, if necessary, and edit its Domain XML file:</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# virsh list
 Id    Name                           State
----------------------------------------------------
 7     ubuntu                         running

% virsh destroy ubuntu
% virsh edit ubuntu</span></pre>
<p>Look at the Domain XML file the section. There should be one XML section for each interface the VM</p>
<div id="stcpDiv">
<p class="section">has.</p>
<div class="highlight-bash">
<div class="highlight">
<pre>&lt;interface <span class="nb">type</span><span class="o">=</span><span class="s1">'network'</span>&gt;
 &lt;mac <span class="nv">address</span><span class="o">=</span><span class="s1">'52:54:00:3a:b6:13'</span>/&gt;
 &lt;<span class="nb">source </span><span class="nv">network</span><span class="o">=</span><span class="s1">'default'</span>/&gt;
 &lt;address <span class="nb">type</span><span class="o">=</span><span class="s1">'pci'</span><span class="nv">domain</span><span class="o">=</span><span class="s1">'0x0000'</span><span class="nv">bus</span><span class="o">=</span><span class="s1">'0x00'</span><span class="nv">slot</span><span class="o">=</span><span class="s1">'0x03'</span><span class="k">function</span><span class="o">=</span><span class="s1">'0x0'</span>/&gt;
&lt;/interface&gt;
</pre>
</div>
</div>
<p class="section">And change it to something like this:</p>
<div id="stcpDiv">
<div class="highlight-bash">
<div class="highlight">
<pre>&lt;interface <span class="nb">type</span><span class="o">=</span><span class="s1">'bridge'</span>&gt;
 &lt;mac <span class="nv">address</span><span class="o">=</span><span class="s1">'52:54:00:3a:b6:13'</span>/&gt;
 &lt;<span class="nb">source </span><span class="nv">bridge</span><span class="o">=</span><span class="s1">'TOR1'</span>/&gt;
 &lt;virtualport <span class="nb">type</span><span class="o">=</span><span class="s1">'openvswitch'</span>/&gt;
 &lt;address <span class="nb">type</span><span class="o">=</span><span class="s1">'pci'</span><span class="nv">domain</span><span class="o">=</span><span class="s1">'0x0000'</span><span class="nv">bus</span><span class="o">=</span><span class="s1">'0x00'</span><span class="nv">slot</span><span class="o">=</span><span class="s1">'0x03'</span><span class="k">function</span><span class="o">=</span><span class="s1">'0x0'</span>/&gt;
&lt;/interface&gt;
</pre>
</div>
</div>
<p>Start VM5 and verify that it uses openvswitch interface. There is automatically created interface</p>
<div id="stcpDiv">
<p class="section">vnet0.</p>
<div class="highlight-bash">
<div class="highlight">
<pre>    % virsh start ubuntu

&lt;interface <span class="nb">type</span><span class="o">=</span><span class="s1">'bridge'</span>&gt;
  &lt;mac <span class="nv">address</span><span class="o">=</span><span class="s1">'52:54:00:3a:b6:13'</span>/&gt;
  &lt;<span class="nb">source </span><span class="nv">bridge</span><span class="o">=</span><span class="s1">'TOR1'</span>/&gt;
  &lt;virtualport <span class="nb">type</span><span class="o">=</span><span class="s1">'openvswitch'</span>&gt;
    &lt;parameters <span class="nv">interfaceid</span><span class="o">=</span><span class="s1">'5def61f9-7123-43a5-b7ae-35f0fbd22fca'</span>/&gt;
  &lt;/virtualport&gt;
  &lt;target <span class="nv">dev</span><span class="o">=</span><span class="s1">'vnet0'</span>/&gt;
  &lt;model <span class="nb">type</span><span class="o">=</span><span class="s1">'virtio'</span>/&gt;
  &lt;<span class="nb">alias </span><span class="nv">name</span><span class="o">=</span><span class="s1">'net0'</span>/&gt;
  &lt;address <span class="nb">type</span><span class="o">=</span><span class="s1">'pci'</span><span class="nv">domain</span><span class="o">=</span><span class="s1">'0x0000'</span><span class="nv">bus</span><span class="o">=</span><span class="s1">'0x00'</span><span class="nv">slot</span><span class="o">=</span><span class="s1">'0x03'</span><span class="k">function</span><span class="o">=</span><span class="s1">'0x0'</span>/&gt;
&lt;/interface&gt;
</pre>
</div>
</div>
<p class="section">Now we can add a new port vnet0 as physical and logical port in Contrail.</p>
</div>
</div>
</div>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6383" src="http://www.opencontrail.org/wp-content/uploads/2015/07/add-vnet0.png" alt="add-vnet0" width="700" height="433" data-id="6383" /></p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/logical-ports-vnet0.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6384" src="http://www.opencontrail.org/wp-content/uploads/2015/07/logical-ports-vnet0.png" alt="logical-ports-vnet0" width="833" height="300" data-id="6384" /></a></p>
<p>We can check new patch interfaces in openvswitch.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ovs-vsctl show
93d90385-f9c6-4cfc-b67d-4f64eec15479
    Bridge "TOR1"
        Port "vnet0"
            Interface "vnet0"
        Port "TOR1"
            Interface "TOR1"type: internal
        Port "tortap1"
            Interface "tortap1"
        Port "0000-vnet0-p"
            Interface "0000-vnet0-p"type: patch
                options: {peer="0000-vnet0-l"}
        Port "0000-tortap1-p"
            Interface "0000-tortap1-p"type: patch
                options: {peer="0000-tortap1-l"}
    Bridge "vtep_ls1"
        Port "vx4"
            Interface "vx4"type: vxlan
                options: {key="10", remote_ip="10.100.10.2"}
        Port "0000-vnet0-l"
            Interface "0000-vnet0-l"type: patch
                options: {peer="0000-vnet0-p"}
        Port "vx2"
            Interface "vx2"type: vxlan
                options: {key="10", remote_ip="10.100.10.5"}
        Port "vx5"
            Interface "vx5"type: vxlan
                options: {key="10", remote_ip="10.100.10.6"}
        Port "vx3"
            Interface "vx3"type: vxlan
                options: {key="10", remote_ip="10.100.10.4"}
        Port "vtep_ls1"
            Interface "vtep_ls1"type: internal
        Port "0000-tortap1-l"
            Interface "0000-tortap1-l"type: patch
                options: {peer="0000-tortap1-p"}</span></pre>
<p>We can open console at VM5, manually set IP address and try to ping VM4.</p>
<p><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-6385" src="http://www.opencontrail.org/wp-content/uploads/2015/07/ubuntu-vm-ping.png" alt="ubuntu-vm-ping" width="796" height="545" data-id="6385" /></p>
<p>We can check the same thing from baremetal namespace.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ip netns exec ns1 ping 10.0.10.121
PING 10.0.10.121 (10.0.10.121) 56(84) bytes of data.
64 bytes from 10.0.10.121: icmp_seq=1 ttl=64 time=0.709 ms
64 bytes from 10.0.10.121: icmp_seq=2 ttl=64 time=0.432 ms
64 bytes from 10.0.10.121: icmp_seq=3 ttl=64 time=0.302 ms
^C
--- 10.0.10.121 ping statistics ---
3 packets transmitted, 3 received, 0% packet loss, time 1999ms
rtt min/avg/max/mdev = 0.302/0.481/0.709/0.169 ms</span></pre>
<p>The following screen shows L2 routes at vRouter with VM4, where you can see all details about VxLAN tunnel.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/l2-vxlan-tunnel.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6386" src="http://www.opencontrail.org/wp-content/uploads/2015/07/l2-vxlan-tunnel.png" alt="l2-vxlan-tunnel" width="692" height="350" data-id="6386" /></a></p>
<h3>Connect BMS to cloud</h3>
<p class="section">Last test use case is to connect another baremetal server BMS02 through the physical NIC of OVS server.In this case OVS server represents a true switch.Add a physical interface eth3to you server with OVS.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~#ovs-vsctl add-port TOR1 eth3

root@ovs:~# ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:95:60:e8 brd ff:ff:ff:ff:ff:ff
    inet 10.10.70.135/24 brd 10.10.70.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::250:56ff:fe95:60e8/64 scope link
       valid_lft forever preferred_lft forever
...
24: eth3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq master ovs-system state UP group default qlen 1000
    link/ether 00:50:56:95:e0:21 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::250:56ff:fe95:e021/64 scope link
       valid_lft forever preferred_lft forever</span></pre>
<p>Create physical and logical ports for eth3.</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2015/07/logical-ports-eth3.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-6387" src="http://www.opencontrail.org/wp-content/uploads/2015/07/logical-ports-eth3.png" alt="logical-ports-eth3" width="722" height="300" data-id="6387" /></a></p>
<p>Check new ports in openvswitch.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ovs-vsctl show
93d90385-f9c6-4cfc-b67d-4f64eec15479
    Bridge "TOR1"
        ...
        Port "TOR1"
            Interface "TOR1"type: internal
        Port "0000-eth3-p"
            Interface "0000-eth3-p"type: patch
                options: {peer="0000-eth3-l"}
        Port "eth3"
            Interface "eth3"
        ...
    Bridge "vtep_ls1"
        Port "vx17"
            Interface "vx17"type: vxlan
                options: {key="10", remote_ip="10.10.80.4"}
        Port "vx4"
            Interface "vx4"type: vxlan
                options: {key="10", remote_ip="10.100.10.2"}
        Port "vx2"
            Interface "vx2"type: vxlan
                options: {key="10", remote_ip="10.100.10.5"}
        Port "0000-vnet0-l"
            Interface "0000-vnet0-l"type: patch
                options: {peer="0000-vnet0-p"}
        Port "0000-tortap1-l"
            Interface "0000-tortap1-l"type: patch
                options: {peer="0000-tortap1-p"}
        Port "vx5"
            Interface "vx5"type: vxlan
                options: {key="10", remote_ip="10.100.10.6"}
        Port "vx3"
            Interface "vx3"type: vxlan
                options: {key="10", remote_ip="10.100.10.4"}
        Port "vtep_ls1"
            Interface "vtep_ls1"type: internal
        Port "0000-eth3-l"
            Interface "0000-eth3-l"type: patch
                options: {peer="0000-eth3-p"}</span></pre>
<p>As you can see it is possible to ping cloud instance from our bare metal server.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs2:~# ping 10.0.10.3
PING 10.0.10.3 (10.0.10.3) 56(84) bytes of data.
64 bytes from 10.0.10.3: icmp_seq=2 ttl=64 time=1.10 ms
64 bytes from 10.0.10.3: icmp_seq=3 ttl=64 time=0.387 ms
64 bytes from 10.0.10.3: icmp_seq=4 ttl=64 time=0.428 ms
64 bytes from 10.0.10.3: icmp_seq=5 ttl=64 time=0.378 ms
64 bytes from 10.0.10.3: icmp_seq=6 ttl=64 time=0.419 ms
64 bytes from 10.0.10.3: icmp_seq=7 ttl=64 time=0.382 ms
^C
--- 10.0.10.3 ping statistics ---
7 packets transmitted, 6 received, 14% packet loss, time 6005ms
rtt min/avg/max/mdev = 0.378/0.516/1.102/0.262 ms</span></pre>
<h3>Verification</h3>
<p>OVS maintains network information in database. To list existing tables use:</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# ovsdb-client list-tables unix:/var/run/openvswitch/db.sock hardware_vtep
Table
---------------------
Physical_Port
Physical_Locator_Set
Physical_Locator
Logical_Binding_Stats
Arp_Sources_Remote
Manager
Mcast_Macs_Local
Global
Ucast_Macs_Local
Logical_Switch
Physical_Switch
Ucast_Macs_Remote
Tunnel
Mcast_Macs_Remote
Logical_Router
Arp_Sources_Local</span></pre>
<p>To view the content of these tables in readable format use vtep-ctl listcommand with table’s name at the end.The list of physical interfaces associated with ovs.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# vtep-ctl list Physical_Port
        _uuid               : ac4a8bb8-bd11-47d3-a5ac-9828c5f68ffc
        description         : ""
        name                : "eth3"
        port_fault_status   : []
        vlan_bindings       : {0=4f016591-56ce-496f-996e-a93203061e07}
        vlan_stats          : {0=288fe0f7-d7b8-430a-beb4-0c0a2c536a9c}

        _uuid               : 41f87dae-6568-4fc9-97fc-46ec3d2fbfdd
        description         : ""
        name                : "vnet0"
        port_fault_status   : []
        vlan_bindings       : {0=4f016591-56ce-496f-996e-a93203061e07}
        vlan_stats          : {0=12d70df8-6448-4784-af0d-754f37847942}

        _uuid               : 82afe753-25f8-4127-839b-2c5c8f7948b2
        description         : ""
        name                : "tortap1"
        port_fault_status   : []
        vlan_bindings       : {0=4f016591-56ce-496f-996e-a93203061e07}
        vlan_stats          : {0=e68cc29d-72c3-4809-a013-32c91a119b11}</span></pre>
<p>To see remote MAC addresses and their next hop VTEPs we have to first find out name of our logical switch.</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# vtep-ctl list-ls
Contrail-c68a622b-9248-4535-bf04-4859012d7a2a</span></pre>
<p>Then list remote macs:</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# vtep-ctl list-remote-macs Contrail-c68a622b-9248-4535-bf04-4859012d7a2a
ucast-mac-remote
  02:30:84:c3:d1:13 -&gt; vxlan_over_ipv4/10.100.10.2
  02:e1:bb:af:65:11 -&gt; vxlan_over_ipv4/10.100.10.4
  02:fc:94:91:42:f2 -&gt; vxlan_over_ipv4/10.100.10.5
  40:a6:77:9a:b3:38 -&gt; vxlan_over_ipv4/10.10.80.4

mcast-mac-remote
  unknown-dst -&gt; vxlan_over_ipv4/10.100.10.6</span></pre>
<p>As we can see, unknown traffic is handled by TOR agent.To list local MACs type:</p>
<pre><span style="font-family: 'courier new', courier;">root@ovs:~# vtep-ctl list-local-macs Contrail-c68a622b-9248-4535-bf04-4859012d7a2a
ucast-mac-local
  1a:7f:6d:fb:0e:3d -&gt; vxlan_over_ipv4/10.100.10.7

mcast-mac-local
  unknown-dst -&gt; vxlan_over_ipv4/10.100.10.7

</span></pre>
<div id="conclusion" class="section">
<h2>CONCLUSION</h2>
<p>We tested almost all scenarios for bare-metal connection to overlay networks on different devices. We proved that OpenContrail is working open source, multi-vendor SDN solution, which moves OpenStack cloud to the next level suitable for large enterprises.</p>
<p>In future parts of this blog we would like to look at High Availability setup of TOR agent, which has been added in Contrail 2.2. Our next post will focus on route gateways with functions like VxLAN to EVPN Stitching for L2 Extension, L3VPN, multi-vendor support and gateway redundancy.</p>
<div class="line-block">
<p class="line"><strong>Marek Celoud &amp; Jakub Pavlik</strong><br />
tcp cloud engineers</p>
</div>
<div class="line-block">
<p class="line"><strong>Rostislav Safar</strong><br />
Arrow ECS network engineer</p>
</div>
</div>
<div id="resources" class="section">
<h2>RESOURCES</h2>
<table id="contrailtor" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label">[ContrailToR]</td>
<td><em>(<a class="fn-backref" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#id1">1</a>, <a class="fn-backref" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#id2">2</a>)</em> Using TOR Switches with OVSDB for Virtual Instance Support <a href="http://www.juniper.net/techpubs/en_US/contrail2.2/topics/concept/using-tor-ovsdb-contrail.html" target="_blank">http://www.juniper.net/techpubs/en_US/contrail2.2/topics/concept/using-tor-ovsdb-contrail.html</a></td>
</tr>
</tbody>
</table>
<table id="torha" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label">[TorHA]</td>
<td>High Availability for Contrail TOR Agent <a href="http://www.juniper.net/techpubs/en_US/contrail2.2/topics/concept/ha-tor-agnt.html" target="_blank">http://www.juniper.net/techpubs/en_US/contrail2.2/topics/concept/ha-tor-agnt.html</a></td>
</tr>
</tbody>
</table>
<table id="site" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label"><a class="fn-backref" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#id3">[site]</a></td>
<td>Juniper Contrail documentation <a href="http://www.juniper.net/techpubs/en_US/contrail2.2/topics/task/installation/install-overview-vnc.html" target="_blank">http://www.juniper.net/techpubs/en_US/contrail2.2/topics/task/installation/install-overview-vnc.html</a></td>
</tr>
</tbody>
</table>
<table id="vtep" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label"><a class="fn-backref" href="http://tcpcloud.eu/en/blog/2015/07/13/opencontrail-sdn-lab-testing-1-tor-switches-ovsdb/#id4">[vtep]</a></td>
<td>How to Use the VTEP Emulator <a class="reference external" href="https://github.com/openvswitch/ovs/blob/master/vtep/README.ovs-vtep.md">https://github.com/openvswitch/ovs/blob/master/vtep/README.ovs-vtep.md</a></td>
</tr>
</tbody>
</table>
<table id="ovscontrail" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label">[ovscontrail]</td>
<td>Setting up openvswitch VM for Contrail Baremetal <a class="reference external" href="https://github.com/Juniper/contrail-test/wiki/Setting-up-an-openvswitch-VM-for-Contrail-Baremetal-tests">https://github.com/Juniper/contrail-test/wiki/Setting-up-an-openvswitch-VM-for-Contrail-Baremetal-tests</a></td>
</tr>
</tbody>
</table>
<table id="vxlanovsdb" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label">[vxlanovsdb]</td>
<td>Enhancing VM mobility with VxLAN OVSDB <a class="reference external" href="http://mcleonard.blogspot.cz/2013/12/enhancing-vm-mobility-with-vxlan-ovsdb.html">http://mcleonard.blogspot.cz/2013/12/enhancing-vm-mobility-with-vxlan-ovsdb.html</a></td>
</tr>
</tbody>
</table>
<table id="ovslibvirt" class="docutils citation" frame="void" rules="none">
<colgroup>
<col class="label" />
<col /></colgroup>
<tbody valign="top">
<tr>
<td class="label">[ovslibvirt]</td>
<td>Libvirt configuration with openvswitch <a class="reference external" href="https://github.com/openvswitch/ovs/blob/master/INSTALL.Libvirt.md">https://github.com/openvswitch/ovs/blob/master/INSTALL.Libvirt.md</a></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>How to setup OpenContrail Gateway &#8211; Juniper MX, Cisco ASR and Software GW</title>
		<link>https://tungsten.io/how-to-setup-opencontrail-gateway-juniper-mx-cisco-asr-and-software-gw/</link>
		
		<dc:creator><![CDATA[Sreelakshmi Sarva]]></dc:creator>
		<pubDate>Sun, 25 May 2014 20:09:56 +0000</pubDate>
				<category><![CDATA[DataCenter]]></category>
		<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<guid isPermaLink="false">http://opencontrail.org/?p=1518</guid>

					<description><![CDATA[Note: This blog is done with user’s own lab environment and all third party references/performance characterization needs to be verified with third party vendor in question. OPENCONTRAIL GATEWAYS &#8211; Use Cases...]]></description>
										<content:encoded><![CDATA[<p><em><strong>Note: This blog is done with user’s own lab environment and all third party references/performance characterization needs to be verified with third party vendor in question.</strong></em></p>
<p><span style="text-decoration: underline;"><em><strong>OPENCONTRAIL GATEWAYS &#8211; Use Cases and Setup Guide </strong></em></span></p>
<h5><strong>1 INTRODUCTION</strong></h5>
<p>Gateway in a virtualized network refers to an entity that allows network traffic to move back and forth between the virtual and the physical networks or between virtual networks operating on different set of technologies. In many cases, the virtual network is created using overlay (i.e. tunneling) technologies and, therefore, a gateway needs to understand the protocols of the overlay network traffic in order to allow traffic to pass back and forth through it.<br />
<span id="more-1518"></span></p>
<p>OpenContrail is based on a standard-based control plane protocols and encapsulation mechanisms to operate. As a result of this approach, industry standard routing platforms can be used as gateways to the virtual networks differentiating OpenContrail from some of the available solutions. In this blog, we will see various approaches in setting up and using a Gateway to a OpenContrail Cloud. In particular, we will focus on three different gateways options which will also cover a vendor agnostic solution and a virtualized gateway as an option. Following gateway options will be covered &#8211;</p>
<ol>
<li>Juniper MX</li>
<li>Cisco ASR 903</li>
<li>Software Gateway</li>
</ol>
<h5><strong>2 Use Cases that require a Gateway to a Cloud Environment</strong></h5>
<h6><strong>2.1 Hybrid Cloud Use Case</strong></h6>
<p>In this use case, a gateway is required to have an enterprise private cloud connect to a public cloud environment (like AWS)  VPC gateway</p>
<p><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Hybrid-Cloud-Use-Case-Picture-GW-Blog.png"><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5718" src="http://www.opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Hybrid-Cloud-Use-Case-Picture-GW-Blog.png" alt="OpenContrail-Hybrid-Cloud-Use-Case-Picture-GW-Blog" width="983" height="426" data-id="5718" /></a></p>
<h6><strong>2.2 Data Center Interconnect &#8211; A Distributed Cloud Scenario</strong></h6>
<p>In this use case, multi-site Data Centers are interconnected to create a distributed cloud environment by constructing a L3VPN domain over EBGP across the gateways</p>
<p><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Data-Center-Interconnect-Use-Case-Picture-GW-Blog.png"><img loading="lazy" decoding="async" class="alignnone wp-image-5720" src="http://www.opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Data-Center-Interconnect-Use-Case-Picture-GW-Blog.png" alt="OpenContrail-Data-Center-Interconnect-Use-Case-Picture-GW-Blog" width="1000" height="479" data-id="5720" /></a></p>
<h6><strong>2.3 Cloud Interconnect + NFV Security Service via L3 VPN Gateway</strong></h6>
<p>BGP MPLS VPN capable Data Center Gateway Router device allows for providing connectivity between the Enterprise customer’s virtual network assets residing in the Data Center and the existing physical PIP L3 VPN network using a standard Inter-AS VPN connectivity methodology. The Data Center edge router will act in an Inter-AS VPN ASBR role bridging the ASN used in the Contrail virtual-network overlay topology to the ASN used in the service provider L3 VPN core.</p>
<p><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-BLOG.bmp"><img loading="lazy" decoding="async" class="alignnone size-medium wp-image-1522" src="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-BLOG.bmp" alt="OpenContrail Cloud Interconnect Security NFV Picture BLOG" width="1" height="1" data-id="1522" /></a><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-BLOG.bmp"><img loading="lazy" decoding="async" class="alignnone size-medium wp-image-1522" src="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-BLOG.bmp" alt="OpenContrail Cloud Interconnect Security NFV Picture BLOG" width="1" height="1" data-id="1522" /></a><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-BLOG.bmp"><img loading="lazy" decoding="async" class="alignnone size-medium wp-image-1522" src="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-BLOG.bmp" alt="OpenContrail Cloud Interconnect Security NFV Picture BLOG" width="1" height="1" data-id="1522" /></a><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-for-Blog.png"><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5721" src="http://www.opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Cloud-Interconnect-Security-NFV-Picture-for-Blog.png" alt="OpenContrail-Cloud-Interconnect-Security-NFV-Picture-for-Blog" width="687" height="380" data-id="5721" /></a></p>
<h6><strong>2.4 Software Gateway to a Virtualized Cloud</strong></h6>
<p><a href="http://opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Software-as-a-GW-Picture-BLOG.png"><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5722" src="http://www.opencontrail.org/wp-content/uploads/2014/05/OpenContrail-Software-as-a-GW-Picture-BLOG.png" alt="OpenContrail-Software-as-a-GW-Picture-BLOG" width="476" height="360" data-id="5722" /></a></p>
<h5><strong>3 What does a Gateway need?            </strong></h5>
<p>A L3 gateway to OpenContrail virtual cloud environment requires standard feature to be supported for control plane signaling</p>
<ol>
<li>L3VPN</li>
<li>MBGP</li>
</ol>
<p>And the following for Data plane functionality</p>
<p>3. Dynamic GRE tunnels</p>
<h5><strong>4 Juniper MX as a Gateway</strong></h5>
<p>In this section, we will cover Junos configuration elements required to enable MX as a gateway router to a OpenContrail cloud environment.</p>
<ol>
<li>Routing instance for the virtual network’s prefixes to show up</li>
<li>Logical tunnels or Rib groups to leak route between routing instances or inet.0</li>
<li>Dynamic tunnel to enable GRE tunnels to</li>
</ol>
<p>Here are the MX configuration snippets:</p>
<pre><span style="font-family: 'courier new', courier;"><code> sroot&gt; show configuration
 ##Enables Dynamic Tunnels on the chassis
 chassis {
 fpc 0 {
 pic 0 {
 tunnel-services;
 }
 }
 }
 interfaces {
 ## For Route leaking between Contrail VRF for Public access and  Global Routing Table
 lt-0/0/0 {
 unit 0 {
 encapsulation frame-relay;
 dlci 1;
 peer-unit 1;
 family inet;
 }
 unit 1 {
 encapsulation frame-relay;
 dlci 1;
 peer-unit 0;
 family inet;
 }
 }
 routing-options {
 static {
 route 0.0.0.0/0 next-hop 10.84.18.254;
 route 10.84.53.80/28 next-hop lt-0/0/0.0;
 }
 route-distinguisher-id 10.84.18.253;
 autonomous-system 64512;
 ## Dynamic Tunnel config with source and destination networks. For each destination network learnt over BGP, there is a dynamic GRE tunnel automatically established to the Compute node.
 dynamic-tunnels {
 dynamic_overlay_tunnels {
 source-address 10.84.18.253;
 gre;
 destination-networks {
 10.84.18.0/24;
 }
 }
 }
 }
 protocols {
 mpls {
 interface all;
 }
 ## Control path , BGP peering to each control node
 bgp {
 group Contrail_Controller {
 type internal;
 local-address 10.84.18.253;
 keep all;
 family inet-vpn {
 unicast;
 }
 neighbor 10.84.18.12; #Contrail Control node 1
 neighbor 10.84.18.13; #Contrail Control node 2
 }
 }
 }
 routing-instances {
 ## Usually, one VRF per Cluster
 public {
 instance-type vrf;
 interface lt-0/0/0.1;
 vrf-target target:64512:10000;
 routing-options {
 static {
 route 0.0.0.0/0 next-hop lt-0/0/0.1; #Default route
 }
 }
 }
 }
 Some CLI/Operation commands to verify Control and Data path:
 ##Upon  Configuring the BGP peering on Contrail Web UI
 root&gt; show bgp summary
 Groups: 1 Peers: 2 Down peers: 0
 Table          Tot Paths  Act Paths Suppressed    History Damp State    Pending
 bgp.l3vpn.0
 78         69          0          0          0          0
 Peer                     AS      InPkt     OutPkt    OutQ   Flaps Last Up/Dwn State|#Active/Received/Accepted/Damped...
 10.84.18.12           64512      22019      23624       0       1     1w0d12h Establ
 bgp.l3vpn.0: 9/9/9/0
 public.inet.0: 1/1/1/0
 10.84.18.13           64512      22023      23624       0       1     1w0d12h Establ
 bgp.l3vpn.0: 0/9/9/0
 public.inet.0: 0/1/1/0</code><code>
 ## Routes being advertised by 18.13 control node
 root&gt; show route receive-protocol bgp 10.84.18.13
 ..
 public.inet.0: 4 destinations, 6 routes (4 active, 0 holddown, 0 hidden)
 Prefix                  Nexthop              MED     Lclpref    AS path
 10.84.53.93/32          10.84.18.13                  100        ?
 bgp.l3vpn.0: 69 destinations, 78 routes (69 active, 0 holddown, 0 hidden)
 Prefix                  Nexthop              MED     Lclpref    AS path
 10.84.18.13:1:0.0.0.0/0
 10.84.18.13                  100        ?
 10.84.18.13:1:1.0.2.253/32
 10.84.18.13                  100        ?
 10.84.18.13:1:10.84.53.93/32
 10.84.18.13                  100        ?
 10.84.18.13:1:192.168.10.252/32
 10.84.18.13                  100        ?
 10.84.18.13:1:192.168.10.253/32
 10.84.18.13                  100        ?
 10.84.18.13:2:10.84.53.93/32
 10.84.18.13                  100        ?
 10.84.18.13:3:250.250.1.253/32
 10.84.18.13                  100        ?
 10.84.18.14:1:192.168.20.253/32
 10.84.18.14                  100        ?
 10.84.18.14:2:250.250.2.253/32
 10.84.18.14                  100        ?
 ## To reach 10.84.53.93 VM, dynamic GRE tunnel path from Gateway to the compute node hosting the VM
 root&gt; show route 10.84.53.93/32
 public.inet.0: 4 destinations, 6 routes (4 active, 0 holddown, 0 hidden)
 + = Active Route, - = Last Active, * = Both
 10.84.53.93/32     *[BGP/170] 20:40:33, localpref 100, from 10.84.18.12
 AS path: ?, validation-state: unverified
 &gt; via gr-0/0/0.32772, Push 22
 [BGP/170] 20:40:33, localpref 100, from 10.84.18.13
 AS path: ?, validation-state: unverified
 &gt; via gr-0/0/0.32772, Push 22</code></span></pre>
<h5><strong>5 ASR 903 as a Gateway</strong></h5>
<p>Configuration below covers leveraging Cisco ASR1k as a Gateway</p>
<pre><code><span style="font-family: 'courier new', courier;"> 
asr903#show running-config 
Building configuration...

Current configuration : 4347 bytes
!
! Last configuration change at 14:20:21 UTC Tue Aug 25 2015
!
version 15.2
no service pad
service timestamps debug datetime msec
service timestamps log datetime msec
no platform punt-keepalive disable-kernel-core
!
hostname asr903
!
boot-start-marker
boot system bootflash:Image/packages.conf
boot-end-marker
!
!
vrf definition Contrail
 rd 64512:10000
 route-target export 64512:10000
 route-target import 64512:10000
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
!
vrf definition Mgmt-intf
 !
 address-family ipv4
 exit-address-family
 !
 address-family ipv6
 exit-address-family
!
enable secret 5 $1$RAyL$SjMKm.r.vzr3sXehjMYNv1
!
no aaa new-model
!
ip vrf mgre
 rd 1:1
!
ip domain name englab.juniper.net
!
!         
!
ipv6 multicast rpf use-bgp
!
!
multilink bundle-name authenticated
!
!
redundancy
 mode sso
!
controller wanphy 0/0/0
!
controller wanphy 0/1/0
!
controller wanphy 0/2/0
!
controller wanphy 0/3/0
!
!
!
ip tftp source-interface GigabitEthernet0
lldp run
!         
!
!
!
!
interface Loopback10
 no ip address
!
interface Loopback30
 vrf forwarding Contrail
 ip address 30.30.40.253 255.255.255.255
!
interface Loopback100
 vrf forwarding Contrail
 ip address 10.250.250.10 255.255.255.255
!
interface Loopback102
 ip address 192.0.2.1 255.255.255.255
!
interface Loopback103
 ip address 192.0.2.2 255.255.255.255
!
interface Tunnel102
 ip address 192.168.0.129 255.255.255.252
 tunnel source Loopback102
 tunnel destination 192.0.2.2
!
interface Tunnel103
 vrf forwarding Contrail
 ip address 192.168.0.130 255.255.255.252
 tunnel source Loopback103
 tunnel destination 192.0.2.1
!
interface TenGigabitEthernet0/0/0
 no ip address
 shutdown
!
interface TenGigabitEthernet0/1/0
 no ip address
 shutdown
!
interface TenGigabitEthernet0/2/0
 no ip address
 shutdown
!
interface TenGigabitEthernet0/3/0
 no ip address
 shutdown
!
interface GigabitEthernet0/4/0
 ip address 10.84.40.190 255.255.255.224
 negotiation auto
!
interface GigabitEthernet0/4/1
 vrf forwarding Contrail
 ip address 30.30.0.3 255.255.255.0
 no ip redirects
 ip local-proxy-arp
 ip route-cache same-interface
 negotiation auto
!
interface GigabitEthernet0/4/2
 no ip address
 shutdown
 negotiation auto
!
interface GigabitEthernet0/4/3
 no ip address
 shutdown
 negotiation auto
!
interface GigabitEthernet0/4/4
 no ip address
 shutdown
 negotiation auto
!
interface GigabitEthernet0/4/5
 no ip address
 shutdown
 negotiation auto
!
interface GigabitEthernet0/4/6
 no ip address
 shutdown
 negotiation auto
!
interface GigabitEthernet0/4/7
 ip address 10.84.40.253 255.255.255.192
 negotiation auto
 cdp enable
!
interface GigabitEthernet0
 vrf forwarding Mgmt-intf
 ip address 10.84.61.201 255.255.254.0
 negotiation auto
!
l3vpn encapsulation ip MGRE
 transport ipv4 source GigabitEthernet0/4/7
 !
router ospf 101 vrf Contrail
 redistribute connected
 redistribute static
 network 192.0.2.130 0.0.0.0 area 0
!
router ospf 100
 redistribute connected
 redistribute static
 network 192.0.2.129 0.0.0.0 area 0
!
router bgp 64512
 bgp router-id 10.84.40.253
 bgp log-neighbor-changes
 neighbor 10.84.30.39 remote-as 64512
 neighbor 10.84.30.39 update-source GigabitEthernet0/4/7
 !
 address-family ipv4
  no neighbor 10.84.30.39 activate
  default-information originate
 exit-address-family
 !
 address-family vpnv4
  neighbor 10.84.30.39 activate
  neighbor 10.84.30.39 send-community extended
  neighbor 10.84.30.39 route-map SELECT_UPDATE_FOR_L3VPN in
 exit-address-family
 !
 address-family ipv4 vrf Contrail
  redistribute connected
  redistribute static
  default-information originate
 exit-address-family
!
no ip forward-protocol nd
!
no ip http server
ip route 0.0.0.0 0.0.0.0 10.84.40.254
ip route 10.84.40.0 255.255.255.192 10.84.40.189
ip route 10.84.40.64 255.255.255.192 10.84.40.189
ip route 10.84.40.128 255.255.255.224 192.168.0.130
ip route vrf Contrail 0.0.0.0 0.0.0.0 192.168.0.129
ip route vrf Contrail 5.5.5.0 255.255.255.0 Null0
!
cdp run
!
route-map setnh-out permit 10
!
route-map SELECT_UPDATE_FOR_L3VPN permit 10
  set ip next-hop encapsulate l3vpn MGRE
!
route-map set-nh permit 10
!
route-map set-nh permit 20
!
route-map set-nh-contrail permit 10
!
route-map set-nh-ip permit 200
!
!
!
control-plane
!
!
line con 0
 stopbits 1
line aux 0
 stopbits 1
line vty 0 4
 exec-timeout 0 0
 no login
line vty 5 16
 exec-timeout 0 0
 login
!
!
!
end

asr903#                                                 
asr903#show ip route vrf Contrail 

Routing Table: Contrail
Codes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP
       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area 
       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2
       E1 - OSPF external type 1, E2 - OSPF external type 2
       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2
       ia - IS-IS inter area, * - candidate default, U - per-user static route
       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP
       + - replicated route, % - next hop override

Gateway of last resort is 192.168.0.129 to network 0.0.0.0

S*    0.0.0.0/0 [1/0] via 192.168.0.129
      5.0.0.0/24 is subnetted, 1 subnets
S        5.5.5.0 is directly connected, Null0
      10.0.0.0/32 is subnetted, 2 subnets
B        10.84.40.131 [200/0] via 10.84.30.39, 00:22:56, Tunnel0
C        10.250.250.10 is directly connected, Loopback100
      30.0.0.0/8 is variably subnetted, 3 subnets, 2 masks
C        30.30.0.0/24 is directly connected, GigabitEthernet0/4/1
L        30.30.0.3/32 is directly connected, GigabitEthernet0/4/1
C        30.30.40.253/32 is directly connected, Loopback30
      192.168.0.0/24 is variably subnetted, 2 subnets, 2 masks
C        192.168.0.128/30 is directly connected, Tunnel103
L        192.168.0.130/32 is directly connected, Tunnel103
asr903# 
asr903#
asr903#sh
asr903#show ip route vrf Contrail 10.84.40.131

Routing Table: Contrail
Routing entry for 10.84.40.131/32
  Known via "bgp 64512", distance 200, metric 0, type internal
  Last update from 10.84.30.39 on Tunnel0, 00:23:08 ago
  Routing Descriptor Blocks:
  * 10.84.30.39 (default), from 10.84.30.39, 00:23:08 ago, via Tunnel0
      Route metric is 0, traffic share count is 1
      AS Hops 0
      MPLS label: 20
      MPLS Flags: MPLS Required
asr903#
asr903#

asr903#show ip cef vrf Contrail 10.84.40.131 detail 
10.84.40.131/32, epoch 2, flags rib defined all labels
  nexthop 10.84.30.39 Tunnel0 label 20
asr903#

asr903#show tunnel endpoints 
 Tunnel0 running in multi-GRE/IP mode

 Endpoint transport 10.84.30.39 Refcount 3 Base 0x3067EEF8 Create Time 00:23:56
   overlay 10.84.30.39 Refcount 2 Parent 0x3067EEF8 Create Time 00:23:56
asr903#

asr903#show l3vpn encapsulation ip MGRE 

 Profile: MGRE
  transport ipv4 source GigabitEthernet0/4/7
  protocol gre
  payload mpls
   mtu default
  Tunnel Tunnel0 Created [OK]
  Tunnel Linestate [OK]
  Tunnel Transport Source GigabitEthernet0/4/7 [OK]
asr903#
asr903#
 </span></code></pre>
<p><strong><br />
6 Software Gateway</strong><br />
In this section, we will see configuration snippets for using Juniper a virtual SRX (firefly perimeter) as a Software gateway to OpenContrail Cloud. Additional details including configuration is published in the link below and is very similar to the Junos MX gateway configuration</p>
<p><a href="http://www.juniper.net/techpubs/en_US/contrail2.21/topics/task/configuration/simple-gateway-support-vnc.html">http://www.juniper.net/techpubs/en_US/contrail2.21/topics/task/configuration/simple-gateway-support-vnc.html</a></p>
<p><strong>7 CONCLUSION</strong></p>
<p>OpenContrail uses truly open standard based control and data plane signaling and hence can interoperate with any standard gateway to realize real complex use cases.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>OpenContrail Virtual Environment Auto-Provisioning Script</title>
		<link>https://tungsten.io/opencontrail-virtual-environment-auto-provisioning-script/</link>
		
		<dc:creator><![CDATA[Michael Henkel]]></dc:creator>
		<pubDate>Thu, 22 May 2014 00:41:50 +0000</pubDate>
				<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://opencontrail.org/?p=1503</guid>

					<description><![CDATA[Quickly setup a virtual environment on top of OpenContrail/OpenStack (This script is not used to install OpenContrail ! – It installs a virtual environment on top of OpenContrail) In order...]]></description>
										<content:encoded><![CDATA[<h6>Quickly setup a virtual environment on top of OpenContrail/OpenStack</h6>
<p>(This script is not used to install OpenContrail ! – It installs a virtual environment on top of OpenContrail)</p>
<p>In order to quickly setup a virtual environment on top of OpenContrail/OpenStack, I enhanced an existing OpenContrail config script allowing to read a JSON file describing the environment to be setup.<br />
<span id="more-1503"></span></p>
<p>The script uses the OpenContrail/OpenStack services client APIs which must be installed.</p>
<p>As of now the script supports:</p>
<p>&#8211; adding images<br />
&#8211; creating aggregation zones<br />
&#8211; creating flavors<br />
&#8211; creating tenants/projects<br />
&#8211; creating/assigning users and roles to tenants/projects<br />
&#8211; creating networks, ipams, policies and floating pools<br />
&#8211; creating bgp peers<br />
&#8211; booting vms using the created objects from above</p>
<p>This example creates:</p>
<p>&#8211; a public Cirros image<br />
&#8211; two aggregation zones (with one host per zone)<br />
&#8211; two tenants<br />
&#8211; one policy per tenant<br />
&#8211; one ipam per tenant<br />
&#8211; two private networks per tenant<br />
&#8211; one floating public network per tenant<br />
&#8211; two vms per tenant (one in each network and part of one aggregation zone)</p>
<p>Network, IPAM and policy object definitions are not specific to a tenant. Different tenants can use the same object definitions in order to instiate a tenant dedicated object (e.g. net1 &amp; 2, pol1, ipam1 definitions are used by both tenants).</p>
<p>VM object definitions are specific to a tenant.</p>
<p>As of now there isn&#8217;t much failure handling. The script checks for the existence of the objects and continues if they already exist, however it doesn&#8217;t do any logical checks on the JSON fie.</p>
<p>For the future I plan to support service instances and templates as well as more specific policies. As said initial intention was to quickly spawn a base environment on which then can be fine-tuned.</p>
<p>Usage:</p>
<p>./orchestrator add configurator usecase2.json</p>
<pre>Sample usage2.json:
 <code>
 <span style="font-family: 'courier new', courier;">{</span>
<span style="font-family: 'courier new', courier;"> "images":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "Cirros 0.3.2-2",</span>
<span style="font-family: 'courier new', courier;"> "path": "/root/cirros-0.3.2-x86_64-disk.img",</span>
<span style="font-family: 'courier new', courier;"> "disk_format": "qcow2",</span>
<span style="font-family: 'courier new', courier;"> "container": "bare",</span>
<span style="font-family: 'courier new', courier;"> "public": "True"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net1",</span>
<span style="font-family: 'courier new', courier;"> "cidr": "192.168.1.0/24"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net2",</span>
<span style="font-family: 'courier new', courier;"> "cidr": "192.168.2.0/24"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "public1",</span>
<span style="font-family: 'courier new', courier;"> "cidr": "172.16.1.0/24"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "public2",</span>
<span style="font-family: 'courier new', courier;"> "cidr": "172.16.2.0/24"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "aggregates":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "zone1",</span>
<span style="font-family: 'courier new', courier;"> "hosts":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "compute1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ]</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "zone2",</span>
<span style="font-family: 'courier new', courier;"> "hosts":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "compute2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ]</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "policies":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "pol1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "floating_pools":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "fp1"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "fp2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "ipams":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "ipam1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "flavors":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "nano",</span>
<span style="font-family: 'courier new', courier;"> "ram": "64",</span>
<span style="font-family: 'courier new', courier;"> "disk": "0",</span>
<span style="font-family: 'courier new', courier;"> "vcpu": "1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "tenants":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "tenant1",</span>
<span style="font-family: 'courier new', courier;"> "user":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "admin",</span>
<span style="font-family: 'courier new', courier;"> "group": "admin",</span>
<span style="font-family: 'courier new', courier;"> "password": "admin"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net1",</span>
<span style="font-family: 'courier new', courier;"> "ipam": "ipam1",</span>
<span style="font-family: 'courier new', courier;"> "policy": "pol1",</span>
<span style="font-family: 'courier new', courier;"> "route-target": "0",</span>
<span style="font-family: 'courier new', courier;"> "fp": "0"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net2",</span>
<span style="font-family: 'courier new', courier;"> "ipam": "ipam1",</span>
<span style="font-family: 'courier new', courier;"> "policy": "pol1",</span>
<span style="font-family: 'courier new', courier;"> "route-target": "0",</span>
<span style="font-family: 'courier new', courier;"> "fp": "0"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "public1",</span>
<span style="font-family: 'courier new', courier;"> "ipam": "ipam1",</span>
<span style="font-family: 'courier new', courier;"> "policy": "pol1",</span>
<span style="font-family: 'courier new', courier;"> "route-target": "64512:10001",</span>
<span style="font-family: 'courier new', courier;"> "fp": "fp1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ]</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "tenant2",</span>
<span style="font-family: 'courier new', courier;"> "user":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "admin",</span>
<span style="font-family: 'courier new', courier;"> "group": "admin",</span>
<span style="font-family: 'courier new', courier;"> "password": "admin"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net1",</span>
<span style="font-family: 'courier new', courier;"> "ipam": "ipam1",</span>
<span style="font-family: 'courier new', courier;"> "policy": "pol1",</span>
<span style="font-family: 'courier new', courier;"> "route-target": "0",</span>
<span style="font-family: 'courier new', courier;"> "fp": "0"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net2",</span>
<span style="font-family: 'courier new', courier;"> "ipam": "ipam1",</span>
<span style="font-family: 'courier new', courier;"> "policy": "pol1",</span>
<span style="font-family: 'courier new', courier;"> "route-target": "0",</span>
<span style="font-family: 'courier new', courier;"> "fp": "0"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "public2",</span>
<span style="font-family: 'courier new', courier;"> "ipam": "ipam1",</span>
<span style="font-family: 'courier new', courier;"> "policy": "pol1",</span>
<span style="font-family: 'courier new', courier;"> "route-target": "64512:10002",</span>
<span style="font-family: 'courier new', courier;"> "fp": "fp2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ]</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "vms":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "i1-tenant2",</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "floating":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "fp2",</span>
<span style="font-family: 'courier new', courier;"> "nic": "net1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "image": "Cirros 0.3.2-2",</span>
<span style="font-family: 'courier new', courier;"> "flavor": "nano",</span>
<span style="font-family: 'courier new', courier;"> "tenant": "tenant2",</span>
<span style="font-family: 'courier new', courier;"> "zone": "zone1"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "i2-tenant2",</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "floating":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "fp2",</span>
<span style="font-family: 'courier new', courier;"> "nic": "net2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "image": "Cirros 0.3.2-2",</span>
<span style="font-family: 'courier new', courier;"> "flavor": "nano",</span>
<span style="font-family: 'courier new', courier;"> "tenant": "tenant2",</span>
<span style="font-family: 'courier new', courier;"> "zone": "zone2"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "i1-tenant1",</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "floating":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "fp1",</span>
<span style="font-family: 'courier new', courier;"> "nic": "net1"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "image": "Cirros 0.3.2-2",</span>
<span style="font-family: 'courier new', courier;"> "flavor": "nano",</span>
<span style="font-family: 'courier new', courier;"> "tenant": "tenant1",</span>
<span style="font-family: 'courier new', courier;"> "zone": "zone1"</span>
<span style="font-family: 'courier new', courier;"> },</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "i2-tenant1",</span>
<span style="font-family: 'courier new', courier;"> "networks":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "net2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "floating":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "fp1",</span>
<span style="font-family: 'courier new', courier;"> "nic": "net2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "image": "Cirros 0.3.2-2",</span>
<span style="font-family: 'courier new', courier;"> "flavor": "nano",</span>
<span style="font-family: 'courier new', courier;"> "tenant": "tenant1",</span>
<span style="font-family: 'courier new', courier;"> "zone": "zone2"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ],</span>
<span style="font-family: 'courier new', courier;"> "bgp_router":</span>
<span style="font-family: 'courier new', courier;"> [</span>
<span style="font-family: 'courier new', courier;"> {</span>
<span style="font-family: 'courier new', courier;"> "name": "dcgw1",</span>
<span style="font-family: 'courier new', courier;"> "ip": "192.168.18.13",</span>
<span style="font-family: 'courier new', courier;"> "asn": "64512",</span>
<span style="font-family: 'courier new', courier;"> "type": "other"</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> ]</span>
<span style="font-family: 'courier new', courier;"> }</span></code></pre>
<h4><strong>Attachments</strong>: <a href="http://www.opencontrail.org/wp-content/uploads/2014/05/orch-agent.tar.gz">orch-agent.tar</a></h4>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>How to Enable Dynamic, Network-Based Services</title>
		<link>https://tungsten.io/how_to_enable_dynamic_network-based_services/</link>
		
		<dc:creator><![CDATA[Marco Rodrigues]]></dc:creator>
		<pubDate>Tue, 26 Nov 2013 00:14:53 +0000</pubDate>
				<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://opencontrail.org/?p=810</guid>

					<description><![CDATA[This blog post will provide a step-by-step guide to configuring a dynamic in-network service chain. As emphasized in prior blog postings on OpenContrail.org, using well known and defined standards allows...]]></description>
										<content:encoded><![CDATA[<p>This blog post will provide a step-by-step guide to configuring a dynamic in-network service chain. As emphasized in prior blog postings on OpenContrail.org, using well known and defined standards allows a service provider to insert network-based technologies and services while leveraging a well known and understood building block of modifying the next-hop of a destination route to point to the appliance/service VM of choice. This is known as “service chaining.”  Below are five steps I will cover to show how automated service chaining can be done:</p>
<p><span id="more-810"></span></p>
<ol>
<li>Configuring a Gateway Router that will provide the bridging of the ‘physical’ VPN world of a customer to a ‘virtual’ world offering services. In this example, the NFV being offered is that of a NAT NFV on a NAT enabled VM. That gateway router will be configured to support L3VPN signaled via BGP for control plane exchanges with the data plane being MPLS over GRE.</li>
<li>Ensure the proper VPN’s (both for Enterprise and Internet Transport) are configured on the PE. VPN parameters must match those of the VN’s being configured.</li>
<li>Import a capable image of offering the service in question. We will be using Juniper’s Firefly Perimeter product to do this.</li>
<li>Using the OpenContrail Web Console to configure the appropriate VN’s that will inject the service in question via a service chain.</li>
<li>Spin up the service VM via compute orchestration using Openstack’s Nova component and the network overlay and service chaining using OpenContrail’s vRouter integration.</li>
</ol>
<p>A video is available which captures the end user experience of how they can enable dynamic cloud based services (NFV) inserted in real-time into their existing VPN offering.</p>
<p>The video entitled “<i>Production-ready Network Function Virtualization through Contrail</i> “ can be viewed below:</p>
[video_lightbox_youtube video_id=&#8221;_64no8P2vUw&#8221; width=&#8221;640&#8243; height=&#8221;480&#8243; anchor=&#8221;http://opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image17.png&#8221;]
<p>The following topology will be our reference for the remainder of this blog:</p>
<p><b>Topology Reference:</b></p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5759 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image1.png" alt="blogpost_810_image1" width="551" height="453" data-id="5759" /></p>
<h6><span style="color: #3366ff;">1. Configuring the BGP L3VPN address family between the Gateway router and OpenContrail Control Node.</span></h6>
<p>In this step, we will configure both the Gateway Router and the OpenContrail System to have an L3VPN BGP session in order to exchange virtual VM reachability (service VM’s) between both the physical network and virtual network.</p>
<p>The IP fabric underlay is on subnet: 172.16.0.0/24<br />
The Control Nodes are: 172.16.0.71 and 172.16.0.72<br />
The PE Gateway is configured: 172.16.0.10</p>
<p>IBGP session between PE Gateway and OpenContrail Control Nodes.</p>
<p><strong>Step 1.</strong></p>
<p>Configure the PE BGP parameters on Contrail.</p>
<p>Go to Configure -&gt; Infrastructure -&gt; BGP Peer and Click ‘Create’ at the Top Right Hand</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5760 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image2.png" alt="blogpost_810_image2" width="564" height="519" data-id="5760" /></p>
<p><strong>Step 2.</strong></p>
<p>Configure the BGP enabled L3VPN address family on the vendor PE Gateway router.</p>
<pre><span style="font-family: 'courier new', courier;">[ protocols bgp ]</span></pre>
<pre><span style="font-family: 'courier new', courier;"> group JUNOSV-CONTRAIL {</span>
<span style="font-family: 'courier new', courier;">     type internal;</span>
<span style="font-family: 'courier new', courier;">     local-address 172.16.0.10;</span></pre>
<pre><span style="font-family: 'courier new', courier;">    family inet-vpn {</span>
<span style="font-family: 'courier new', courier;">         unicast;</span>
<span style="font-family: 'courier new', courier;">     }</span>
<span style="font-family: 'courier new', courier;">     vpn-apply-export;</span>
<span style="font-family: 'courier new', courier;">     peer-as 64512;</span>
<span style="font-family: 'courier new', courier;">     allow 172.16.0.0/24;</span>
<span style="font-family: 'courier new', courier;"> }</span></pre>
<p><strong>Step 3.</strong></p>
<p>Ensure dynamic MPLSoGRE tunneling is enabled on the PE Gateway router.</p>
<pre><span style="font-family: 'courier new', courier;">[ routing-options ]</span></pre>
<pre><span style="font-family: 'courier new', courier;">dynamic-tunnels {</span>
<span style="font-family: 'courier new', courier;">     JUNOSV-CONTRAIL {</span>
<span style="font-family: 'courier new', courier;">         source-address 172.16.0.10;</span>
<span style="font-family: 'courier new', courier;">         gre;</span>
<span style="font-family: 'courier new', courier;">         destination-networks {</span>
<span style="font-family: 'courier new', courier;">             172.16.0.0/24;</span>
<span style="font-family: 'courier new', courier;">         }</span>
<span style="font-family: 'courier new', courier;">     }</span>
<span style="font-family: 'courier new', courier;"> }</span></pre>
<h6><span style="color: #3366ff;">2. Setup Enterprise and Internet Transport VRF’s</span></h6>
<p>In this section, we ensure that the corresponding L3VPN customer is properly configured on the PE Gateway router along with the proper L3VPN VRF to transport Internet Traffic.</p>
<p><i>The VPN Green customer has the parameters:</i></p>
<p>RT 64512:100<br />
RD (default/auto allocation based on PE implementation)</p>
<p>&nbsp;</p>
<p><i>The Internet Transport VPN has the parameters:</i></p>
<p>RT 666:100<br />
RD (default/auto allocation based on PE implementation)</p>
<p><strong> Step 1.</strong></p>
<p>Configure the customer L3VPN VRF on the PE.</p>
<pre><span style="font-family: 'courier new', courier;">[ routing-instances VPN-GREEN ]</span>
<span style="font-family: 'courier new', courier;"> instance-type vrf;</span>
<span style="font-family: 'courier new', courier;"> interface ge-1/1/0.0;</span>
<span style="font-family: 'courier new', courier;"> interface lo0.500;</span>
<span style="font-family: 'courier new', courier;"> route-distinguisher 11.11.11.137:64512;</span>
<span style="font-family: 'courier new', courier;"> vrf-target 64512:100</span>
<span style="font-family: 'courier new', courier;"> vrf-table-label;</span>
<span style="font-family: 'courier new', courier;"> routing-options {</span>
<span style="font-family: 'courier new', courier;"> static {</span>
<span style="font-family: 'courier new', courier;"> route 192.168.100.0/24 next-hop 192.168.200.2;</span>
<span style="font-family: 'courier new', courier;"> }</span>
<span style="font-family: 'courier new', courier;"> }</span></pre>
<p><strong>Step 2.</strong></p>
<p>Configure the customer Internet Transport VRF on the PE. Note the expectation is that IANA allocated routable public blocks are provided as part of the public VN, available through this VRF (LSP) with the appropriate routing on ingress/egress points. In this example we follow RFC 6598 which provides non-routable public blocks for CGNAT purposes for the left (private) VN. This also ensures uniqueness of routes being injected into a customer VPN. It is required that the public VN uses a pool of IP addresses that are indeed publically routable and owned by the service provider.</p>
<pre><span style="font-family: 'courier new', courier;">[ routing-instances INTERNET-TRANSPORT ]</span>
<span style="font-family: 'courier new', courier;"> instance-type vrf;</span>
<span style="font-family: 'courier new', courier;"> interface lo0.200;</span>
<span style="font-family: 'courier new', courier;"> route-distinguisher 11.11.11.137:666;</span>
<span style="font-family: 'courier new', courier;"> vrf-target 666:100</span>
<span style="font-family: 'courier new', courier;"> vrf-table-label;</span></pre>
<p>With the above configuration we expect to learn a default route to the Internet (and/or any specific NET/CIDR blocks) with reachability to an Internet Gateway device within the SP network.</p>
<h6><span style="color: #3366ff;">3. Import Service Image via Openstack Horizon GUI (or glance via CLI)</span></h6>
<p>In this step, we import a Juniper Firefly Perimeter image which is preconfigured to perform a NAT function between anything being received on the left virtual network (aka. the internal Private VPN network, aka. 64512:1000) to the right virtual network (aka. The public internet, aka. 666:100).</p>
<p>An alternative to importing an image can be done via the glance CLI on the Configuration node. This will not be covered here.</p>
<p><strong>Step 1.</strong></p>
<p>Login to the Horizon GUI interface:</p>
<p>Go to Project (in this case it’s demo) -&gt; Images &amp; Snapshots -&gt; Create Image</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5762" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image3.png" alt="blogpost_810_image3" width="719" height="618" data-id="5762" /></p>
<p>If importing is successful you should see an image called firefly-12-1-nat:</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5763" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image4.png" alt="blogpost_810_image4" width="1131" height="327" data-id="5763" /></p>
<h6><span style="color: #3366ff;">4. Create the supporting parameters via the Contrail Web Console (virtual networks, net blocks, service templates, etc.)</span></h6>
<p>In this section, we will create two virtual networks for the ‘left’ network, also known as private, within the VPN. The other virtual network will be the ‘right’ network also known as public (scary Internet). The VN will have RT’s that match the VRF parameters we configured on the PE Gateway router and net blocks from RFC 6598. We will also create a service template in which future service instances will inherit the template properties.</p>
<p>Virtual Network Overlay Parameters:</p>
<p>Left Network: RT 64512:100, NETBLOCK: 100.64.0.0/24</p>
<p>Right Network: RT 666:100, NETBLOCK: 100.65.0.0/24 (This should be a publicly owned and routeable netblock!)</p>
<p>Associate a default network policy (which is configured for any to any)</p>
<p><strong>Step 1.</strong></p>
<p>Configure the ‘left’ virtual network by going to:</p>
<p>Configure -&gt; Networking -&gt; Networks  (Create Top Right Hand Corner).</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5764 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image5.png" alt="blogpost_810_image5" width="700" height="512" data-id="5764" /><img loading="lazy" decoding="async" class="size-full wp-image-5765 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image6.png" alt="blogpost_810_image6" width="698" height="226" data-id="5765" /></p>
<p>By clicking save you should see the following VN created:</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5766" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image7.png" alt="blogpost_810_image7" width="945" height="178" data-id="5766" /></p>
<p><strong>Step 2.</strong></p>
<p>Configure the ‘right virtual network by going to:</p>
<p>Configure -&gt; Networking -&gt; Networks  (Create Top Right Hand Corner).</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5767 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image8.png" alt="blogpost_810_image8" width="696" height="515" data-id="5767" /><img loading="lazy" decoding="async" class="size-full wp-image-5768 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image9.png" alt="blogpost_810_image9" width="674" height="188" data-id="5768" /></p>
<p>By clicking save you should see the following VN created:</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5769" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image10.png" alt="blogpost_810_image10" width="932" height="149" data-id="5769" /></p>
<p><strong>Step 3.</strong> Create a service template for Internet NFV Services.</p>
<p>Configure -&gt; Services -&gt; Service Templates  (Create Top Right Hand Corner).</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5770 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image11.png" alt="blogpost_810_image11" width="570" height="450" data-id="5770" /></p>
<p>By clicking save you should see the following Service Template created:</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5771" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image12.png" alt="blogpost_810_image12" width="937" height="198" data-id="5771" /></p>
<h6><span style="color: #3366ff;">5. Complete the Chain! Instantiate a service image based on the Internet NFV Service Template and enable the Service Chain.</span></h6>
<p>In this step we spin up a service VM for a customer (in this example customer Green VPN) and once the VM is orchestrated along with the network virtual overlay(s) we then ‘chain’ the two VN’s with one another; through the service; thus completing the dynamic instantiation of the NAT NFV service.</p>
<p><strong> Step 1.</strong></p>
<p>Configure -&gt; Services -&gt; Service Instances  (Create Top Right Hand Corner).</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5772 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image13.png" alt="blogpost_810_image13" width="698" height="316" data-id="5772" /></p>
<p><strong>Step 2.</strong></p>
<p>Click save and a service instance should spin up. You can confirm this by looking at the Horizon GUI (you can see IP addressing being selected in each VN, both left and right).</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5773" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image14.png" alt="blogpost_810_image14" width="895" height="305" data-id="5773" /></p>
<p>You can also confirm this in the Contrail Web Console as shown below:</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5774" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image15.png" alt="blogpost_810_image15" width="951" height="267" data-id="5774" /></p>
<p><strong>Step 3.</strong></p>
<p>Create a network policy, which enables the service chain.</p>
<p>Configure -&gt; Networks -&gt; Policies</p>
<p><img loading="lazy" decoding="async" class="alignnone size-full wp-image-5775" src="http://www.opencontrail.org/wp-content/uploads/2013/11/blogpost_810_image16.png" alt="blogpost_810_image16" width="994" height="378" data-id="5775" /></p>
<p>The OpenContrail (vRouter) is configured for the service VM (green-vpn) that ALL routes being learnt via the right network from the PE Gateway (RT 666:100) to be leaked into the left network (RT 64512:100) but with a new next-hop of left VN VIF (which in this instance’s case is 100.64.0.253 and reachable via the GREEN VPN).</p>
<p>This will pull all default route traffic (assuming a 0.0.0.0/0 is learnt with RT 666:100) to be sent to the ‘left’ interface of the service VM, NAT’d through the service VM itself, and sent out the right VN towards the public network.</p>
<h6><span style="color: #3366ff;">Conclusion</span></h6>
<p>The above steps will allow a user to configure a single in-network (routed) service chain using well-known and tested technologies. What should also be pointed out is that all of the above configurations are available through the vnc_api RESTful interface. As shown by the video “<a href="http://www.youtube.com/watch?v=_64no8P2vUw">Production-ready Network Function Virtualization through Contrail</a>” referenced above; it used a custom web interface using standard REST interfaces that the same Contrail Web Console leverages.</p>
<p>&nbsp;</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Comparing Network Virtualization Techniques Available in the Cloud</title>
		<link>https://tungsten.io/comparing-network-virtualization-techniques-available-in-the-cloud/</link>
		
		<dc:creator><![CDATA[Parantap Lahiri]]></dc:creator>
		<pubDate>Tue, 19 Nov 2013 04:46:18 +0000</pubDate>
				<category><![CDATA[Gateway]]></category>
		<category><![CDATA[Network Services]]></category>
		<category><![CDATA[Routing/Switching]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">http://opencontrail.org/?p=745</guid>

					<description><![CDATA[Technologies used to virtualize cloud networks are evolving quickly. Many times it is non-trivial to sort out the different technologies and understand merits and demerits of specific approaches. This write-up...]]></description>
										<content:encoded><![CDATA[<p style="text-align: justify;">Technologies used to virtualize cloud networks are evolving quickly. Many times it is non-trivial to sort out the different technologies and understand merits and demerits of specific approaches.</p>
<p style="text-align: justify;">This write-up 1) explains the network virtualization techniques used in legacy virtualized environments, 2) discusses the difference between legacy and cloud datacenters, and 3) compares the virtualization techniques used in cloud datacenters.</p>
<p><span id="more-745"></span></p>
<p style="text-align: center;"> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-5777" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image1.png" alt="post_745_image1" width="659" height="402" data-id="5777" /></p>
<p style="text-align: center;"><span style="color: #3366ff;">Figure 1: Legacy Datacenter Network</span></p>
<p style="text-align: justify;">Figure 1 shows a legacy datacenter that runs server virtualization.</p>
<p style="text-align: justify;">Typically the network is segmented into multiple L2 domains. Each L2 domain comprises of a set of ToRs (Top of Rack) switches that connect to a pair of aggregation switches. The virtualized servers are connected to ToR switches. There are other variances with “End-of-Row” switch etc., but fundamentally it comes down to a small and well-defined span of L2 domain. These designs create a tree-topology where each node essentially imposes serious over-subscriptions. The performance of the network is managed by adding additional links/ports etc. to each aggregation switch through the process of “capacity management”. In most cases, the network looks different in different parts of the datacenter, even though they are based of the same architecture.</p>
<p style="text-align: center;"> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-5779" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image2.png" alt="post_745_image2" width="662" height="434" data-id="5779" /></p>
<p style="text-align: center;"><span style="color: #3366ff;">Figure 2: Network Virtualization in legacy Datacenters</span></p>
<p style="text-align: justify;">Figure 2 shows how network virtualization is implemented in such a legacy network. Typically, an Ethernet trunk (carrying all or selected VLANs) is extended from ToR switches to each virtualized server. The virtual machines instantiated within the server connect to one or more VLANs. The VLANs span is limited by the size of the L2 domain as shown in the diagram.</p>
<p style="text-align: justify;"> In most cases only a single routing space is supported on the L2/L3 aggregation switches. Any inter-VLAN traffic gets routed at the aggregation switch. Packet filters (ACLs, access control lists) are also typically applied on the aggregation switch on each VLAN interface port.</p>
<p style="text-align: center;"> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-5780" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image3.png" alt="post_745_image3" width="669" height="439" data-id="5780" /></p>
<p style="text-align: center;"><span style="color: #3366ff;">Figure 3: Multi-tenancy in legacy datacenter</span></p>
<p style="text-align: justify;">People who want to use a similar frame-work for scenarios that support multiple tenants with overlapping address spaces, have used either VRF-lite (Virtual Routing Forwarding without MPLS) or proper VRF with MPLS to create separate routing space for each tenant on the aggregation layer.</p>
<p style="text-align: justify;">In many cases this approach also entails separate pairs of physical firewall and load-balancer appliances dedicated to each tenant. As shown in Figure 3, the core layer would need to be enabled with MPLS (along with control plane protocol like LDP etc.) to span a tenant across multiple L2 domains.</p>
<p style="text-align: center;"> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-5781" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image4.png" alt="post_745_image4" width="663" height="422" data-id="5781" /></p>
<p style="text-align: center;"><span style="color: #3366ff;">Figure 4: Basic Network Virtualization in Cloud Datacenters</span></p>
<p style="text-align: justify;">A large number of the modern cloud datacenters are built using L3 (routing) to the ToR switch. This is primarily done since L3 routing protocols like OSPF or BGP can easily support densely intermeshed topologies (e.g. CLOS) and help utilize the symmetrical IP fabrics by distributing flows over multiple equal cost paths. IP network is also chosen due to its ubiquity as it can span across multiple data centers etc.</p>
<p style="text-align: justify;">In such datacenters, the VLAN construct is largely inapplicable due to lack of L2 domains. Hence, there has been some adoption of encapsulation mechanisms like VXLAN (or NVGRE) that encapsulates each Ethernet frame into an IP packet that transports it over IP network.  In Figure 4, each colored dotted line represents a similar L2 network. The soft switches (virtual switches) that run on each virtualized servers are typically L2 only. Any traffic that is trying to leave the L2 LAN is send to a software router that runs as a virtual machine (VM). In some cases the virtual switch has been given the capability to switch traffic between LANs, but the gateway is typically a software router that runs as a VM.</p>
<p>In such setups, each tenant potentially gets a pair of software routers that act as gateways. They get additional routers based on capacity needs. The routers implement policies like packet filtering, NAT (network address translation) etc.<img loading="lazy" decoding="async" class="size-full wp-image-5782 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image5.png" alt="post_745_image5" width="660" height="436" data-id="5782" /></p>
<p style="text-align: center;"><span style="color: #3366ff; text-align: center;">Figure 5: Multi-tenant Cloud Datacenters with advanced server based capabilities</span></p>
<p style="text-align: justify;">However, technologies that can help avoid the complexity of operating multiple software routers, etc. are available.</p>
<p style="text-align: justify;">Figure 5 shows how each software switch inside the virtualized servers can be made to perform switching, routing and in-line packet en(de)capsulation. Thus the kernel modules residing within each virtualized server acts as a multi-VRF router that performs NAT, packet filtering and external access etc. directly. Such deployments are not subject to operational issues that come naturally with multiple instances of software routers.</p>
<p style="text-align: center;"> <img loading="lazy" decoding="async" class="alignnone size-full wp-image-5783" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image6.png" alt="post_745_image6" width="604" height="472" data-id="5783" /></p>
<p style="text-align: center;"><span style="color: #3366ff;">Figure 6: Virtual Services within multi-tenant cloud datacenters</span></p>
<p style="text-align: justify;">Deployments that utilize technologies as described in Figure 5, easily abstract out the virtual machines within cloud datacenters into multiple tenants. Each of these tenants can have multiple Virtual Networks within security policies between the networks. These tenants can also impose security groups on individual instance basis or insert virtualized service instances (e.g. virtual firewall, virtual DDos  mitigation etc.) as needed between the virtual networks.</p>
<p style="text-align: justify;">Typically these datacenters would need some standard routers that supports IP en(de)capsulations and MBGP (multi-protocol BGP) to act as a gateway to external public networks. Thus the extreme forwarding power of ASIC within the routers can be leveraged in this approach.</p>
<p style="text-align: justify;">The other advantage of this approach is the easy integration with the service provider L3VPN infrastructure through the same set of gateway router.</p>
<p><img loading="lazy" decoding="async" class="size-full wp-image-5784 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2013/11/post_745_image7.png" alt="post_745_image7" width="637" height="384" data-id="5784" /></p>
<p style="text-align: center;"><span style="color: #3366ff; text-align: center;">Figure 7: Multi-tenant multi-datacenter cloud network</span></p>
<p style="text-align: justify;">Figure 7 shows, how the network virtualization technology can be easily extended to build a multi-tenant cloud infrastructure over multiple datacenters. Since the underlying fabric that typically inter-connects the different datacenters are IP, the cloud virtualization technologies that use IP encapsulations easily span across multiple datacenters.</p>
<p style="text-align: justify;">All the datacenters can use a common router gateway to access external network or service provider L3VPN infrastructure.</p>
<h4 style="text-align: justify;">Conclusion</h4>
<p style="text-align: justify;">Serious cloud service providers and large scale enterprise networks are considering cloud network virtualization techniques that don’t put artificial constraint on the scaling of the networks based on archaic L2-L3 demarcation, etc. They are looking for technologies and solutions that provide multi-datacenter and multi-tenant L2-L3 support in a scaled out way that can be heavily operationalized. More so, the technologies need to not only be open sourced but also based on open standards where the protocol interactions, etc. are clearly spelled out.</p>
<p style="text-align: justify;">Interestingly, the technology now available with OpenContrail (shown in Figure 5, 6, and 7) meets many of these requirements. Time will tell which vendor/provider/enterprise adopts strategic technologies that lead to their overall success.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
