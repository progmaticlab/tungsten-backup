<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Kubernetes Archives - Tungsten Fabric</title>
	<atom:link href="https://tungsten.io/category/kubernetes/feed/" rel="self" type="application/rss+xml" />
	<link>https://tungsten.io/category/kubernetes/</link>
	<description>multicloud multistack SDN</description>
	<lastBuildDate>Tue, 25 Jul 2023 20:59:21 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.1</generator>

<image>
	<url>https://tungsten.io/wp-content/uploads/sites/73/2018/03/cropped-TungstenFabric_Stacked_Gradient_3000px-150x150.png</url>
	<title>Kubernetes Archives - Tungsten Fabric</title>
	<link>https://tungsten.io/category/kubernetes/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Deploying a Kubernetes operator in OpenShift 4.x platform</title>
		<link>https://tungsten.io/deploying-a-kubernetes-operator-in-openshift-4-x-platform/</link>
		
		<dc:creator><![CDATA[tungstenfabric]]></dc:creator>
		<pubDate>Mon, 16 Aug 2021 14:54:00 +0000</pubDate>
				<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[OpenShift]]></category>
		<category><![CDATA[SDN]]></category>
		<category><![CDATA[Use Case]]></category>
		<category><![CDATA[CoreOS]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Tungsten Fabric]]></category>
		<guid isPermaLink="false">https://tungsten.io/?p=8367</guid>

					<description><![CDATA[This is a contributed blog from LF Networking Member CodiLime.&#160;Originally published here. Contrail-operator&#160;is a recently released open-source Kubernetes operator that implements Tungsten Fabric&#160; as a custom resource. Tungsten Fabric is...]]></description>
										<content:encoded><![CDATA[<link rel="canonical" href="https://codilime.com/blog/deploying-a-kubernetes-operator-in-openshift-4-x-platform">


<p><em><strong>This is a contributed blog from LF Networking Member CodiLime.&nbsp;<a href="https://codilime.com/blog/deploying-a-kubernetes-operator-in-openshift-4-x-platform" target="_blank" rel="noreferrer noopener">Originally published here</a></strong>.</em></p>



<p><strong><a href="https://github.com/Juniper/contrail-operator">Contrail-operator</a>&nbsp;is a recently released open-source Kubernetes operator that implements Tungsten Fabric&nbsp; as a custom resource. Tungsten Fabric is an open-source Kubernetes-compatible, network virtualization solution for providing connectivity and security for virtual, containerized or bare-metal workloads. An operator needed to be adjusted to the OpenShift 4.x platform, which introduced numerous changes to its architecture compared with previous versions. In this blog post, you’ll read about three interesting use cases and their solutions. All of these solutions are a part of&nbsp;<a href="https://github.com/Juniper/contrail-operator/tree/master/deploy/openshift">contrail-operator public repository</a>.</strong></p>



<h2 class="wp-block-heading">Use case 1: inject kernel in CoreOS with OverlayFS</h2>



<p><a href="https://en.wikipedia.org/wiki/OpenShift">OpenShift</a>&nbsp;is a container platform designed by Red Hat. Its version 4.x is based on nodes that use&nbsp;<a href="https://en.wikipedia.org/wiki/Container_Linux">CoreOS</a>, an open-source operating system based on the Linux kernel. CoreOS has been designed specifically to allow changes in the system only when booting it for the first time. These changes are introduced using ignition configs—JSON files containing, for example, names of services, files or users to be created. When the OS is up and running, most of its settings can be seen in read-only mode and users are not allowed to modify system settings.</p>



<p>The setup is presented in Figure 1:</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/c03564453aee495e0696081960dd4ff0399cbc73/dff79/img/codilime_desired_deployment_of_tf_with_an_openshift_setup.png" alt="Deployment of Tungsten Fabric with an Openshift setup"/></figure>



<p><strong>Fig 1. The desired deployment of Tungsten Fabric with an OpenShift setup</strong></p>



<p>In Tungsten Fabric, vRouter is injected into the system as a kernel module. In the contrail-operator (and also in the&nbsp;<a href="https://github.com/tungstenfabric/tf-ansible-deployer">tf-ansible-deployer</a>, effectively the operator’s predecessor) this is done by launching a container that injects this module into the system. With OpenShift, this task is handled by daemonSet, which launches a pod on every node. In such a pod, one of the initContainers (i.e. containers launched to perform a given operation only once and then shut down, thus allowing the proper containers to be launched) injects the kernel module into the system. Yet given the characteristics of the CoreOS, this operation cannot be performed because a container will inject a read-only kernel module to /lib/modules.</p>



<p>Enter the solution to this challenge: overlayFS, which virtually merges the two directories:/lib/modules (read only) and /opt/modules (writable). Ignition config is now created, which will set OverlayFS /lib/modules with /opt/modules directories. The latter was accessible and it was possible to inject a kernel module there (see Figure 2). Such a solution did not make any difference from the perspective of the Tungsten Fabric Controller. Hence, it was not necessary to change anything in TF itself.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/69eb2300a7ad4c2deafbe9ef00fd48a64557b067/b684f/img/codilime_overlayfs_of_two_directories.png" alt="The OverlayFS of two directories"/></figure>



<p><strong>Fig. 2 The OverlayFS of two directories</strong></p>



<p>Ignition config looks like this:</p>



<pre class="wp-block-code"><code>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
 labels:
   machineconfiguration.openshift.io/role: master
 name: 02-master-modules
spec:
 config:
   ignition:
     version: 2.2.0
   storage:
     directories:
       - filesystem: "root"
         path: "/opt/modules"
         mode: 0755
       - filesystem: "root"
         path: "/opt/modules.wd"
         mode: 0755
     files:
       - filesystem: "root"
         path: "/etc/fstab"
         mode: 0644
         contents:
           source: "data:,overlay%20/lib/modules%20overlay%20lowerdir=/lib/modules,upperdir=/opt/modules,workdir=/opt/modules.wd%200%200"</code></pre>



<p>Source:&nbsp;<a href="https://github.com/Juniper/contrail-operator/blob/master/deploy/openshift/openshift/99_master-kernel-modules-overlay.yaml">GitHub</a></p>



<p>Ignition config creates two directories: /opt/modules, to inject modules, and /opt/modules.wd, a working directory. Next, in the /etc/fstab, the mount is defined:</p>



<p><code>overlay /lib/modules overlay lowerdir=/lib/modules,upperdir=/opt/modules,workdir=/opt/modules.wd</code></p>



<p>Interestingly, it is not a typical ignition config for CoreOS, but a custom resource from an OpenShift cluster—MachineConfig. It performs the same functions as ignition config but is also visible as a cluster resource and allows you to edit the config when the cluster is running. In this way, you can apply changes to the CoreOS node even after first boot, which is not usually supported by standard CoreOS-based deployments. This is a feature specific to OpenShift.</p>



<h2 class="wp-block-heading">Use case 2: set nftables rules of CoreOS with ignition config</h2>



<p>CoreOS uses nftables, a newer framework for packet management than iptables. With a normal system like RHEL8, this is still an iptables command-line tool but in its backend it uses nftables. The iptables syntax is converted into respective nftables commands in the backend, so you can still use classic iptables commands and nftables will be still properly configured. Of course, in the CoreOS there is no such tool as iptables, as nobody assumes that the rules for packet handling will be changed when the system is up and running.&nbsp;</p>



<p>It is true that in one of the initContainers located in a vRouter configuration pod, an iptables tool is used to carry out several operations. But the container is based on RHEL7 which in turn uses iptables backend. It is worth noting that a CLI iptables tool can support backend with iptables or nftables, though this depends on the system’s backend in which it was compiled.</p>



<p>To check what backend is used by iptables (CLI), just write the following command:&nbsp;<code>iptables --version</code>. If&nbsp;<code>(nftables)</code>&nbsp;is the reply, the tool supports nftables backend. If there is no such reply, it means that the tool supports iptables backend.&nbsp;</p>



<p>Meanwhile, a container had a version without nftables, so it was impossible to establish rules using a container. Ignition configs helped solve this challenge. During the system boot, rules can be established using a native iptables tool:</p>



<pre class="wp-block-code"><code>apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
 labels:
   machineconfiguration.openshift.io/role: master
 name: 10-master-iptables
spec:
 config:
   ignition:
     version: 2.2.0
   systemd:
     units:
     - name: iptables-contrail.service
       enabled: <strong>true</strong>
       contents: |
         &#91;Unit]
         Description=Inserts iptables rules required by Contrail
         After=syslog.target
         AssertPathExists=/etc/contrail/iptables_script.sh
 
         &#91;Service]
         Type=oneshot
         RemainAfterExit=yes
         ExecStart=/etc/contrail/iptables_script.sh
         StandardOutput=syslog
         StandardError=syslog
 
         &#91;Install]
         WantedBy=basic.target
   storage:
     files:
     - filesystem: root
       path: /blog/etc/contrail/iptables_script.sh
       mode: 0744
       user:
         name: root
       contents:
         # 'data:,' and URL encoded openshift-install/sources/iptables_script.sh
         source: data:...,</code></pre>



<p>The full version of the code can be found on&nbsp;<a href="https://github.com/Juniper/contrail-operator/blob/master/deploy/openshift/openshift/99_worker-iptables-machine-config.yaml">GitHub</a>.</p>



<p>In this config a service is created and run as a oneshot script during the system boot. The script is then created on the path:&nbsp;<code>/etc/contrail/iptables_script.sh</code>. The full version of the script is available in the&nbsp;<a href="https://github.com/Juniper/contrail-operator/blob/master/deploy/openshift/sources/iptables_script.sh">GitHub repository</a>. Generally speaking, these are simple iptables commands setting up the resources needed to run Tungsten Fabric.</p>



<h2 class="wp-block-heading">Use case 3: why namespaced owner orphans cluster resource</h2>



<p>The last use case concerns the implementation of Kubernetes. During the tests one of the child resources was constantly being deleted, for no apparent reason. An investigation revealed the source of the problem: Owner reference is set for resources like Persistent Volume and Storage Class. According to the&nbsp;<a href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">Kubernetes documentation</a>:&nbsp;</p>



<blockquote class="wp-block-quote"><p><em>Cross-namespace owner references are disallowed by design. This means that namespace-scoped dependents can only specify owners in the same namespace, and owners that are cluster-scoped. Cluster-scoped dependents can only specify cluster-scoped owners, but not namespace-scoped owners.</em></p></blockquote>



<p>So owner reference for Persistent Volume and Storage Class (both cluster-wide resources) was set for a namespaced resource. That was why the garbage collector in Kubernetes kept deleting the entire component. Garbage collector saw that the cluster-scoped resource had set the owner and tried to find it only in cluster-scoped resources. However, the owner was hidden in the namespace. As a result, the garbage collector recognized the resource as orphaned and deleted it in order to keep the cluster clean.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/62e099a3e5dd426221a23e4078597d92919738fb/37e61/img/codilime_namespace_owner.png" alt="MY RESOURCE can be the owner only of another resource in its namespace but not in a different namespace (cluster-scoped resource)"/></figure>



<p><strong>Fig 3. MY RESOURCE can be the owner only of another resource in its namespace but not in a different namespace (cluster-scoped resource)</strong></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tungsten Fabric as a Kubernetes CNI plugin</title>
		<link>https://tungsten.io/tungsten-fabric-as-a-kubernetes-cni-plugin/</link>
		
		<dc:creator><![CDATA[tungstenfabric]]></dc:creator>
		<pubDate>Mon, 09 Aug 2021 17:42:32 +0000</pubDate>
				<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[OpenStack]]></category>
		<category><![CDATA[SDN]]></category>
		<category><![CDATA[LF Networking]]></category>
		<category><![CDATA[LFN]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Tungsten Fabric]]></category>
		<guid isPermaLink="false">https://tungsten.io/?p=8358</guid>

					<description><![CDATA[This is a contributed blog from LF Networking Member CodiLime. Originally published here. CNI (Container Networking Interface) is an interface between container runtime and network implementation. It allows different projects,...]]></description>
										<content:encoded><![CDATA[
<p><em><strong>This is a contributed blog from LF Networking Member CodiLime. <a href="https://codilime.com/tungsten-fabric-as-a-kubernetes-cni-plugin/" target="_blank" rel="noreferrer noopener">Originally published here</a></strong>.</em></p>



<p><strong>CNI (Container Networking Interface) is an interface between container runtime and network implementation. It allows different projects, like&nbsp;<a href="https://codilime.com/tungsten-fabric-architecture-an-overview/">Tungsten Fabric</a>, to provide their implementation of the CNI plugins and use them to manage networking in a&nbsp;<a href="https://codilime.com/glossary/kubernetes/">Kubernetes</a>&nbsp;cluster. In this blog post, you will learn how to use Tungsten Fabric as a Kubernetes CNI plugin to ensure network connectivity between containers and bare metals. You will also see an example of a nested deployment of a Kubernetes cluster into OpenStack VM with a TF CNI plugin.</strong></p>



<p>The CNI interface itself is very simple. The most important operations it has to implement are ADD and DEL. As the names suggest, ADD’s role is to add a container to the network and DEL’s is to delete it from the network. That’s all. But are these functions performed?&nbsp;</p>



<p>First things first: a kubelet is a Kubernetes daemon running on each node in a cluster. When the user creates a new pod, the Kubernetes API server orders a kubelet running on the node where the pod has been scheduled to create the pod. The kubelet will then create a network namespace for the pod, and allocate it by running the so-called “pause” container. One of the roles of this container is to maintain the network namespace which will be shared across all the containers in the pod. That’s why the containers inside the pod can “talk” to each other using the loopback interface. Then, for each container defined in the pod, the kubelet will call the CNI plugin.&nbsp;</p>



<p>But how does it know how to use each plugin? First, it looks for the CNI configuration file in a predefined directory ( /etc/cni/net.d&nbsp;<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni" target="_blank" rel="noreferrer noopener">by default</a>). When using Tungsten Fabric, the kubelet is going to find a file like this:</p>



<pre class="wp-block-code"><code>{
    "cniVersion": "0.3.1",
    "contrail" : {
        "cluster-name"  : "&lt;CLUSTER-NAME&gt;",
        "meta-plugin"   : "&lt;CNI-META-PLUGIN&gt;",
        "vrouter-ip"    : "&lt;VROUTER-IP&gt;",
        "vrouter-port"  : &lt;VROUTER-PORT&gt;,
        "config-dir"    : "/var/lib/contrail/ports/vm",
        "poll-timeout"  : &lt;POLL-TIMEOUT&gt;,
        "poll-retries"  : &lt;POLL-RETRIES&gt;,
        "log-file"      : "/var/log/contrail/cni/opencontrail.log",
        "log-level"     : "&lt;LOG-LEVEL&gt;"
    },
    "name": "contrail-k8s-cni",
    "type": "contrail-k8s-cni"
  }</code></pre>



<p>This file, among other parameters, specifies the name of the CNI plugin and IP (vrouter-ip) and port (vrouter-port) of the vRouter agent. By looking at this file, the kubelet knows it should use the CNI plugin binary called “contrail-k8s-cni”. It looks for it in a predefined directory ( /opt/cni/bin&nbsp;<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#cni" target="_blank" rel="noreferrer noopener">by default</a>) and, when it wants to create a new container, executes it with the command ADD passed through environment variables together with other parameters like: path to the pod’s network namespace, container id and container network interface name. The contrail-k8s-cni binary (you can find its source code&nbsp;<a href="https://github.com/tungstenfabric/tf-controller/tree/master/src/container/cni" target="_blank" rel="noreferrer noopener">here</a>) will read those parameters and send appropriate requests to the vRouter Agent.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/bb1cb15886dadd0311a7daf94482d72eab948af1/7fa29/img/codilime_tungsten-fabric-compute-with-kubernetes.png" alt="Tungsten Fabric compute with Kubernetes"/></figure>



<p><strong>Fig 1. Tungsten Fabric compute with Kubernetes&nbsp;<a href="https://d33wubrfki0l68.cloudfront.net/bb1cb15886dadd0311a7daf94482d72eab948af1/7fa29/img/codilime_tungsten-fabric-compute-with-kubernetes.png" target="_blank" rel="noreferrer noopener">(Enlarge)</a></strong></p>



<p>The vRouter Agent’s job is to create actual interfaces for the containers. But how does it know how to configure an interface? As you can see in the diagram above, it gets all this information from the Tungsten Fabric Control. So then how does the Tungsten Fabric Control know about all the pods, their namespaces, etc.? That’s where the Tungsten Fabric Kube Manager (you can find its source code&nbsp;<a href="https://github.com/tungstenfabric/tf-controller/tree/master/src/container/kube-manager" target="_blank" rel="noreferrer noopener">here</a>) comes in. It’s a separate service, launched together with other Tungsten Fabric SDN Controller components. It can be seen in the bottom left part of the diagram below.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/ae5f2aa3d9d32d1df4f649baa05ea40cd4f30dff/52ab7/img/codilime_tungsten-fabric-config-with-kubernetes.png" alt="Tungsten Fabric Config with Kubernetes"/></figure>



<p><strong>Fig 2. Tungsten Fabric Config with Kubernetes&nbsp;<a href="https://d33wubrfki0l68.cloudfront.net/ae5f2aa3d9d32d1df4f649baa05ea40cd4f30dff/52ab7/img/codilime_tungsten-fabric-config-with-kubernetes.png" target="_blank" rel="noreferrer noopener">(Enlarge)</a></strong></p>



<p>Kubemanager’s role is to listen for Kubernetes API server events like: pod creation, namespace creation, service creation, deletion. It listens for those events, processes them, and then creates, modifies or deletes appropriate objects in the Tungsten Fabric Config API. Tungsten Fabric Control will then find those objects and provide information about them to the vRouter agent. The vRouter Agent can then finally create the properly configured interface for the container. And that is how Tungsten Fabric can work as a Kubernetes CNI Plugin.</p>



<p>Because Tungsten Fabric and Kubernetes are integrated, container-based workloads can be combined with virtual machines or bare metal server workloads. Moreover, rules for connectivity between those environments can all be managed in one place.</p>



<h2 class="wp-block-heading">Tungsten Fabric nested deployment</h2>



<p>From the networking point of view, virtual machines and containers are almost the same thing for Tungsten Fabric, so deployments that combine them are possible. Moreover, in addition to Kubernetes, Tungsten Fabric can also be integrated with OpenStack. Thanks to that, the two platforms can be combined. Let’s say that we have an already deployed OpenStack with Tungsten Fabric, but we want to deploy some of our workloads using containers. With Tungsten Fabric we can create what is called a nested deployment—OpenStack compute virtual machines with a Kubernetes cluster deployed on them with Tungsten Fabric acting as the CNI plugin.&nbsp;</p>



<p>All of the Tungsten components need not be deployed as most of them are already running and controlling the OpenStack networking. However, on one of the nodes in the nested Kubernetes cluster, preferably the Kubernetes master node, we have to launch the Tungsten Fabric Kube Manager (described above). It will connect to the Kubernetes API Server in the nested cluster and to the Tungsten Fabric Config Api server deployed with OpenStack.&nbsp;</p>



<p>Finally, the Tungsten Fabric CNI plugin and its configuration file must be present on each of the nested Kubernetes compute nodes. Please note that neither the Tungsten Fabric vRouter nor vRouter Agent need to be deployed on the nested Kubernetes nodes, as those components are already running on the OpenStack compute nodes and the Tungsten Fabric CNI plugin can send requests directly to them.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/b306396c1d01a92fc84cd1c46ebfd75aa05bf728/6a0c8/img/codilime_fig3.png" alt="Kubernetes on OpenStack with Tungsten Fabric Networking"/></figure>



<p><strong>Fig 3. Kubernetes on OpenStack with Tungsten Fabric Networking&nbsp;<a href="https://d33wubrfki0l68.cloudfront.net/b306396c1d01a92fc84cd1c46ebfd75aa05bf728/6a0c8/img/codilime_fig3.png" target="_blank" rel="noreferrer noopener">(Enlarge)</a></strong></p>



<p>A nested deployment of a Kubernetes cluster integrated with Tungsten Fabric is an easy way to start deploying container-based workloads, especially for enterprises that have been using OpenStack to manage their virtual machines. Network admins can use their Tungsten Fabric expertise and need not necessarily master new tools and concepts.</p>



<h2 class="wp-block-heading">Summary</h2>



<p>As you can see, a Kubernetes CNI plugin allows you to benefit from one of Tungsten Fabric’s key features—its ability to connect different workloads regardless of their function— containers, VMs or bare metals. Should you need to use containers and ensure their connectivity with your legacy infrastructure based on OpenStack, you can create a nested deployment of the Kubernetes cluster integrated with TF</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tungsten Fabric architecture—an overview</title>
		<link>https://tungsten.io/tungsten-fabric-architecture-an-overview/</link>
		
		<dc:creator><![CDATA[tungstenfabric]]></dc:creator>
		<pubDate>Tue, 03 Aug 2021 00:14:33 +0000</pubDate>
				<category><![CDATA[Analytics]]></category>
		<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[OpenStack]]></category>
		<category><![CDATA[SDN]]></category>
		<category><![CDATA[Linux Foundation]]></category>
		<category><![CDATA[Open Source]]></category>
		<category><![CDATA[Tungsten Fabric]]></category>
		<guid isPermaLink="false">https://tungsten.io/?p=8350</guid>

					<description><![CDATA[This is a contributed blog from LF Networking Member CodiLime. Originally published here. SDN or Software-Defined Networking is an approach to networking that enables the programmatic and dynamic control of...]]></description>
										<content:encoded><![CDATA[
<p><strong><em>This is a contributed blog from LF Networking Member CodiLime. </em></strong><a href="https://codilime.com/blog/tungsten-fabric-architecture-an-overview/"><strong><em>Originally published here</em></strong>.</a></p>



<p><strong>SDN or Software-Defined Networking is an approach to networking that enables the programmatic and dynamic control of a network. It is considered the next step in the evolution of network architecture. To implement this approach effectively, you will need a mature SDN Controller such as Tungsten Fabric. Read our blog post to get a comprehensive overview of Tungsten Fabric architecture.</strong></p>



<h2 class="wp-block-heading">What is Tungsten Fabric</h2>



<p><a href="https://codilime.com/tungsten-fabric/">Tungsten Fabric</a>&nbsp;(previously OpenContrail) is an open-source&nbsp;<a href="https://codilime.com/glossary/sdn-controller/">SDN controller</a>&nbsp;that provides connectivity and security for virtual, containerized or bare-metal workloads. It is developed under the umbrella of&nbsp;<a href="https://tungsten.io/">the Linux Foundation</a>. Since most of its features are platform- or device agnostic, TF can connect mixed VM-container-legacy stacks. What Tungsten Fabric sees is only a source and target API. The technology stack that TF can connect includes:</p>



<ul>
<li>Orchestrators or virtualization platforms (e.g. OpenShift, Kubernetes, Mesos or VMware vSphere/Orchestrator)</li>



<li>OpenStack (via a monolithic plug-in or an ML2/3 network driver mechanism)</li>



<li>SmartNIC devices</li>



<li>SR-IOV clusters</li>



<li>Public clouds (multi-cloud or hybrid solutions)</li>



<li>Third-party proprietary solutions</li>
</ul>



<p>One of TF’s main strengths is its ability to connect both the physical and virtual worlds. In other words, to connect in one network different workloads regardless of their nature. They can be Virtual Machines, physical servers or containers.</p>



<p>To deploy Tungsten Fabric, you may need&nbsp;<a href="https://codilime.com/network-professional-services/">Professional Services (PS)</a>&nbsp;to integrate it with your existing infrastructure and ensure ease of use and security.</p>



<h2 class="wp-block-heading">Tungsten Fabric components</h2>



<p>The entire TF architecture can be divided into the&nbsp;<a href="https://codilime.com/glossary/control-plane/">control plane</a>&nbsp;and&nbsp;<a href="https://codilime.com/glossary/data-plane/">data plane</a>&nbsp;components. Control plane components include:</p>



<ul>
<li>Config—managing the entire platform</li>



<li>Control—sending rules for network traffic management to vRouter agents</li>



<li>Analytics—collecting data from other TF components (config, control, compute)</li>
</ul>



<p>Additionally, there are two optional components of the Config:</p>



<ul>
<li>Device Manager—managing underlay physical devices like switches or routers</li>



<li>Kube Manager—observing and reporting the status of Kubernetes cluster</li>
</ul>



<p>Data plane or compute components include:</p>



<ul>
<li>vRouter and its agents—managing packet flow at the virtual interface vhost0 according to the rule defined in the control component and received using vRouter agents</li>
</ul>



<h2 class="wp-block-heading">TF Config—the brain of the platform</h2>



<p>TF Config is the main part of the platform where network topologies are configured. It is the biggest TF component developed by the largest number of developers. In a nutshell, it is a database where all configurations are stored. All other TF components depend on the Config. The term itself has two meanings:</p>



<ul>
<li>VM where all containers are stored</li>



<li>A container named “config” where the entire business logic is stored</li>
</ul>



<p>TF Config has two APIs: North API (provided by Config itself) and South API (provided by other control plane components). The first one is more important here because it is the API used for communication. The South API is used by Device Manager (also a part of TF and discussed later) and other tools.</p>



<p>TF Config uses an intent-based approach. The network administrator does not need to define all conditions but only how the network is expected to work. Other elements are configured automatically. For example, you want to enable network traffic from one network to another. It is enough to define this intention, and all the magic is done under the hood.</p>



<p>The schema transformer listens to the database to check if there is a new entry. When such an entry is added, it checks for lacking data and completes it using the Northbound API. In this way, network routings are created, a firewall is unblocked to enable the traffic to flow between these two networks, and the devices obtain all the data necessary to get the network up and running.&nbsp;</p>



<p>An intent-based approach automates network creation. There are many settings that need to be defined when creating a new network, and it takes time to set up all of them. As a process, it is also error-prone. Using TF simplifies everything, as most settings are default ones and are completed automatically.</p>



<p>When it comes to communication with Config, its API is shared via http. Alternatively, you can use a TF UI or cURL, a command line tool for file transfer with a URL syntax supporting a number of protocols including HTTP, HTTPS, FTP, etc. There is also a TF CLI tool.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/2e290badcc110dcce6a0552dbc336d7aff19ec7a/0799e/img/codilime_tungsten-fabric-config-with-openstack.png" alt="Tungsten Fabric Config with OpenStack" title="Fig 1. Tungsten Fabric Config with OpenStack"/></figure>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/ae5f2aa3d9d32d1df4f649baa05ea40cd4f30dff/7e06b/img/codilime_tungsten_fabric_config_with_kubernetes.png" alt="Tungsten Fabric Config with Kubernetes" title="Fig 2. Tungsten Fabric Config with Kubernetes"/></figure>



<p></p>



<h2 class="wp-block-heading">Managing physical devices with Device Manager</h2>



<p>Device Manager is an optional component with two major functions. Both are related to fabric management, which is the management of underlay physical devices like switches or routers.</p>



<p>First, it is responsible for listening to configuration events from the Config API Server and then for pushing required configuration changes to physical devices. Virtual Networks, Logical Routers and other overlay objects can be extended to physical routers and switches. Device Manager enables homogeneous configuration management of overlay networking across compute hosts and hardware devices. In other words, bare-metal servers connected to physical switches or routers may be a part of the same Virtual Network as virtual machines or containers running on compute hosts.</p>



<p>Secondly, this component manages the life cycle of physical devices. It supports the following features:</p>



<ul>
<li>onboarding fabric—detect and import brownfield devices</li>



<li>zero-touch provisioning—detect, import and configure greenfield devices</li>



<li>software image upgrade—individual or bulk upgrade of device software</li>
</ul>



<p>Today only Juniper’s MX routers and QFX switches have&nbsp;<a href="https://github.com/tungstenfabric/tf-controller/tree/master/src/config/device-manager/device_manager/plugins/juniper/">an open-source plug-in</a>.</p>



<h2 class="wp-block-heading">Device Manager: under the hood</h2>



<p>Device Manager reports job progress by sending UVEs (User Visible Entities) to the Collector. Users can retrieve job status and logs using the Analytics API and it’s Query Engine. Device Manager works in full or partial mode. There can be only one active instance in the full mode. In this mode, it is responsible for processing events sent via RABBITMQ. It evaluates high-level intents like Virtual Networks or Logical Routers and translates them into a low-level configuration that can be pushed into physical devices. It also schedules jobs on the message queue that can be consumed by other instances running in partial mode. Those followers listen for new job requests and execute ansible scripts, which&nbsp; push the desired configuration to devices.</p>



<p>Device Manager has the following components:</p>



<ul>
<li>device-manager—translates high-level intents into a low-level configuration</li>



<li>device-job-manager—executes ansible playbooks, which configure routers and switches</li>



<li>DHCP server—in a zero-touch provisioning use case, physical device gets management IP address from a local DHCP server running alongside device-manager</li>



<li>TFTP server—in the zero-touch provision use case, this server is used to provide a script with the initial configuration</li>
</ul>



<h2 class="wp-block-heading">Kube Manager</h2>



<p>Kube Manager is an additional component launched together with other Tungsten Fabric SDN Controller components. It is used to establish communication between Tungsten Fabric and Kubernetes, and is essential to their integration. In a nutshell, it listens to the Kubernetes API server events such as creation, modification or deletion of k8s objects (pods, namespaces or services). When such an event occurs, Kube Manager processes it and creates, modifies or deletes an appropriate object in the Tungsten Fabric Config API. Tungsten Fabric Control will then find those objects and send information about them along to the vRouter agent. After that, the vRouter agent can finally create the correctly configured interface for the container.&nbsp;</p>



<p>The following example should clarify this process. Let’s say that an annotation is added to the namespace in Kubernetes, saying that the network in this namespace should be isolated from the rest of the network. Kube Manager gets the information about it and changes the setup of the TF object accordingly.</p>



<h2 class="wp-block-heading">Control</h2>



<p>The Control component is responsible for sending network traffic configurations to vRouter agents. Such configurations are received from the Config’s Cassandra database, which offers consistency, high availability and easy scalability. To represent the configuration and operational state of the environment, the IF-MAP (The Interface to Metadata Access Point) protocol is used. The control nodes exchange routes with one another using IBGP protocol to ensure that all control nodes have the same network state. Communication between Control and vRouter agents is done via Extensible Messaging and the Presence Protocol (XMPP)—a communications protocol for message-oriented middleware based on XML. Finally, the Control communicates with gateway nodes (routers and switches) using the BGP protocol.</p>



<p>TF Control works similarly to a hardware router. Control is a control plane component responsible for steering the data plane and sending the traffic flow configuration to vRouter agents. For their part, hardware routers are responsible for handling traffic according to the instructions they receive from the control plane. In TF architecture, physical routers and their agent services work alongside vRouters and vRouter agents, as Tungsten Fabric can handle both physical and virtual worlds.</p>



<p>TF Control communicates with a vRouter using XMPP, which is equivalent to a standard BGP session, though XMPP carries more information (e.g. configurations). Still, thanks to its reliance on XMPP, TF Control can send network traffic configurations to both vRouters and physical ones—the code used for communication is exactly the same.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/78293605fc2e819777b61ebc74e950624e0ebc2b/46b16/img/codilime_tungsten_fabric_control.png" alt="Tungsten Fabric Control" title="Fig. 3 Tungsten Fabric Control"/></figure>



<p></p>



<h2 class="wp-block-heading">Analytics</h2>



<p>Analytics is a separate TF component that collects data from other components (config, control, compute). The following data are collected:</p>



<ul>
<li>Object logs (concrete objects in the TF structure)</li>



<li>System logs</li>



<li>Trace buffers</li>



<li>Flow statistics in TF modules</li>



<li>Status of TF modules (i.e. if they are working and what their state is)</li>



<li>Debugging data (if a required data collection level is enabled in the debugging mode)</li>
</ul>



<p>Analytics is an additional component of Tungsten Fabric. TF works fine without it using just its main components. It can even be enabled as an additional plugin long after the TF solution was originally deployed.</p>



<p>To collect the data coming from other TF components, an original Juniper protocol called Sandesh is used. The name comes from&nbsp;<a href="http://sandesh.com/">an Indian newspaper in Gujarati language</a>. “Sandesh” means “message” or “news”. Analogically, the protocol is the messenger that brings news about the SDN.</p>



<p>In the Analytics component, there are two databases. One is based on the Cassandra database and contains historical data: statistics, logs, TF data flow information. It is commonly used for Analytics and Config components. Cassandra is the database that allows you to write data quickly, but it reads data more slowly. It is therefore used to write and store historical data. If there is a need to analyze how TF deployment worked over a longer period of time, this data can be read. In practice, such a need does not occur very often. This feature is most often used by developers to debug a problem.</p>



<p>The second database is based on the Redis database and collects UVE (User Visible Entities) such as information about existing virtual networks, vRouters, virtual machines and about their actual state (whether it’s working or not). These are the components of the system infrastructure defined by users (in contrast to the elements created automatically under the hood by TF). Since the data about their state are dynamic, they are stored in the Redis database, which allows users to read them much more quickly than in the Cassandra database.&nbsp;</p>



<p>All these TF components send data to the Collector, which writes them in either the Cassandra or Redis database. On the other side, there is an API Server which is sometimes called the Analytics API to distinguish it from the API Server, e.g. in the Config. This Analytics API provides a REST API for extracting data from the database.</p>



<p>Apart from these, Analytics has one additional component, called QueryEngine. This is an indirect process taking a user query for historical data. The user sends an SQL-like query to the Analytics API (API Server) REST port. Then the query is sent to QueryEngine, which performs a database query in Cassandra and, via the Analytics API, sends the result back to the user.</p>



<p>&nbsp;Figure 4 shows the Analytics Node Manager and Analytics Database Node Manager. In fact, there are many different node managers in the TF architecture that are used to monitor specific parts of the architecture and send reports about them. In our case, Analytics Node Manager monitors Collector, QueryEngine and API Server, while the Analytics Database Node Manager monitors databases in the Analytics component. In this way, Analytics also collects data on itself.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/ed127b73f4599b0f2fb94db2feca0d26b6878792/b3bb1/img/codilime_tungsten_fabric_analytics.png" alt="Tungsten Fabric Analytics" title="Fig. 4 Tungsten Fabric Analytics"/></figure>



<p></p>



<h2 class="wp-block-heading">The VRouter forwarder and agent</h2>



<p>This component is installed on all compute hosts that run the workload. It provides Integrated routing and bridging functions for network traffic from and between Virtual Machines, Containers and external networks. It applies network and security rules defined by the Tungsten Fabric controller. This component is not mandatory, but it is required for any use case with virtualized workloads.&nbsp;</p>



<ul>
<li>Agent</li>
</ul>



<p>The agent is a user-space application that maintains XMPP sessions with the Tungsten Fabric controllers. It is used to get VRF (Virtual Routing and Forwarding) and ACLs (Access Control Lists) that are derived from high-level intents like Virtual Networks. The agent maintains a local database of VRFs and ACLs. This component reports its state to the Analytics API by sending Sandesh messages with UVEs (User Visible Entities) with logs and statistics. It is responsible for maintaining the correct forwarding state in Forwarder. The agent also handles some protocols like DHCP, DNS or ARP.</p>



<p>Communication with the forwarder is achieved with the help of a KSync module, which uses Netlink sockets and shared memory between the agent and the forwarder. In some cases, application and kernel modules also use the pkt0 tap interface to exchange packets. Those mechanisms are used to update the flow table with flow entries based on the agent’s local data.</p>



<ul>
<li>Forwarder</li>
</ul>



<p>The forwarder performs packet processing based on flows pushed by the agent. It may drop the packet, forward it to the local virtual machine, or encapsulate it and send it to another destination.</p>



<p>The forwarder is usually deployed as a kernel module. In that case, it is a software solution independent of NIC or server type. Packet processing in kernel space is more efficient than in user-space and provides some room for optimization. The drawback is that it can only be installed with a specific supported kernel version. For advanced users, modules for a different kernel version can be built. Default kernel versions are specified&nbsp;<a href="https://github.com/tungstenfabric/tf-packages">here</a>.</p>



<p>This kernel module is released as a docker image that contains a pre-built module and user-space tools. When this image is run, it copies binaries to the host system and installs the kernel module on the host (it needs to be run in privileged mode). After successful installation, a vrouter module should be loaded into the kernel (“lsmod | grep vrouter”) and new tap interfaces pkt0 and vhost0 created. If problems occur, checking the kernel logs (“dmesg”) can help you arrive at a solution.</p>



<p>The forwarder can also be installed as a userspace application that uses The Data Plane Development Kit (DPDK), which enables higher performance than the kernel module.</p>



<ul>
<li>Packet flow</li>
</ul>



<p>For every incoming packet from a VM, vRouter forwarder needs to decide how to process it. The options are DROP, FORWARD, MIRROR, NAT or HOLD. Information about what to do is stored in flow table entries. The forwarder is using packet headers to find a corresponding entry in the above-mentioned tables. With the first packet from a new flow, the entry might be empty. In that case, the vRouter forwarder sends this packet to the pkt0 interface, where the agent is listening. Using its local information about VRFs and ACLs, the agent pushes (using KSync and shared memory) a new flow to the forwarder and resends a packet. In other words, the vRouter forwarder doesn’t have full knowledge of how to process every packet in the system so it cooperates with the agent to get that knowledge. It is because this process may take some time that the first packet sent through the vRouter may come with a visible delay.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/ce7df33860634d6884826a143b59fff25354c849/9bb3f/img/codilime_tungsten-fabric-compute-with-openstack.png" alt="Tungsten Fabric Compute with OpenStack" title="Fig. 5 Tungsten Fabric Compute with OpenStack"/></figure>



<p></p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/bb1cb15886dadd0311a7daf94482d72eab948af1/f69c8/img/codilime_tungsten_fabric_compute_with_kubernetes.png" alt="Tungsten Fabric Compute with Kubernetes" title="Fig. 6 Tungsten Fabric Compute with Kubernetes"/></figure>



<p></p>



<h2 class="wp-block-heading">Tungsten Fabric with OpenStack and Kubernetes—an overview</h2>



<p>To sum up, Figures 7 and 8 provide an overview of the TF integration with Openstack and Kubernetes, respectively.</p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/66268e490e805fb49cfee8bea9e0da362f9bdd17/31273/img/codilime_tungsten-fabric-with-openstack.png" alt="Tungsten Fabric with Openstack" title="Fig. 7 Tungsten Fabric with Openstack"/></figure>



<p></p>



<figure class="wp-block-image"><img decoding="async" src="https://d33wubrfki0l68.cloudfront.net/b2f1d82056fa087b400a34859992f4e7f5fc36ff/1af44/img/codilime_tungsten_fabric_with_kubernetes.png" alt="Tungsten Fabric with Kubernetes" title="Fig. 8 Tungsten Fabric with Kubernetes"/></figure>



<p></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Tungsten Fabric as SDN for Akraino Based Network Edges</title>
		<link>https://tungsten.io/tungsten-fabric-as-sdn-for-akraino-based-network-edges/</link>
		
		<dc:creator><![CDATA[tungstenfabric]]></dc:creator>
		<pubDate>Mon, 04 Mar 2019 03:55:04 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[NFV]]></category>
		<category><![CDATA[SDN]]></category>
		<category><![CDATA[Uncategorized]]></category>
		<guid isPermaLink="false">https://tungsten.io/?p=8100</guid>

					<description><![CDATA[Originally published on CalSoft Posted on&#160;February 28, 2019&#160;by&#160;Sagar Nangare SDN is a crucial technology in a roadmap for a dynamic and intelligent network, be it enterprise level connecting several devices...]]></description>
										<content:encoded><![CDATA[
<p><a href="https://blog.calsoftinc.com/2019/02/tungsten-fabric-sdn-akraino-based-network-edges.html" target="_blank" rel="noreferrer noopener" aria-label="Originally published on CalSoft (opens in a new tab)">Originally published on CalSoft</a></p>



<p>Posted on&nbsp;<a href="https://blog.calsoftinc.com/2019/02/tungsten-fabric-sdn-akraino-based-network-edges.html">February 28, 2019</a>&nbsp;by&nbsp;<a href="https://blog.calsoftinc.com/author/sagar-nangare">Sagar Nangare</a></p>



<p>SDN is a crucial technology in a roadmap for a dynamic and intelligent network, be it enterprise level connecting several devices on-premises or branches across the wide area (SD-WAN). With the power of SDN technology, telecom operators are taking it to achieve central control over the network and compute nodes.</p>



<p>As the 5G going mainstream disruption for telecom as well as technology business players, there is a growing need to handle data flows within the network in real time along with smartly minimizing bandwidth usage plus latency. The emergence of SDN and NFV technologies had majorly set up a foundation to build a network having expected requirements for end users along with a dynamic and central control for service providers or enterprises.</p>



<p>In this article, let us discuss about SDN stack,&nbsp;<a href="https://tungsten.io/start/" target="_blank" rel="noreferrer noopener">Tungsten Fabric</a>&nbsp;and how it can be used for 5G-network edge cloud that is based on&nbsp;<a href="https://www.lfedge.org/projects/akraino/">Akraino Edge Stack</a>.</p>



<p><a href="https://tungsten.io/">Tungsten Fabric</a>&nbsp;is an open source SDN initiative from Juniper and merged into Linux Foundation as a community project. TF provides a single point of control, visibility, and management for networking &amp; security for different types of data center deployments or clouds. It has taken SDN technology to next level by</p>



<ul><li>Providing consistent network functionality and enforcing security policies for different types of workloads (virtual machines, containers, bare metal) orchestrated with different available orchestrators (OpenStack, Kubernetes, VMware , etc)</li><li>Providing production grade networking &amp; security stack for Data Center and Public Clouds (AWS, Azure, GCP) &amp; Edge cloud deployments</li></ul>



<p>Tungsten Fabric evolved as a network software stack for providing an SDN solution for Telco Cloud and NFV use cases<del>.</del></p>



<p>To understand the application of TF for telco cloud, let us discuss about the PoC&nbsp;<a href="https://wiki.akraino.org/display/AK/Akraino+Network+Cloud+and+TF+Integration">proposed</a>&nbsp;by Juniper’s Sukhdev Kapur to Linux Foundation’s Akraino community (an open source software stack for network edges). This proof of concept is approved by the Akraino.</p>



<p><strong>Tungsten Fabric Integration with Akraino based Network Edge Cloud</strong></p>



<p>TF, by integrating with Akraino Edge Stack, can act as unified SDN controller to enhance many features for 5G core and edge nodes, including</p>



<ul><li>Enabling distributed edge computing using TF remote compute architecture,</li><li>A common SDN controller for different workloads in network cloud i.e. CNF, VNF, PNFs</li><li>Service chaining at different types of edge sites or clouds (public or private)</li><li>Common security policy enforcement for all nodes</li><li>Advanced networking performance features: SR/IOV, DPDK, BGP-VPN, IPSec/TLS Support, etc</li></ul>



<figure class="wp-block-image"><img decoding="async" src="https://blog.calsoftinc.com/wp-content/uploads/2019/02/Akraino-Network-Cloud-TF-Integration-Blueprint.png" alt="" class="wp-image-4810" /><figcaption>Figure – Akraino Network Cloud &amp; TF Integration (Blueprint)</figcaption></figure>



<p><em>Image source:&nbsp;<a href="https://wiki.akraino.org/display/AK/Network+Cloud+Family+-+Reference+Architecture">Akraino Reference architecture</a></em></p>



<p>You can see from above image, like other open source projects, TF place at edge platform software component, enhancing with new feature set to act as unified SDN controller for any type of workload &amp; compute orchestration.</p>



<p><strong>Deployment</strong></p>



<p>Tungsten fabric is composed of components like controller and vRouter; plus additional components for analytics and third party integration. In this PoC, TF integrates with Kubernetes (CNI) and OpenStack (Neutron) as SDN plugin to enable rich networking capabilities and lifecycle management of VMs and containers where TF components or control functions deployed.</p>



<p>The configuration declared at the central data center is enforced on edge nodes to set up consistent network and security policies. The deployment and life cycle management of Tungsten Fabric can be done with tools like Ansible or Helm. These configuration files are termed as playbooks if Ansible is used or charts in case of Helm. These tools provides benefits of automation and management of components, further reducing operational costs for edge deployments.</p>



<p>Tungsten fabric along with Helm offers a seamless solution where TF services are deployed in containers using a microservices architecture to enable advantages like self-healing, updates, CI/CD, etc. Helm uses Kubernetes to declare charts for subsequent microservices, allowing greater automation in managing TF services. In this case, Kubernetes become a single orchestrator to manage lifecycle of all control operations. Such integrated solution evolved as Tungsten Fabric Helm (TF Helm).</p>



<p>OpenStacks’s Airship (Armada) is an umbrella project with which TF integrates for installation using helm charts and set up interaction with edge nodes using CNI and Neutron.</p>



<p>The basic idea behind this PoC is to define an architecture for a distributed Edge Cloud keeping operational and deployment cost low. Another objective is to build a network where failure of any edge node application should not hamper availability and functionality of edge network to avoid traffic loss. To implement such architecture, a solution proposed in this PoC exercise utilizes the same TF based on a single SDN cluster that spans across all the edge nodes. A central SDN cluster located at main data center will have TF installed with Kubernetes and OpenStack orchestrators along with TF control components. Dedicated control functions, which handles compute and networking operations for all edge nodes, are located at the central data center, and connected to vRouter (TF component) set at the edge nodes using set of gateways. The dedicated control function is logically present at the Primary POP to control vRouters of the edge nodes located on the remote POPS. MP-BGP protocol is used between SDN Gateways and control functions, and XMPP is used for communication between vRouters and dedicated control functions.</p>



<figure class="wp-block-image"><img decoding="async" src="https://blog.calsoftinc.com/wp-content/uploads/2019/02/Interconnection-between-component-of-data-center-with-edge-POPs.png" alt="" class="wp-image-4811" /><figcaption>Figure – Interconnection between component of data center with edge POPs</figcaption></figure>



<p>To have an end-to-end data transmission, an overlay network is established between edge nodes and central data centers in which MPLS over IP (MPLSoXoIP) is used. Communication between gateways can optionally use IPsec encryption to protect network data.</p>



<figure class="wp-block-image"><img decoding="async" src="https://blog.calsoftinc.com/wp-content/uploads/2019/02/2.png" alt="" class="wp-image-4812" /><figcaption>Figure – Secure data and network with overlay network</figcaption></figure>



<p><strong>Summary</strong></p>



<p>TF has emerged as a leading SDN solution with every release. Deployment of TF using helm charts and orchestration of every type of workloads from a diverse set of clouds has increased the potential of TF for Telco use case. Akraino Edge Stack is pre-integrated with a set of projects, which promotes various orchestrations and performance benefits. Integration of TF with Akraino edge stack enable enhanced features and utilizes remote compute architecture of TF. A solution can orchestrate all types of workloads like PNFs, VNFs and CNF, implement service chaining at edge sites, workload and data transfer security, automating deployment of control functions and workloads, and more.</p>



<p><strong><em>Republished with Permission from Republished with permission from </em></strong><a rel="noreferrer noopener" href="https://blog.calsoftinc.com/2019/02/tungsten-fabric-sdn-akraino-based-network-edges.html" target="_blank"><strong><em>CalSoft.</em></strong></a></p>



<p><strong><em>About the author</em></strong></p>



<p><em>Sagar Nangare is technology blogger, currently serving&nbsp;<a href="https://urldefense.proofpoint.com/v2/url?u=https-3A__calsoftinc.com_&amp;d=DwMFaQ&amp;c=HAkYuh63rsuhr6Scbfh0UjBXeMK-ndb3voDTXcWzoCI&amp;r=Y9QaEJ2cs4La8kQDqQ-N2rBJnxrPyFqAIO8efLhSqZ0&amp;m=8ToqEPYme7wCk2CZy01mmL8Y5T9FiF4E0mhEwhaU27c&amp;s=KqnJF-1Qbjb1_jI40uohUwGCiT8mFL0WZo-StdvVmVA&amp;e=" target="_blank" rel="noreferrer noopener">Calsoft Inc.</a>&nbsp;as a digital strategist.</em></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>In the News: Carbide Quick Start, New Version, LFN</title>
		<link>https://tungsten.io/in-the-news-carbide-quick-start-new-version-lfn/</link>
		
		<dc:creator><![CDATA[Robert Cathey]]></dc:creator>
		<pubDate>Fri, 05 Oct 2018 20:43:43 +0000</pubDate>
				<category><![CDATA[Community]]></category>
		<category><![CDATA[Features]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<guid isPermaLink="false">http://tf.lfprojects.linuxfoundation.org/?p=8069</guid>

					<description><![CDATA[Help us share recent project news via your social networks: completed the move to the LFN released a new version launched a quick start environment on AWS to help enterprise...]]></description>
										<content:encoded><![CDATA[<p>Help us share recent project news via your social networks:</p>
<ul>
<li>completed the move to the LFN</li>
<li>released a new version</li>
<li>launched a quick start environment on AWS to help enterprise Kubernetes devs get going fast with Tungsten Fabric</li>
</ul>
<p>&nbsp;</p>
<p><strong>Tungsten Fabric Completes Move to the LF Networking and Releases New Version</strong></p>
<p>&lt;via the Linux Foundation&gt; <a href="https://www.linuxfoundation.org/press-release/2018/09/tungsten-fabric-completes-move-to-the-lf-networking-and-releases-new-version/">https://www.linuxfoundation.org/press-release/2018/09/tungsten-fabric-completes-move-to-the-lf-networking-and-releases-new-version/</a></p>
<p>&nbsp;</p>
<p><strong>Tungsten Fabric Launches Carbide Quick Start Environment on AWS</strong></p>
<p>&lt;via the Linux Foundation&gt; <a href="https://www.linuxfoundation.org/press-release/2018/09/tungsten-fabric-launches-carbide-quick-start-environment-on-aws/">https://www.linuxfoundation.org/press-release/2018/09/tungsten-fabric-launches-carbide-quick-start-environment-on-aws/</a></p>
<p>&nbsp;</p>
<p><strong>Tungsten Fabric project joins LF Networking community</strong></p>
<p>&lt;via Datacenter Dynamics&gt; <a href="https://www.datacenterdynamics.com/news/tungsten-fabric-becomes-part-lf-networking/">https://www.datacenterdynamics.com/news/tungsten-fabric-becomes-part-lf-networking/</a></p>
<div id="jp-relatedposts" class="jp-relatedposts"></div>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Calls for Papers: Tell Your Story at Two Upcoming Tungsten Fabric Events</title>
		<link>https://tungsten.io/calls-for-papers-tell-your-story-at-two-upcoming-tungsten-fabric-events/</link>
		
		<dc:creator><![CDATA[Robert Cathey]]></dc:creator>
		<pubDate>Mon, 01 Oct 2018 20:41:53 +0000</pubDate>
				<category><![CDATA[Community]]></category>
		<category><![CDATA[Conference]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[SDN]]></category>
		<category><![CDATA[Use Case]]></category>
		<guid isPermaLink="false">http://tf.lfprojects.linuxfoundation.org/?p=8067</guid>

					<description><![CDATA[Users and developers in the Tungsten Fabric community are invited to participate in the calls for papers at two upcoming events: Tungsten Fabric Workshop at KubeCon China (Nov. 13 in...]]></description>
										<content:encoded><![CDATA[<p>Users and developers in the Tungsten Fabric community are invited to participate in the calls for papers at two upcoming events:</p>
<ol>
<li><b>Tungsten Fabric Workshop at KubeCon China (Nov. 13 in Shanghai)</b></li>
<li><b>Tungsten Fabric Developers Summit at KubeCon North America (Dec. 10 in Seattle)</b></li>
</ol>
<p>These full-day sessions are official Co-Located Events of KubeCon.</p>
<p>Open source software is shaping how organizations compete, and SDN networking technology is evolving to support that revolution. However, building an SDN/NFVi platform that supports different open source infrastructure and app development technologies is not easy. Tungsten Fabric is an <a href="https://www.linuxfoundation.org/projects/networking/">LFN-hosted</a> open source SDN with a proven track record at scale in the demanding world of production carrier deployments.</p>
<p>Innovation happens when the community combines its talents, so come and interact with the leading minds building this exciting project! Join us to learn about the project, share your experiences, and talk about how the community is preparing for some exciting edge computing use cases.</p>
<p>We are looking for technology presentations and short (“lightning”) talks on topics related to Tungsten Fabric and open source multi-cloud networking at scale. We expect long talks to last 30 minutes with an additional 5 minutes for questions, and short talks to last 15 minutes. Please specify the talk time when you submitted the proposal. The best presentation will receive a special gift.</p>
<p>Topics that may be considered, among others, include:</p>
<ul>
<li>Use cases for enterprises, carriers, and cloud networking</li>
<li>Features of Tungsten 5.0 and future release (including proposals)</li>
<li>Multi-cloud use case, challenges, progress; also the experience of Carbide Quick Start Environment (TF) on AWS</li>
<li>Deploying Tungsten Fabric; testing and scaling Tungsten Fabric.</li>
<li>Using Tungsten Fabric: load balancing, HA, service chaining, analytics and orchestration, private clouds using Kubernetes, OpenStack and/or VMware, and hybrid cloud with AWS, Azure or Google</li>
<li>Integrating Tungsten Fabric with other open source technologies</li>
<li>Troubleshooting and debugging Tungsten Fabric installations</li>
<li>Performance measurements or approaches to improving performance</li>
<li>Increasing the size and diversity of the Tungsten Fabric user and developer base</li>
<li>Tutorials and demo videos</li>
<li>Network virtualization innovative Ideas</li>
<li>Container networking challenges and learnings</li>
<li>Linux Foundation Networking project status &amp; proposal</li>
<li>Data plane networking innovation to Tungsten Fabric</li>
<li>Edge computing challenges and service innovation</li>
<li>Multi-vendor switch/router support</li>
<li>BGP-EVPN L2/L3 integration and use cases</li>
</ul>
<p>Talks will be recorded and made available online.</p>
<p><b>How to Propose a Talk</b></p>
<p>You may propose a talk as a full talk, a lightning talk, or for either one at the committee’s discretion. We will also accept proposals for panel discussions. Please submit panel discussions as full talks, and make it clear in the description that it is a panel. Please submit proposals and questions to <a href="mailto:Conferences@lists.tungsten.io">Conferences@lists.tungsten.io</a>, and include:</p>
<ul>
<li>Title and abstract</li>
<li>Names, email address, and affiliation for each speaker</li>
<li>Whether you are proposing a full talk or a lightning talk or either one at the committee’s discretion; please specific time required</li>
<li>Please specify if the session includes a live demo</li>
</ul>
<p><b>Special Notes for KubeCon China</b></p>
<p>Deadline for submissions is 11:59 pm PDT, <b>October 12</b>. We will notify speakers of acceptance by <b>Oct 19. </b>Once accepted, the event committee will collaborate with the speakers on the content review from <b>Oct 22 to Nov 9.  </b>The event committee will request all speakers to use the community slide template.</p>
<p>Speakers should plan to attend the event in person. Travel to and accommodations in China and registration for KubeCon ($300 fee) are the responsibility of attendees. Registration link is added<a href="https://www.bagevent.com/event/ticket/1419821?bag_track%3Dundefined%26_ga%3D2.263650828.1674888792.1534696869-1258576529.1534174264&amp;sa=D&amp;source=hangouts&amp;ust=1536763796113000&amp;usg=AFQjCNFzW2PlEmPpCoO1vmEm73I9NWGj3Q&amp;bag_track=undefined&amp;bag_track=undefined&amp;_ga=2.210507797.1459331173.1537294480-712675146.1535979664"> here</a>. After registering for the conference, you will have the opportunity to select and RSVP for the Tungsten Fabric Workshop.</p>
<p>The workshop will feature the announcement of the first Tungsten Fabric lab in China, the first Linux Foundation Certified Lab for Network Technology Innovation. Another highlight of the workshop will be a drawing for a new Apple iWatch, provided through the generosity of SDNLAB (www.sdnlab.com).</p>
<p><b>Special Notes for KubeCon North America</b></p>
<p>Deadline for submissions is 11:59 pm PDT, <strong>October 31</strong>. We will notify speakers of acceptance by <b>November 9. </b>Once accepted, the event committee will collaborate with the speakers on the content review from <b>November 9 to December 7.  </b>The event committee will request all speakers to use the community slide template.</p>
<p>Speakers should plan to attend the event in person. Travel to and accommodations in Seattle, Washington, and registration for KubeCon are the responsibility of attendees.</p>
<p>The Tungsten Fabric Developers Summit is free of charge. However, everyone attending a co-located event must have a ticket to KubeCon + CloudNativeCon North America 2018 (cost is estimated at $1150). This provides access to the entire KubeCon NA conference, and we encourage you to enjoy all that the conference has to offer. Register <a href="https://www.regonline.com/registration/Checkin.aspx?EventID=2246960&amp;_ga=2.176207264.467216612.1538146480-2070928008.1535589349">here</a>. After registering for the conference, you will have the opportunity to select and RSVP for the Tungsten Fabric Developers Summit.</p>
<p>Join us in Shanghai and/or Seattle to tell your Tungsten Fabric story and engage with the community to shape the future of open, scalable, multi-cloud networking.</p>
<div id="jp-relatedposts" class="jp-relatedposts"></div>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Deployments and management made easy with Openstack &#038; Opencontrail Helm</title>
		<link>https://tungsten.io/deployments-and-management-made-easy-with-openstack-opencontrail-helm/</link>
		
		<dc:creator><![CDATA[Ranjini Rajendran]]></dc:creator>
		<pubDate>Mon, 06 Nov 2017 22:52:17 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[OpenStack]]></category>
		<category><![CDATA[Orchestration]]></category>
		<category><![CDATA[SDN]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7625</guid>

					<description><![CDATA[Note: This blog is co&#8211;authored by Ranjini Rajendran and Madhukar Nayakbomman from Juniper Networks. OpenStack provides modular architecture to enable IaaS for primarily managing virtualized workloads. However, this open source platform is a...]]></description>
										<content:encoded><![CDATA[<p>Note: This blog is <em>co</em>&#8211;<em>authored</em> by Ranjini Rajendran and Madhukar Nayakbomman from Juniper Networks.</p>
<p>OpenStack provides modular architecture to enable IaaS for primarily managing virtualized workloads. However, this open source platform is a complex piece of architecture, one that provides organizations with a tall order problem for dealing with configuration and management of  applications. One of the biggest challenge or pain point that Cloud administrators experience is around life cycle management of Openstack enabled Cloud environments.</p>
<p>To simplify the life cycle management of Openstack components, Openstack-helm project was started during the Barcelona Openstack Summit in 2016. In October 2017, Openstack-helm became an official Openstack Project.</p>
<p>&nbsp;</p>
<h3>What is Helm</h3>
<p>Well, think of it as the apt-get / yum of Kubernetes, it is a package manager for Kubernetes. If you deploy applications to Kubernetes, Helm makes it incredibly easy to</p>
<ul>
<li>version deployments</li>
<li>package deployments</li>
<li>make a release of it</li>
<li>and deploy, delete, upgrade and</li>
<li>even rollback those deployments</li>
</ul>
<p>as “charts”.</p>
<p>“Charts” being the terminology that Helm uses for a package of configured Kubernetes resources.</p>
<h3>What is Openstack-Helm ?</h3>
<p>Openstack-Helm project enables deployment, maintenance and upgrades of loosely coupled Openstack services and its dependencies as kubernetes pods. The different components of Openstack like glance, keystone, nova, neutron, heat etc are deployed as kubernetes pods. More details about the openstack-helm charts can be found at:</p>
<p><a href="https://github.com/openstack/openstack-helm">https://github.com/openstack/openstack-helm</a></p>
<h3>OpenContrail Helm charts</h3>
<p>Recently, OpenContrail components have been containerized. There are mainly three containers in Opencontrail &#8211;</p>
<ul>
<li>OpenContrail Controller (config and control nodes)</li>
<li>OpenContrail Analytics (Analytics node)</li>
<li>OpenContrail Analytics DB (Analytics Database node)</li>
</ul>
<p>There is now support for  OpenContrail Helm charts for deployment, maintenance, and upgrade of these  Opencontrail Container pods. The details on OpenContrail Helm charts can be found here:</p>
<p><a href="https://github.com/Juniper/contrail-docker/tree/master/kubernetes/helm/contrail">https://github.com/Juniper/contrail-docker/tree/master/kubernetes/helm/contrail</a></p>
<p>The video below shows the integration of OpenContrail helm charts with Openstack-helm and how easy it is to upgrade OpenContrail using helm charts with minimal downtime for existing tenant workloads.</p>
<p><iframe src="https://www.youtube.com/embed/nDZvJEkkt2U" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>&nbsp;</p>
<p>You can also see this at Openstack Sydney summit session on Tuesday 7<sup>th</sup> November at 5:50 pm. <a href="https://www.openstack.org/summit/sydney-2017/summit-schedule/events/19938">https://www.openstack.org/summit/sydney-2017/summit-schedule/events/19938</a></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>OpenContrail Containers Now on DockerHub</title>
		<link>https://tungsten.io/opencontrail-containers-now-on-dockerhub/</link>
		
		<dc:creator><![CDATA[James Kelly]]></dc:creator>
		<pubDate>Mon, 11 Sep 2017 07:01:13 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Docker]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[OpenShift]]></category>
		<category><![CDATA[OpenStack]]></category>
		<category><![CDATA[4.0]]></category>
		<category><![CDATA[4.0.1]]></category>
		<category><![CDATA[containers]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7579</guid>

					<description><![CDATA[Yes, a most powerful of warriors – patience – offers a gift today: at last, new OpenContrail container images are on Docker Hub. If you were paying attention, you know...]]></description>
										<content:encoded><![CDATA[<p><img fetchpriority="high" decoding="async" class="alignnone wp-image-7580" src="http://www.opencontrail.org/wp-content/uploads/2017/09/colorful-2567060_1280.jpg" alt="" width="500" height="361" /></p>
<p>Yes, a most powerful of warriors – patience – offers a gift today: at last, new OpenContrail container images are on <a href="https://hub.docker.com/u/opencontrail/">Docker Hub</a>.</p>
<p>If you were paying attention, you know that OpenContrail software was recently containerized. The control and management components were packaged into 4 containers, and the vRouter’s kernel module deployment is container-enabled too.</p>
<p>This new canonical way to deploy OpenContrail was eagerly anticipated, simplifying the day-1 user experience. News of the package refactoring was revealed in a <a href="http://www.opencontrail.org/container-networking-made-simple-with-opencontrail-and-kubernetes/">past blog</a> that also covered integration with Kubernetes. The formal software containerization support in Juniper’s Contrail Networking followed this June in the release of version 4.0.</p>
<p>The support of networking containers as endpoints came a while ago however. Some of us have been using it <a href="https://engineering.riotgames.com/news/running-online-services-riot-part-iii">in production</a> and others have been musing with that support paired with Kubernetes; it’s been 2^9 days since my early <a href="http://www.opencontrail.org/getting-to-gifee-with-sdn-demo/">demo</a> of OpenContrail with Kubernetes and OpenShift (see the newer <a href="https://www.youtube.com/watch?v=LKL3vLErsvY&amp;t=43s">demo</a> now).</p>
<p>That original open-sourced demo was in fact using Docker container images that Juniper uploaded to Docker Hub way back for version 2.20. After none of the subsequent releases made it to Docker Hub, you may have been wondering if those images were a one-hit wonder: nope. While the community is rolling a CI/CD <a href="https://github.com/Juniper/contrail-controller/wiki/OpenContrail-Continuous-Integration-(CI)">pipeline</a> for OpenContrail’s core elements, today’s posting of the version 4.0.1 images is an intermediate step until that fully codifies.</p>
<p>The containerization of the OpenContrail software itself, may understandably lead you to associate it with other container tools like CNI, Kubernetes, Mesos or OpenShift – all of which are supported – but it’s worth noting that the containerized deployment is also used <a href="https://gitlab.com/gokulpch/OpenContrail-Kolla/blob/master/README.md">with OpenStack</a> Kolla. That being said, it’s exciting to imagine the possibility of deploying OpenContrail containers directly on top of container orchestration platforms, bringing their features to bear to manage an OpenContrail deployment. This is exactly what’s being planned with the help of Helm. In the meantime, it’s still click-click easy with the server manager GUI, and equally simple with <a href="https://github.com/Juniper/contrail-ansible">Ansible</a>, which also affords you the opportunity to deploy your SDN as code a la DevNetOps, perhaps upholding your application stack and DevOps; now there’s a dynamic duo!</p>
<p>The new Docker Hub images shouldn’t lower the barrier to entry, with any luck, they should remove it entirely. For example, if you’re working with Kubernetes, you’ve already done enough learning and lifting to get that going, and the hope is to keep you focused on that: until you want to dig into SDN, the OpenContrail networking and security features just work. To make that a reality, the download and <a href="https://github.com/Juniper/contrail-docker/wiki/Provision-Contrail-CNI-for-Kubernetes">installation of OpenContrail</a> needs to be simple and steady, and then get out of your way. Hopefully that’s what you’ll find. If you do, please support the community by giving us some stars on <a href="https://hub.docker.com/u/opencontrail/">Docker Hub</a>, and tell others about your experience.</p>
<p>&nbsp;</p>
<p><strong>Recap of key resources:</strong></p>
<ul>
<li><a href="https://github.com/Juniper/contrail-docker/wiki/Provision-Contrail-CNI-for-Kubernetes">Installation with Kubernetes</a></li>
<li><a href="https://gitlab.com/gokulpch/OpenContrail-Kolla/blob/master/README.md">Installation with OpenStack</a></li>
<li><a href="https://github.com/Juniper/contrail-controller/wiki">OpenContrail Wiki</a></li>
</ul>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Are Service Meshes the Next-gen SDN?</title>
		<link>https://tungsten.io/are-service-meshes-the-next-gen-sdn/</link>
		
		<dc:creator><![CDATA[James Kelly]]></dc:creator>
		<pubDate>Tue, 20 Jun 2017 06:03:38 +0000</pubDate>
				<category><![CDATA[Cloud]]></category>
		<category><![CDATA[Containers]]></category>
		<category><![CDATA[Docker]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<category><![CDATA[OpenShift]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7533</guid>

					<description><![CDATA[June 28, 2017 update: more awesome background on service meshes, proxies and Istio in particular on yet another new SE Daily podcast with Istio engineers from Google. June 26, 2017...]]></description>
										<content:encoded><![CDATA[<p><img decoding="async" class="alignnone wp-image-7534" src="http://www.opencontrail.org/wp-content/uploads/2017/06/mesh-1430107.png" alt="" width="100%" /></p>
<p><a href="https://soundcloud.com/james-kelly-63/are-service-meshes-the-next-gen-sdn" target="_blank" rel="noopener"><img decoding="async" class="alignnone wp-image-7544" src="http://www.opencontrail.org/wp-content/uploads/2017/06/Screen-Shot-2017-06-19-at-11.01.15-PM.png" alt="" width="100%" /></a></p>
<p><span style="font-family: arial, helvetica, sans-serif"><em><strong>June 28, 2017 update:</strong></em> more awesome background on service meshes, proxies and Istio in particular on yet another new SE Daily <a href="https://softwareengineeringdaily.com/2017/06/27/istio-service-mesh-with-varun-talwar-and-louis-ryan/" target="_blank" rel="noopener">podcast</a> with Istio engineers from Google.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"><strong><em>June 26, 2017 update: </em></strong>For great background on service meshes (a relatively new concept) check out <a href="https://softwareengineeringdaily.com/2017/06/26/service-mesh-with-william-morgan/" target="_blank" rel="noopener">today&#8217;s podcast</a> on SE Daily with the founder of Linkerd.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">Whether you’re adopting a containers- or functions-as-a-Service stack, or both, in the new world of micro-services architectures, one thing that has grown in importance is the network because:</span></p>
<ol>
<li><span style="font-family: arial, helvetica, sans-serif">Micro-service application building blocks are decoupled from one another over the network. They re-integrate over the network with APIs as remote procedure calls (RPC) that have evolved from the likes of CORBA and RMI, past web services with SOAP and REST, to new methods like Apache <a href="https://thrift.apache.org/">Thrift</a> and the even-fresher <a href="http://www.grpc.io/">gRPC</a>: a new CNCF project donated by Google for secure and fast http2-based RPC. RPC has been around for a long time, but now the network is actually fast enough to handle it as a general means of communication between application components, allowing us to break down monoliths where service modules would have previously been bundled or coupled with tighter API communications based on package includes, libraries, and some of us may even remember more esoteric IPC.</span></li>
</ol>
<p>&nbsp;</p>
<ol start="2">
<li><span style="font-family: arial, helvetica, sans-serif">Each micro-service building block scales out by instance <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">replication</a>. The front-end of each micro-service is thus a load balancer which is itself a network component, but beyond that, the services need to discover their dependent services, which is generally done with DNS and service discovery.</span></li>
</ol>
<p>&nbsp;</p>
<ol start="3">
<li><span style="font-family: arial, helvetica, sans-serif">To boost processing scale out and engineer better reliability, each micro-service instance is often itself decoupled from application state and its storage. The state is saved over the network as well. For example, using an API into an object store, a database, a k/v-store, a streaming queue or a message queue. There is also good-ol’ disk, but such disk and accompanying file systems too, may be virtual network-mounted volumes. The API- and RPC-accessible variants of storing state are, themselves, systems that are micro-services too, and probably the best example of using disks in fact. They would also incorporate a lot of distributed storage magic to deliver on whatever promises they make, and that magic is often performed over the network.</span></li>
</ol>
<p>&nbsp;</p>
<p><span style="font-family: arial, helvetica, sans-serif">Hopefully we’re all on the same page now as to why the network is important micro-services glue. If this was familiar to you, then maybe you already know about cloud-native SDN solutions and service meshes too.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">The idea and implementation of a service mesh is fairly new. The topic is also garnering a lot of attention because they handle the main networking challenges listed above (esp #1 &amp; 2), and much more in the case of new projects like the CNCF Linkerd and newly launched project Istio.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">Since I’ve written about SDN for container stacks before, namely OpenContrail for Kubernetes and OpenShift, I’m not going to cover it super deeply. Nor am I going to cover service meshes in general detail except to make comparisons. I will also put some references below and throughout. And I’ve tried to organize my blog by compartmentalizing the comparisons, so you can skip technical bits that you might not care for, and dive into the ones that matter most to you.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">So on to the fun! Let’s look at some of the use cases and features of services meshes and compare them to SDN solutions, mostly OpenContrail, so we can answer the question posed in the title. Are service meshes the “Next-Generation” of SDN?</span></p>
<p>&nbsp;</p>
<p><img decoding="async" class="wp-image-7535 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2017/06/logos-service-mesh.png" alt="" width="508" height="286" /></p>
<p>&nbsp;</p>
<p><span style="font-family: arial, helvetica, sans-serif"><strong>Automating SDN and Service Meshes</strong></span></p>
<p><span style="font-family: arial, helvetica, sans-serif">First, let’s have a look at 3 general aspects of automation in various contexts where SDN and service meshes are used: 1 &#8211; programmability, 2 &#8211; configuration and 3 &#8211; installation.</span></p>
<ol>
<li><span style="font-family: arial, helvetica, sans-serif"><em>Programmability</em></span><br />
<span style="font-family: arial, helvetica, sans-serif"> When it comes to automating everything, programmability is a must. Good SDNs are untethered from the hardware side of networking, and many, like OpenContrail, offer a logically centralized control plane with an <a href="http://www.opencontrail.org/documentation/api/r4.0/">API</a>. The main two service meshes introduced above do this too, and they follow an architectural pattern similar to SDNs of centralized control plane with a distributed forwarding plane agent. While Istio has a centralized control plane <a href="https://istio.io/docs/concepts/what-is-istio/overview.html#architecture">API</a>, Linkerd is more distributed but offers an <a href="https://blog.buoyant.io/2017/05/24/a-service-mesh-for-kubernetes-part-x-the-service-mesh-api/">API</a> through its Namerd counterpart. Most people would probably say that the two service meshes’ gRPC API is more modern and advantageous than the OpenContrail RESTful API, but then again OpenContrail’s API is very well built-out and tested compared to Istio’s still-primordial API functions.</span></li>
<li><span style="font-family: arial, helvetica, sans-serif"><em>Configuration<br />
</em>A bigger difference than the API, is in how functionality can be accessed. The service meshes take in YAML to declare configuration intent that can be delivered through a CLI. I suppose most people would agree that’s an advantage over SDNs that don’t offer that (at least OpenContrail doesn’t today). In terms of a web-based interface, the service meshes do offer those, as so many SDNs. OpenContrail’s web interface is fairly sophisticated after 5 years of development, yet still modern-feeling and enterprise friendly.</span><span style="font-family: arial, helvetica, sans-serif"> Looking toward “network as code” trends however, CLI and YAML is codable and version controllable more easily than say OpenContrail’s API calls. In an OpenStack environment OpenContrail can be configured with YAML-based <a href="https://docs.openstack.org/developer/heat/template_guide/hot_spec.html#hot-spec">Heat</a> templates, but that’s less relevant for container-based K8s and OpenShift world. In a K8s world, OpenContrail SDN configuration is annotated into K8s objects. It’s intentionally simple, so it’s just exposing a fraction of the OpenContrail functionality. It remains to be seen what will be done with K8s <a href="https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-third-party-resource/">TPRs</a>, ConfigMaps or through some OpenContrail interpreter of its own.</span></li>
<li><span style="font-family: arial, helvetica, sans-serif"><em>Installation</em></span><br />
<span style="font-family: arial, helvetica, sans-serif"> When it comes to getting going with Linkerd, having a company behind it, Buoyant, means anyone can get support, but getting through day-one looks pretty straightforward on one’s own anyway. Deployed with Kubernetes in the model of a <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>, it is straightforward to use out of the box.</span><span style="font-family: arial, helvetica, sans-serif"> Istio is brand new, but already has Helm charts to deploy it quickly with Kubernetes thanks to our friends at Deis (<a href="https://twitter.com/LachlanEvenson">@LachlanEvenson</a> has done some amazing demo videos already –links below). Using Istio, on the other hand, means bundling its Envoy proxy into every Kubernetes pod as a <a href="http://blog.kubernetes.io/2015/06/the-distributed-system-toolkit-patterns.html">sidecar</a>. It’s an extra step, but it looks fairly painless with the <a href="https://www.istio.io/docs/tasks/integrating-services-into-istio.html">kube-inject</a> Sidecar <a href="https://linkerd.io/getting-started/k8s-daemonset/">vs.</a> DaemonSet considerations aside, this bundling is doing some magic, and it’s important to understand for debugging later.</span><span style="font-family: arial, helvetica, sans-serif"> When it comes to SDNs, they’re all different wrt deployments. OpenContrail is working on a Juniper-supported Helm chart for simple deployment, but in the meantime there are Ansible playbooks and other comparable configuration management solutions offered by the community.</span><span style="font-family: arial, helvetica, sans-serif"> One thing OpenContrail has in common with the two service meshes, is that it is deployed as containers. One difference is that OpenContrail’s forwarding agent on each node is both a user-space component and Kernel module (or DPDK-based or SmartNIC-based). They’re containerized, but the kernel module is only there for installation purposes to bootstrap the insmod installation. You may feel ambivalent towards kernel modules… The kernel module will obviously streamline performance and integration with the networking stack, but the resources it uses are not container-based, and thus not resource restricted, so resource management is different than say a user-space sidecar process. Anyway, this is same deal as using the kube-proxy or any IP tables-based networking which OpenContrail vRouter replaces.</span></li>
</ol>
<p><span style="font-family: arial, helvetica, sans-serif"><strong>SDN and Service Meshes: Considerations in DevOps</strong></span></p>
<p><span style="font-family: arial, helvetica, sans-serif">When reflecting on micro-services architectures, we must remember that the complexity doesn’t stop there. There is also the devops apparatus to manage the application through dev, test, staging and prod, and through continuous integration, delivery, and response. Let’s look at some of the considerations:</span></p>
<ol>
<li><span style="font-family: arial, helvetica, sans-serif"><em><strong>Multi-tenancy / multi-environment</strong><br />
</em>In a shared cluster, code shouldn’t focus on operational contexts like operator or application dev/test environments. To achieve this, we need isolation mechanisms. Kubernetes namespaces and RBAC help this, but there is still more to do. I’ll quickly recap my understanding of the routing in OpenContrail and service meshes to better dissect the considerations for context isolation.</span><span style="font-family: arial, helvetica, sans-serif"> <strong><em>&lt;background&gt;</em></strong></span><br />
<span style="font-family: arial, helvetica, sans-serif"> OpenContrail for K8s recap: One common SDN approach to isolation is overlay networks. They allow us to create virtual networks that are separate from each other on the wire (different encapsulation markings) and often in the forwarding agent as well. This is indeed the case with OpenContrail, but OpenContrail also allows higher-level namespace-like wrappers called <a href="http://www.opencontrail.org/opencontrail-architecture-documentation/#section3_2">domains/tenants and projects</a>. Domains are isolated from each other, projects within domains are isolated from each other, and virtual networks within projects are isolated from each other. This hierarchy maps nicely to isolate tenants and dev/test/staging/prod environments, and then we can use a virtual network to isolate every micro-service. To connect networks (optionally across domains and projects), a policy is created and applied to the networks that need connecting, and this policy can optionally specify direction, network names, ports, and service chains to insert (for example, a stateful firewall service). </span><span style="font-family: arial, helvetica, sans-serif"> The way these domains, projects, and networks are created for Kubernetes is based on annotations. OpenContrail maps namespaces to their own OpenContrail project or their own virtual network, so optionally micro-services can all be reachable to each other on one big network (similar to the default cluster behavior). There are security concerns there, and OpenContrail can also enforce ACL rules and automate their creation as a method of isolating micro-services for security based on K8s object annotations or implementing Kubernetes <a href="https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">NetworkPolicy</a> objects as OpenContrail <a href="http://www.opencontrail.org/opencontrail-architecture-documentation/#section3_2">security groups</a> and rules. Another kind of new annotations on objects like K8s deployments, jobs or services would specify the whole OpenContrail domain, project, and virtual network of choice. Personally, I think the best approach is a hierarchy designed to match devops teams and environments structure that makes use of the OpenContrail model of segmentation by domain, project and network. This is in (unfortunately) contrast to the simpler yet more frequently used global default-deny rule and ever-growing whitelist that ensues that turns your cluster into Swiss cheese. Have fun managing that :/ </span><span style="font-family: arial, helvetica, sans-serif"> The overlay for SDN is at layer 2, 3 and 4, meaning that when the packet is received on the node, the vRouter (in OpenContrail’s case) will receive the packet destined to it and look at the inner header (the VXLAN ID or MPLS LSP number) to determine the domain/tenant, project and network. Basically, the number identifies which routing table will be used as a lookup context for the inner destination address, and then (pending ACLs) the packet is handed off to the right container/pod interface (per <a href="https://github.com/containernetworking/cni">CNI</a> standards).</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> Service mesh background: The model of Istio’s Envoy and Linkerd insofar as they are used (which can be on a per-microservice basis), is that there is a layer-7 router and proxy in front of your microservices. All traffic is intercepted at this proxy, and tunneled between nodes. Basically, it is also an overlay at a higher layer.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> The overlay at layer-7 is conceptually the same as SDN overlays except that the overlay protocol over the wire is generally HTTP or HTTP2, or TLS with one of those. In the DaemonSet deployment mode of Linkerd, there is one IP address for the host and Linkerd will proxy all traffic. It’s conceptually similar to the vRouter except in reality it is just handling HTTP traffic on certain ports, not all traffic. Traffic is routed and destinations are resolved using a delegation tables (<a href="https://twitter.github.io/finagle/guide/Names.html#interpreting-paths-with-delegation-tables">dtabs</a>) format inherited from Finagle. In the <a href="https://linkerd.io/in-depth/deployment/">sidecar</a> deployment model for Linkerd or for Istio’s Envoy (which is always a sidecar), the proxy is actually in the same container network context as each micro-service because it is in the same pod. There are some IP tables <a href="https://istio.io/docs/tasks/integrating-services-into-istio.html#understanding-what-happened">tricks</a> they do to sit between your application and the network. In Istio Pilot (the control plane) and Envoy (the data plane), traffic routing and destination resolution is based primarily on the Kubernetes service name.</span><br />
<span style="font-family: arial, helvetica, sans-serif"> <strong><em>&lt;/background&gt;</em></strong></span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> With that background, here are a few implications for multi-tenancy.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> Let’s observe that in the SDN setup, the tenant, environment and application (network) classification happens in the kernel vRouter. In service mesh proxies, we still need a CNI solution to get the packets into the pod in the first place. In Linkerd, we need dtab <a href="https://linkerd.io/in-depth/routing/">routing rules</a> that include tenant, environment and service. Dtabs seems to give a good way to break this down that is manageable. In the sidecar mode, more frequently used for Envoy, it’s likely that the pod in which traffic ends up already has a K8s namespace associated with it, and so we would map a tenant or environment outside of the Istio <a href="https://istio.io/docs/concepts/traffic-management/rules-configuration.html">rules</a>, and just focus on resolving the service name to a container and port when it comes to Envoy.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> It seems that OpenContrail here has a good way to match the hierarchy of separate teams, and separate out those RBAC and routing contexts. Linkerd dtabs are probably a more flexible way to create as many layers of routing interpretation as you want, but it may need a stronger RBAC to allow the splitting of dtabs among team tenants for security and coordination. Istio doesn’t do much in the way of isolating tenants and environments at all. Maybe that is out of scope for it which seems reasonable since Envoy is always a sidecar container and you should have underlying multi-tenant networking anyway to get traffic into the sidecar’s pod.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> One more point is that service discovery baked into the service mesh solutions, but it is still important in the SDN world, and systems that include DNS (OpenContrail does) can help manage name resolution in a multi-tenant way as well as provide IP address management (like bring your own IPs) across the environments you carve up. This is out of scope for service meshes, but with respect to multiple team and dev/test/staging/prod environments, it may be desirable to have the same IP address management pools and subnets.</span><em style="font-family: arial, helvetica, sans-serif"> </em></li>
</ol>
<ol start="2">
<li><span style="font-family: arial, helvetica, sans-serif"><em><strong>Deployment and load balancing</strong><br />
</em>When it comes to deployment and continuous delivery (CD), the fact that SDN is programmable helps, but service meshes have a clear advantage here because they’re designed with CD in mind.</span><span style="font-family: arial, helvetica, sans-serif"> To do <a href="https://martinfowler.com/bliki/BlueGreenDeployment.html">blue-green</a> deployments with SDN, it helps to have floating IP functionality. Basically, we can cut over to green (float a virtual IP to the new version of the micro-service) and safely float it back to blue if we needed to in case of an issue. As you continuously deliver or promote staging into the non-live deployment, you can still reach it with a different floating IP address. OpenContrail handles overlapping floating IPs to let you juggle this however you want to.</span><span style="font-family: arial, helvetica, sans-serif"> Service mesh routing rules can achieve the same thing, but based on routing switch overs at the HTTP level that point to for <a href="https://istio.io/docs/concepts/traffic-management/rules-configuration.html#split-traffic-between-service-versions">example</a> a newer backend version. What service meshes further allow is traffic roll over like this <a href="https://blog.buoyant.io/2016/11/04/a-service-mesh-for-kubernetes-part-iv-continuous-deployment-via-traffic-shifting/">example</a> showing a small percentage of traffic at first and then all of it, effectively giving you a canary deployment that is traffic load-oriented as opposed to a Kubernetes rolling upgrade or the Kubernetes deployment canary <a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/#canary-deployments">strategy</a> that gives you a canary that is instance-count based, and relies on the load balancing across instances to partition traffic.</span><span style="font-family: arial, helvetica, sans-serif"> This brings us to load balancing. Balancing traffic between the instances of a micro-service, by default happens with the K8s kube-proxy controller by its programming of IP tables. There is a bit of a performance and scale advantage here of using OpenContrail’s vRouter which uses its own ECMP load balancing and NAT instead of the kernel’s IP tables.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> Service meshes also handle such load balancing. They support wider ranging features, both in terms of load balancing schemes like <a href="https://blog.buoyant.io/2016/03/16/beyond-round-robin-load-balancing-for-latency/">EWMA</a> and also in terms of cases to eject an instance from the load balancing pool, like if they’re too slow.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> Of course service meshes do also handle load balancing for ingress HTTP frontending. Linkerd and Istio integrate with the K8s Ingress as ingress controllers. While most SDNs don’t seem to offer this, OpenContrail does have a solution here that is based on haproxy, an open source TCP proxy project. One difference, is that OpenContrail does not yet support SSL/TLS, but there are also K8s pluggable alternatives like nginx for pure software-defined load balancing.</span></li>
<li><span style="font-family: arial, helvetica, sans-serif"><em><strong>Reliability Engineering</strong><br />
</em>Yes, I categorize SRE and continuous response under the DevOps umbrella. In this area, since service meshes are more application-aware, it’s no surprise, they do the most further the causes of reliability.</span><span style="font-family: arial, helvetica, sans-serif"> When it comes to reliably optimizing and engineering performance, one point here from above is that EWMA and such advanced load balancing policies will assist in avoiding or ejecting slow instances, thus improving tail latency. A Buoyant <a href="https://blog.buoyant.io/2017/01/31/making-things-faster-by-adding-more-steps/">article</a> about performance addresses performance in terms of latency directly. Envoy and Linkerd are after all TCP proxies, and unpacking and repacking a TCP stream is seen as notoriously slow if you’re in the networking world (I can attest to this personally recalling one project I assisted with that did HTTP header injection for ad placement purposes). Anyway, processors have come far, and Envoy and Linkerd are probably some of the fastest TCP proxies you can get. That said, there are always the sensitive folks that balk at inserting such latency. I thought it was enlightening that in the test conducted in the article cited above, they’ve added more latency and steps, but because they’re also adding intelligence, they’re netting an overall latency speed up!</span><span style="font-family: arial, helvetica, sans-serif"> The consensus seems to be that service meshes solve more problems than they create, such as latency. Are they right for your particular case? As somebody highly quoted once said, “it depends.” As is the case with DPI-based firewalls, these kind of traffic processing applications can have great latency and throughput with a given feature set or load, but wildly different performance by turning on certain features or under load. Not that it’s a fair comparison, but the lightweight stateless processing that an SDN forwarding agent does is always going to be way faster than such proxies, especially when, like for OpenContrail, there are smart NIC vendors implementing the vRouter in hardware.</span><span style="font-family: arial, helvetica, sans-serif"> Another area that needs more attention in terms of reliability is security. As soon as I think of a TCP proxy, my mind wonders about protecting against a DoS attack because so much state is created to track each session. A nice way that service meshes nicely solve this is through the use of TLS. While Linkerd can support this, Istio makes this even easier because of the Istio Auth controller for key management. This is a great step to not only securing traffic over the wire (which SDNs could do too with IPsec etc.), but also making strong identity-based AAA for each micro-service. It’s worth noting that these proxies can change the wire protocol to anything they can configure, regardless of if it was initiated as such from the application. So an HTTP request could be sent as HTTP2 within TLS on the wire.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"> I’ll cap off this section by mentioning circuit breaking. I don’t know of any means that an SDN solution could do this very well without interpreting a lot of analytics and application tracing information and feeding that back into the API of the SDN. Even if that is possible in theory, service meshes already do this today as a built-in <a href="https://istio.io/docs/concepts/traffic-management/handling-failures.html">feature</a> to gracefully handle failures instead of having them escalate.</span></li>
<li><span style="font-family: arial, helvetica, sans-serif"><em><strong>Testing and debugging</strong><br />
</em>This is an important topic, but there’s not really an apples-to-apples comparison of features, so I’ll just hit prominent points on this topic separately.</span><span style="font-family: arial, helvetica, sans-serif"> Services meshes provide an application RPC-oriented view into the intercommunication in the mesh of micro-services. This information can be very useful for monitoring and ops visibility and during debugging by tracing the communication path across an application. Linkerd <a href="https://linkerd.io/features/distributed-tracing-and-instrumentation/">integrates</a> with Zipkin for tracing and other tools for metrics, and works for applications written in any language unlike some language-specific tracing libraries.</span><span style="font-family: arial, helvetica, sans-serif"> Service meshes also provide per-request routing based on things like HTTP headers, which can be manipulated for testing. Additionally, Istio also provides fault <a href="https://istio.io/docs/concepts/traffic-management/fault-injection.html">injection</a> to simulate blunders in the application.</span><span style="font-family: arial, helvetica, sans-serif"> On the SDN side of things, solutions differ. OpenContrail is fairly mature in this space compared to the other choices one has with CNI providers. OpenContrail has the ability to run packet capture and sniffers like Wireshark on demand, and its comprehensive analytics engines and visibility tools expose flow records and other traffic stats. Aside from debugging (at a more of network level), there are interesting security applications for auditing ACL deny logs. Finally, OpenContrail can tell you the end-to-end path of your traffic if it’s run atop of a physical network (not a cloud). All of this can potentially help debugging, but the kind of information is far more indirect vis-à-vis the applications, and is probably better suited for NetOps.</span></li>
</ol>
<p><span style="font-family: arial, helvetica, sans-serif"><strong>Legacy and Other Interconnection</strong></span></p>
<p><span style="font-family: arial, helvetica, sans-serif">Service meshes seem great in many ways, but one hitch to watch out for is how they can allow or block your micro-services connecting to your legacy services or any services that don’t have a proxy in front of them.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">If you are storing state in S3 or making a call to a cloud service, that’s an external call. If you’re reaching back to a legacy application like an Oracle database, same deal. If you’re calling an RPC of another micro-service that isn’t on the service mesh (for example it’s sitting in virtual machine instead of a container), same again. If your micro-service is supposed to deal with traffic that isn’t TCP traffic, that too isn’t going to be handled through your service mesh (for example, DNS is UDP traffic, ping is ICMP).</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">In the case of Istio, you can setup <a href="https://istio.io/docs/tasks/egress.html">egress</a> connectivity with a service alias, but that may require changes to the application, so a direct pass-thru is perhaps a simpler option. Also there are a lot of variants of TCP traffic that are not HTTP nor directly supported as higher-level protocols riding on HTTP. Common examples might be ssh and mail protocols.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">There is also the question of how service meshes will handle multiple IPs per pod and multiple network interfaces per pod once CNI soon allows it.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">You most certainly have some of this communication in your applications that doesn’t quite fit the mesh. In these cases you not only need to plan how to allow this communication, but also how to do it securely, probably with an underlying SDN solution like OpenContrail that can span Kubernetes as well as OpenStack, VMware and metal.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"><strong>What do you think?</strong></span></p>
<p><span style="font-family: arial, helvetica, sans-serif">Going back to the original question in the title: Are Service Meshes the Next-Gen SDN?</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">On one hand: yes! because they‘re eating a lot of the value that some SDNs provided by enabling micro-segmentation and security for RPC between micro-services. Service meshes are able to do this with improved TLS-based security and identity assignment to each micro-service. Also service meshes are adding advanced application-aware load balancing and fault handling that is otherwise hard to achieve without application analytics and code refactoring.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">On the other hand: no! because service meshes sit atop of CNI and container connectivity. They ride on top of SDN, so they’ll still need a solid foundation. Moreover, most teams will want multiple layers of security isolation when they can get micro-segmentation and multi-tenancy that comes with SDN solutions without any performance penalty. SDN solutions can also span connectivity across clusters, stacks and runtimes other than containers, and satisfy the latency obsessed.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif">Either way, service meshes are a new, cool and shiny networking toy. They offer a lot of value beyond the networking and security values that they subsume, and I think we’ll soon see them in just about every micro-services architecture and stack.</span></p>
<p><span style="font-family: arial, helvetica, sans-serif"><strong>More questions…</strong></span></p>
<p><span style="font-family: arial, helvetica, sans-serif">Hopefully something in this never-ending blog makes you question SDN or the service meshes. Share your thoughts or questions. The anti-pattern of technology forecasting is thinking we’re done, so some open questions:</span></p>
<ol>
<li><span style="font-family: arial, helvetica, sans-serif">Should we mash-up service meshes and fit Linkerd into the Istio framework as an alternative to Envoy? If so, why?</span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Should we mash-up OpenContrail and service meshes and how?</span></li>
</ol>
<p><span style="font-family: arial, helvetica, sans-serif"><strong>Resources on Learning About Service Meshes</strong></span></p>
<ul>
<li><span style="font-family: arial, helvetica, sans-serif">Istio blog: <a href="https://istio.io/blog/">https://istio.io/blog/</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Buoyant blog: <a href="https://blog.buoyant.io/">https://blog.buoyant.io/</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Demo videos with Istio and Kubernetes thanks to Lachie:</span></li>
<li><span style="font-family: arial, helvetica, sans-serif"><a href="https://www.youtube.com/watch?v=ePwd5bK2Cuo&amp;list=PLbj_Bz58yLCw09JYfG2xbFMi5-jN89LfB">https://www.youtube.com/watch?v=ePwd5bK2Cuo&amp;list=PLbj_Bz58yLCw09JYfG2xbFMi5-jN89LfB</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Istio Service Mesh Podcast: <a href="https://softwareengineeringdaily.com/2017/06/27/istio-service-mesh-with-varun-talwar-and-louis-ryan/" target="_blank" rel="noopener">https://softwareengineeringdaily.com/2017/06/27/istio-service-mesh-with-varun-talwar-and-louis-ryan/</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Linkerd Service Mesh Podcast: <a href="https://softwareengineeringdaily.com/2017/06/26/service-mesh-with-william-morgan/" target="_blank" rel="noopener">https://softwareengineeringdaily.com/2017/06/26/service-mesh-with-william-morgan/</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Scaling Twitter Podcast about Linkerd: <a href="https://softwareengineeringdaily.com/2016/06/22/scaling-twitter-buoyant-ios-william-morgan/">https://softwareengineeringdaily.com/2016/06/22/scaling-twitter-buoyant-ios-william-morgan/</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Service Proxying Podcast about Envoy: <a href="https://softwareengineeringdaily.com/2017/02/14/service-proxying-with-matt-klein/">https://softwareengineeringdaily.com/2017/02/14/service-proxying-with-matt-klein/</a></span></li>
<li><span style="font-family: arial, helvetica, sans-serif">Comparing Envoy and Linkerd: <a href="https://lyft.github.io/envoy/docs/intro/comparison.html#id7">https://lyft.github.io/envoy/docs/intro/comparison.html#id7</a></span></li>
</ul>
<p>&nbsp;</p>
<p><em><span style="font-family: arial, helvetica, sans-serif">This blog was originally posted at <a href="http://jameskelly.net/blog/2017/6/19/are-service-meshes-the-next-gen-sdn" target="_blank" rel="noopener">http://jameskelly.net/blog/2017/6/19/are-service-meshes-the-next-gen-sdn</a> </span></em></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Container networking made simple with OpenContrail and Kubernetes</title>
		<link>https://tungsten.io/container-networking-made-simple-with-opencontrail-and-kubernetes/</link>
		
		<dc:creator><![CDATA[Yuvaraja Mariappan]]></dc:creator>
		<pubDate>Thu, 18 May 2017 18:18:33 +0000</pubDate>
				<category><![CDATA[Containers]]></category>
		<category><![CDATA[Kubernetes]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7510</guid>

					<description><![CDATA[This blog is co-authored by Gokul Chandra Purnachandra Reddy and Yuvaraja Mariappan from Juniper Networks. We have recently announced the latest enhancements done to OpenContrail where one of the features that stood...]]></description>
										<content:encoded><![CDATA[<p><em>This blog is co-authored by Gokul Chandra Purnachandra Reddy and Yuvaraja Mariappan from Juniper Networks.</em></p>
<p>We have recently <a href="https://forums.juniper.net/t5/SDN-and-NFV-Era/Here-s-Why-Contrail-Was-Chosen-as-the-Best-Commercial-SDN/ba-p/307517">announced </a>the latest enhancements done to OpenContrail where one of the features that stood out is containerizing the contrail controller services to enable ease of deployment and operation.</p>
<p>Additionally, OpenContrail will be available as a network plugin for CNI enabling contrail to provide networking services for containers in frameworks such as Kubernetes, OpenShift and Mesos.</p>
<p>In any Kubernetes deployment, OpenContrail offers the following capabilities &#8211; pod addressing, network isolation, policy based security, gateway services, snat, ecmp load balancing in services and ingress load balancing</p>
<p>The following are some of the key advantages of the Opencontrail networking solution for K8s.</p>
<ul>
<li>Load Balancing: A non-proxy load balancing capability based on ECMP paths without the need for any additional hops. This is implemented as a distributed construct in vRouters for the virtual IP addresses used in K8s service objects. This also eliminates the need for kube-proxy which is a drag on the performance due to its constant reconfiguration of iptables NAT rules.</li>
<li>Network Policy: OpenContrail implements the Kubernetes network policy objects applied to pods.  This framework enables tenant and network isolation with constructs for micro-segmentation using custom security groups. It also provides capabilities to insert transparent network services such as firewalls, DPI etc.</li>
<li>Ingress Controller: Stock Kubernetes solution does not provide an implementation to support ingress load balancing for services. With OpenContrail this implementation comes standard which provides a solution with HAProxy.</li>
</ul>
<p>The following video provides a view into how these features work with OpenContrail.</p>
<p>&nbsp;</p>
<p><iframe loading="lazy" src="https://www.youtube.com/embed/KL0E4SaRCs0" width="560" height="315" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
