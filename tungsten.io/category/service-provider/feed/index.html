<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Service Provider Archives - Tungsten Fabric</title>
	<atom:link href="https://tungsten.io/category/service-provider/feed/" rel="self" type="application/rss+xml" />
	<link>https://tungsten.io/category/service-provider/</link>
	<description>multicloud multistack SDN</description>
	<lastBuildDate>Fri, 14 Apr 2017 18:19:18 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.1</generator>

<image>
	<url>https://tungsten.io/wp-content/uploads/sites/73/2018/03/cropped-TungstenFabric_Stacked_Gradient_3000px-150x150.png</url>
	<title>Service Provider Archives - Tungsten Fabric</title>
	<link>https://tungsten.io/category/service-provider/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>OpenContrail &#8211; Enabling Advancements in Cloud Infrastructure Adoption</title>
		<link>https://tungsten.io/opencontrail-enabling-advancements-in-cloud-infrastructure-adoption/</link>
		
		<dc:creator><![CDATA[Geoff Sullivan]]></dc:creator>
		<pubDate>Fri, 14 Apr 2017 18:19:18 +0000</pubDate>
				<category><![CDATA[OpenStack]]></category>
		<category><![CDATA[Orchestration]]></category>
		<category><![CDATA[SDN]]></category>
		<category><![CDATA[Service Provider]]></category>
		<category><![CDATA[Use Case]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7458</guid>

					<description><![CDATA[OpenContrail has solidified its position as the top SDN and network automation solution for OpenStack, and recently announced it&#8217;s integration into Kubernetes (K8s) and thus OpenShift (recent versions are powered by Kubernetes.) What does this mean for...]]></description>
										<content:encoded><![CDATA[<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/AAEAAQAAAAAAAAm9AAAAJDIwMDFhYWI3LWI2YWYtNDVhNC1iOTMwLTFiNGNjODZkYTdhZg.png"><img fetchpriority="high" decoding="async" class="aligncenter wp-image-7460" src="http://www.opencontrail.org/wp-content/uploads/2017/04/AAEAAQAAAAAAAAm9AAAAJDIwMDFhYWI3LWI2YWYtNDVhNC1iOTMwLTFiNGNjODZkYTdhZg.png" alt="" width="651" height="350" data-id="7460" /></a></p>
<p>OpenContrail has solidified its position as the top SDN and network automation solution for <a href="https://www.openstack.org/" target="_blank" rel="nofollow noopener">OpenStack</a>, and <a href="http://www.opencontrail.org/the-best-sdn-for-openstack-now-for-kubernetes/" target="_blank" rel="nofollow noopener">recently announced</a> it&#8217;s integration into <a href="https://kubernetes.io/" target="_blank" rel="nofollow noopener">Kubernetes (K8s)</a> and thus <a href="https://www.openshift.com/" target="_blank" rel="nofollow noopener">OpenShift</a> (recent versions are powered by Kubernetes.) What does this mean for the organization mapping out their cloud infrastructure architecture?</p>
<h3>VMware -&gt; OpenStack -&gt; Kubernetes</h3>
<p>At this juncture in cloud adoption, service providers and enterprises alike are defining their ever evolving stacks. The emergence of containers has changed the game &#8211; changing the conversation from &#8220;OpenStack or Kubernetes?&#8221; to &#8220;OpenStack and Kubernetes.&#8221; In many cases VMware is still in the mix somewhere. The combination of these platforms allows an organization to select the best hosting platform for each workload, based on it&#8217;s own unique characteristics:</p>
<ul>
<li>Aging monolithic application that will be deprecated in the next few years? Keep it where it is (VMware.)</li>
<li>Distributed multi-tier web app? Perhaps OpenStack is a good candidate.</li>
<li>Highly variable, net new microservices based application(s) &#8211; Kubernetes will allow the individual microservices to scale up and down independently of one another.</li>
</ul>
<p>While all of this choice provides flexibility, it introduces complexity &#8211; especially on the network with each of the three platforms with three different networking stacks.</p>
<h3>How Contrail enables Successful Cloud Adoption</h3>
<p>Introducing Contrail into a heterogeneous cloud/virtualization environment will help an organization move towards SDN adoption based on open standards. One SDN solution to manage them all! Furthermore, it isn&#8217;t realistic or likely for an organization to go from VMware to OpenStack to Kubernetes all at once &#8211; that is too much change to manage and it introduces risk. Because of it&#8217;s interoperability with all of these platforms, introducing Contrail will allow an organization to stitch together their virtualization/container platforms on their terms as their infrastructure evolves with the applications that sit atop it.</p>
<p>Check out how Contrail integrates with VMware, OpenStack, Kubernetes and OpenShift:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/AAEAAQAAAAAAAA2YAAAAJDA1ZDg2MGU3LWU4OTAtNDA2NS1hMjJhLTgwMzVhZjM4ZmRiMQ.png"><img decoding="async" class="size-full wp-image-7459 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2017/04/AAEAAQAAAAAAAA2YAAAAJDA1ZDg2MGU3LWU4OTAtNDA2NS1hMjJhLTgwMzVhZjM4ZmRiMQ.png" alt="" width="960" height="458" data-id="7459" /></a></p>
<ul>
<li><a href="http://www.opencontrail.org/integrating-vmware-esxi-with-openstack-opencontrail/" target="_blank" rel="nofollow noopener">Integrating VMware ESXi with OpenStack, OpenContrail</a></li>
<li><a href="http://www.juniper.net/techpubs/en_US/contrail2.2/topics/task/configuration/vcenter-integration-vnc.html" target="_blank" rel="nofollow noopener">Installing Contrail with VMware vCenter</a></li>
<li><a href="http://www.opencontrail.org/opencontrail-kubernetes-integration/" target="_blank" rel="nofollow noopener">OpenContrail Kubernetes Integration</a></li>
<li><a href="https://www.youtube.com/user/OpenContrail" target="_blank">Video Demo: OpenContrail integration with OpenShift, Kubernetes</a></li>
</ul>
<h3>3 ways to get Getting Started with OpenContrail</h3>
<ul>
<li>Try it yourself! &#8211; <a href="http://www.opencontrail.org/opencontrail-quick-start-guide/" target="_blank" rel="nofollow noopener">OpenContrail Quick Start Guide</a> (Software <em>Installation guide</em> for <em>OpenContrail)</em></li>
<li>Try OpenContrail Sandbox &#8211; <a href="http://www.opencontrail.org/sandbox/" target="_blank" rel="nofollow noopener">The Contrail Sandbox</a> is a cloud based testing environment that can help you evaluate Contrail Networking product on our infrastructure free of cost</li>
<li><a href="http://mailto:gsullivan@juniper.net/" target="_blank" rel="nofollow noopener">Let&#8217;s Chat &#8211; Shoot me an email!</a></li>
</ul>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Intelligent NFV performance with OpenContrail</title>
		<link>https://tungsten.io/intelligent-nfv-performance-with-opencontrail/</link>
		
		<dc:creator><![CDATA[Sergey Matov]]></dc:creator>
		<pubDate>Tue, 04 Apr 2017 23:38:03 +0000</pubDate>
				<category><![CDATA[NFV]]></category>
		<category><![CDATA[Service Provider]]></category>
		<category><![CDATA[SmartNIC]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7431</guid>

					<description><![CDATA[The private cloud market has changed in the past year, and our customers are no longer interested in just getting an amazing tool for installing OpenStack; instead, they are looking...]]></description>
										<content:encoded><![CDATA[<p>The private cloud market has changed in the past year, and our customers are no longer interested in just getting an amazing tool for installing OpenStack; instead, they are looking more at use cases. Because we see a lot of interest in NFV cloud use cases, Mirantis includes OpenContrail as the default SDN for its new Mirantis Cloud Platform. In fact, NFV has become a mantra for most service providers, and because Mirantis is a key player in this market, we work on a lot of testing and performance validation.</p>
<p>The most common value for performance comparison between solutions is bandwidth, which shows how much capacity a network connection has for supporting data transfer, as measured in bits per second. In this domain, the OpenContrail vRouter can reach near line speed (<a href="http://www.opencontrail.org/evaluating-opencontrail-virtual-router-performance/">about 90%, in fact</a>). However, performance also depends on other factors, such as latency, or packets-per-second (pps), which are as important as bandwidth. Packets per second rate is a key factor for VNF (firewalls, routers, etc.) instances running on top of NFV clouds. In this article, we’ll compare PPS rate for different OpenContrail setups so you can decide what will work best for your specific use case.</p>
<p>The simplest way to test PPS rate is to run a VM to VM test. We will provide a short overview of OpenContrail low-level techniques for NFV infrastructure, and perform a comparative analysis of different approaches using simple PPS benchmarking. To make testing fair, we will use only a 10GbE physical interface, and will limit resource consumption for data plane acceleration technologies, making the environment identical for all approaches.</p>
<h2>OpenContrail vRouter modes</h2>
<p>For different use cases, Mirantis supports several ways of running the OpenContrail vRouter as part of Mirantis Cloud Platform 1.0 (MCP). Let’s look at each of them before we go ahead and take measurements.</p>
<h3><b>Kernel vRouter</b></h3>
<p>OpenContrail has a module called vRouter that performs data forwarding in the kernel. The vRouter module is an alternative to Linux bridge or Open vSwitch (OVS) in the kernel, and one of its functionalities is encapsulating packets sent to the overlay network and decapsulating packets received from the overlay network. A simplified schematic of VM to VM connectivity for 2 compute nodes can be found in Figure 1:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image00.png"><img decoding="async" class="size-full wp-image-7432 aligncenter" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image00.png" alt="" width="600" height="371" data-id="7432" /></a></p>
<p><b>Figure 1: A simplified schematic of VM to VM connectivity for 2 compute nodes</b></p>
<p>The problem with a kernel module is that packets-per-second is limited by various factors, such as memory copies, the number of VM exits, and the overhead of processing interrupts. Therefore vRouter can be integrated with the Intel DPDK to optimize PPS performance.</p>
<h3><b>DPDK vRouter</b></h3>
<p>Intel DPDK is an open source set of libraries and drivers that perform fast packet processing by enabling drivers to obtain direct control of the NIC address space and map packets directly into an application. The polling model of NIC drivers helps to avoid the overhead of interrupts from the NIC. To integrate with DPDK, the vRouter can now run in a user process instead of a kernel module. This process links with the DPDK libraries and communicates with the vrouter host agent, which runs as a separate process. The schematic for a simplified overview of vRouter-DPDK based nodes is shown in Figure 2:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image01.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7433" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image01.png" alt="" width="780" height="600" data-id="7433" /></a></p>
<p><b>Figure 2: The schematic for a simplified overview of vRouter-DPDK based nodes</b></p>
<p>vRouter-DPDK uses user-space packet processing and CPU affinity to dedicate poll mode drivers being served by a particular CPU. This approach enables packets to be processed in user-space during the complete life time – from physical NIC to vhost-user port.</p>
<h3><b>Netronome Agilio Solution</b></h3>
<p>Software and hardware components distributed by Netronome provide an OpenContrail-based platform to perform high-speed packet processing. It’s a scalable, easy to operate solution that includes all server-side networking features, such as overlay networking based on MPLS over UDP/GRE and VXLAN. The Agilio SmartNIC solution supports DPDK, SR-IOV and Express Virtio (XVIO) for data plane acceleration while running the OpenContrail control plane. Wide integration with OpenStack enables you to run VMs with Virtio devices or SR-IOV Passthrough vNICs, as in Figure 3:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image02.png"><img loading="lazy" decoding="async" class="aligncenter wp-image-7434" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image02.png" alt="" width="838" height="600" data-id="7434" /></a></p>
<p><b>Figure 3:  OpenContrail network schematic based on Netronome Agilio SmartNICs and software</b></p>
<p>A key feature of the Netronome Agilio solution is deep integration with OpenContrail and offloading of lookups and actions for vRouter tables.</p>
<p>Compute nodes based on Agilio SmartNICs and software can work in an OpenStack cluster based on OpenContrail without changes to orchestration. That means it’s scale-independent and can be plugged into existing OpenContrail environments with zero downtime.</p>
<p>Mirantis Cloud Platform can be used as an easy and fast delivery tool to set up Netronome Agilio-based compute nodes and provide orchestration and analysis of the cluster environment. Using Agilio and MCP, it is easily to setup a high-performance cluster with a ready-to-use NFV infrastructure.</p>
<h2>Testing scenario</h2>
<p>To make the test fair and clear, we will use an OpenStack cluster with two compute nodes. Each node will have a 10GbE NIC for the tenant network.</p>
<p>As we mentioned before, the simplest way to test the PPS rate is to run a VM to VM test. Each VM will have 2 Virtio interfaces to receive and transmit packets, 4 vCPU cores, 4096 MB of RAM and will run Pktgen-DPDK inside to generate and receive a high rate of traffic. For each VM a single Virtio interface will be used for generation, and another interface will be used for receiving incoming traffic from the other VM.</p>
<p>To make an analytic comparison of all technologies, we will not use more than 2 cores for the data plane acceleration engines. The results of the RX PPS rate for all VMs will be considered as a result for the VM to VM test.</p>
<p>First of all, we will try to measure kernel vRouter VM to VM performance. Nodes will be connected with Intel 82599 NICs. The following results were achieved for a UDP traffic performance test:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image03.png"><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-7435" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image03.png" alt="" width="600" height="371" data-id="7435" /></a></p>
<p>As you can see, the kernel vRouter is not suitable for providing a high packet per second rate, mostly because the interrupt-based model can’t handle a high rate of packets per second. With 64 byte packets we can only achieve 3% of line rate.</p>
<p>For the DPDK-based vRouter, we achieved the following results:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image04.png"><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-7436" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image04.png" alt="" width="600" height="371" data-id="7436" /></a></p>
<p>Based on these results, the DPDK based solution is better at handling high-rated traffic based on small UDP packets.</p>
<p>Lastly, we tested the Netronome Agilio SmartNIC-based compute nodes:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image05.png"><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-7437" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image05.png" alt="" width="600" height="371" data-id="7437" /></a></p>
<p>With only 2 forwarder cores, we are able to achieve line-rate speed on Netronome Agilio CX 10GbE SmartNICs on all size of packets.</p>
<p>You can also see a demonstration of the Netronome Agilio Solution <a href="https://www.youtube.com/watch?v=wZ8t63UyWw8">here</a>.</p>
<p>Since we have achieved line-rate speed on the 10GbE interface using Netronome Agilio SmartNICs we wanted to have the maximum possible PPS rate based on 2 CPUs. To determine the maximum performance result for this deployment, we will upgrade existing nodes with Netronome Agilio CX 40GbE SmartNIC and repeat the maximum PPS scenario one more time. We will use direct wire connection between 40GbE ports and will set up 64-bytes UDP traffic. Even with hard resources limitations, we achieved:</p>
<table style="height: 79px;" border="1" width="645" cellpadding="5">
<tbody>
<tr>
<td></td>
<td>Rate</td>
<td>Packet size, Bytes</td>
</tr>
<tr>
<td>Netronome Agilio Agilio CX 40GbE SmartNIC</td>
<td>19.9 Mpps</td>
<td>64</td>
</tr>
</tbody>
</table>
<h2>What we learned</h2>
<p>Taking all of the results together, we can see a pattern:</p>
<p><a href="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image06.png"><img loading="lazy" decoding="async" class="aligncenter size-full wp-image-7438" src="http://www.opencontrail.org/wp-content/uploads/2017/04/intelligent-nfv-performance-with-opencontrail-image06.png" alt="" width="600" height="371" data-id="7438" /></a></p>
<p>Based on 64 byte UDP traffic, we can also see where each solution stands compared to 10GbE line rate:</p>
<table>
<tbody>
<tr>
<td width="257"></td>
<td width="174">Rate</td>
<td width="150">% of line rate</td>
</tr>
<tr>
<td width="257">Netronome Agilio</td>
<td width="174">14.9 Mpps</td>
<td width="150">100</td>
</tr>
<tr>
<td width="257">vRouter DPDK</td>
<td width="174">4.0 Mpps</td>
<td width="150">26</td>
</tr>
<tr>
<td width="257">Kernel vRouter</td>
<td width="174">0.56 Mpps</td>
<td width="150">3</td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>OpenContrail remains the best production-ready SDN solution for OpenStack clusters, but to provide NFV-related infrastructure, OpenContrail can be used in different ways:</p>
<ul>
<li>The Kernel vRouter, based on interrupt model packet processing, works, but does not satisfy the high PPS rate requirement.</li>
<li>The DPDK-based vRouter significantly improves the PPS rate, but due to high resource consumption and because of defined limitations, it can’t achieve the required performance. We also can assume that using a modern DPDK library will improve performance and optimise resource consumption.</li>
<li>The Netronome Agilio SmartNIC solution significantly improves OpenContrail SDN performance, focusing on saving host resources and providing a stable high-performance infrastructure.</li>
</ul>
<p>With Mirantis Cloud Platform tooling, it is possible to provision, orchestrate and destroy high performance clusters with various networking features, making networking intelligent and agile.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Physical Network Function (PNF) service chaining with Contrail</title>
		<link>https://tungsten.io/physical-network-function-service-chaining-with-contrail/</link>
		
		<dc:creator><![CDATA[Graeme Robertson]]></dc:creator>
		<pubDate>Thu, 16 Jun 2016 19:35:37 +0000</pubDate>
				<category><![CDATA[Service Chaining]]></category>
		<category><![CDATA[Service Provider]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7058</guid>

					<description><![CDATA[When it comes to service functions there is a huge amount of focus on making the shift to virtualization or NFV. Virtual Network Functions (VNF) can be Firewalls, Load Balancers,...]]></description>
										<content:encoded><![CDATA[<p>When it comes to service functions there is a huge amount of focus on making the shift to virtualization or NFV. Virtual Network Functions (VNF) can be Firewalls, Load Balancers, routers, route reflectors, BNGs, EPCs, the list goes on. This is not without good reason as virtualization comes with many benefits such as increased agility, simpler automation, more granular scaling, licensing models that allow a true “pay as you grow” business model and in general the opportunity for Service Providers to revolutionize how they offer services to their customers. So why would we want to keep using physical network functions (PNF) and develop SDN solutions that support these PNF? Well, there are actually some pretty good reasons. Firstly, many service providers have made huge investments in these appliance based solutions and quite rightly expect to continue to realize the benefit of these investments for some years into the future. Secondly, when it come to raw throughput performance ASIC based forwarding is still far superior compared to x86 powered forwarding.  Serious improvements have been made but the gap is still wide.</p>
<p>As you probably know, Contrail provides the capability to insert network functions providing network services like those described above in the traffic path between two different virtual networks on demand and in a dynamic way.  There is no explicit dependency on the network function itself to allow service stitching to happen. As of the most recent Contrail releases PNF service chaining is also supported, we can now create service chains that are PNF only, VNF only or a hybrid of PNF and VNF with multiple instances of both physical and virtual in a single service chain. These PNF and VNF are included as part of network policy definition that is applied between two virtual networks as has always been the case for VNF service chaining. While using slightly different mechanisms under the hood to realize the correct route-leaking and next-hop updates that ensure traffic between the two virtual networks is correctly directed through the service appliances, the logic for PNF service chaining is the same as that used in the VNF service chaining approach. The main difference is that in the case of PNF service chaining Contrail pushes the required configuration to the MX router via Netconf rather than installing forwarding state on the vrouters running on the compute nodes. What’s really nice is that you can add many distinct chains running over the same physical appliance using the same interfaces, with each chain using different VLAN tag in order to maintain traffic segregation on the PNF.</p>
<p>Below is an example workflow of traffic flowing between two virtual networks/zones that is subject to a physical and/or virtual network services and an additional service chain between two different virtual networks that uses the same appliance. Some of this is covered in the video below.</p>
<p><iframe loading="lazy" src="https://www.youtube.com/embed/zlpdtqxgqQU" width="600" height="340" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<p>This is an attempt to show how to unleash the power of automation by leveraging existing network services as well as virtual services for Cloud environments!</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Service Chaining Enhancements in OpenContrail 3.0</title>
		<link>https://tungsten.io/port-tuples-service-chain-redundancy/</link>
		
		<dc:creator><![CDATA[Antonio Sanchez-Monge]]></dc:creator>
		<pubDate>Thu, 09 Jun 2016 00:10:22 +0000</pubDate>
				<category><![CDATA[Service Chaining]]></category>
		<category><![CDATA[Service Provider]]></category>
		<category><![CDATA[Use Case]]></category>
		<guid isPermaLink="false">http://www.opencontrail.org/?p=7048</guid>

					<description><![CDATA[OpenContrail 3.0 brings to the table an impressive series of new features and enhancements. Some of them are especially useful for Service Chaining – and not only for Virtual Network...]]></description>
										<content:encoded><![CDATA[<p>OpenContrail 3.0 brings to the table an impressive series of new features and enhancements. Some of them are especially useful for Service Chaining – and not only for Virtual Network Functions (VNF), but also for Physical Network Functions (PNF). This article describes a subset of these new features.</p>
<h2><strong>Decoupling Network from Compute in Service Chaining</strong></h2>
<p>In previous OpenContrail versions, Contrail acts like a Nova client and triggers the spawning of VNFs in OpenStack. We call this model v1 service chaining. Now, OpenContrail 3.0 supports both v1 and v2 (new!) service chaining. In the new model Contrail only takes care of the networking piece, which makes it easier to chain network functions implemented on a variety of compute flavors (VMs, containers, physical appliances).</p>
<p>The way we implement v2 service chaining is through Port Tuples. Let’s introduce one key concept (the VMI) and then discuss what Port Tuples are and their motivation.</p>
<p>One of the reasons why OpenContrail is so flexible in adopting new compute flavors (different typical hypervisors, containers, bare metal servers, etc.) is the VM Interface (VMI) concept. OpenContrail uses the VMI object abstraction as a means to interconnect a heterogeneous compute environment to the overlay network. Thanks to this abstraction layer, many of the features that were originally made to work for VMs also work seamlessly for other compute flavors.</p>
<p>In OpenContrail 3.0 we go one step further and define a Port Tuple as an ordered set of VM Interfaces. A given Port Tuple is an ordered list of network interfaces connected to the same VM, or container, or physical appliance. By chaining port tuples, it possible to build a service chain out of heterogeneous network functions, some of them virtual (VMs, containers) and others physical. In summary, Port Tuples are to NFV what VM Interfaces are to overlay networking.</p>
<p>Disclaimer: One of the scenarios displayed in the video (bare metal servers connected to vrouter CPE) is not supported at the time of this writing. It is shown as a way to illustrate the power of the port tuple concept and a possibly upcoming feature.</p>
<p><iframe loading="lazy" src="https://www.youtube.com/embed/wDRQq0pmln4" width="600" height="400" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
<h2><strong>Service Chain Redundancy in Contrail</strong></h2>
<p>OpenContrail has recently boosted its control plane feature set in several ways. One of them is routing policies. If you are familiar with network operating systems like Junos or IOS XR, you have certainly experienced routing policies more than once. Well, in OpenContrail this feature works exactly in the same way. Thanks to routing policies, it is possible to filter and modify routes in a fine-grained manner, adding a much greater control plane flexibility within OpenContrail itself.</p>
<p>At the time of publication of this post, the infrastructure has been completely developed and routing policies can be applied to Service Instance interfaces (left, right). In other words, it is currently possible to do fine-grained route leaking through a Service Chain. And the same infrastructure can be used in the future for other purposes too.</p>
<p>Among other use cases, the currently available routing policy feature set allows for Service Chain Redundancy. You can have a primary service chain in data center #1 and the backup service chain in data center #2. Combined with the Service Health Check feature, Service Providers can easily implement HA on their NFV offerings.</p>
<p><iframe loading="lazy" src="https://www.youtube.com/embed/RhcT8IcvCxo" width="600" height="400" frameborder="0" allowfullscreen="allowfullscreen"></iframe></p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
